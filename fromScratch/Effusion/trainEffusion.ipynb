{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import pickle\n",
    "from callback import MultipleClassAUROC, MultiGPUModelCheckpoint\n",
    "from configparser import ConfigParser\n",
    "from generator import AugmentedImageSequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from utility import get_sample_counts\n",
    "from weights import get_class_weights\n",
    "from augmenter import augmenter\n",
    "from tensorflow.keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer, Flatten, Input, GaussianNoise\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_radam import RAdam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#print(\"TensorFlow version: \", tf.__version__)\n",
    "#assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "#    \"This notebook requires TensorFlow 2.0 or above.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "# UPDATED: import from tensorflow.keras instead of keras\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics\n",
    "import gc\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneClass = \"Effusion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"./config.ini\"\n",
    "cp = ConfigParser()\n",
    "cp.read(config_file)\n",
    "\n",
    "    # default config\n",
    "output_dir = cp[\"DEFAULT\"].get(\"output_dir\")\n",
    "image_source_dir = cp[\"DEFAULT\"].get(\"image_source_dir\")\n",
    "base_model_name = cp[\"DEFAULT\"].get(\"base_model_name\")\n",
    "class_names = cp[\"DEFAULT\"].get(\"class_names\").split(\",\")\n",
    "\n",
    "    # train config\n",
    "use_base_model_weights = cp[\"TRAIN\"].getboolean(\"use_base_model_weights\")\n",
    "use_trained_model_weights = cp[\"TRAIN\"].getboolean(\"use_trained_model_weights\")\n",
    "use_best_weights = cp[\"TRAIN\"].getboolean(\"use_best_weights\")\n",
    "output_weights_name = cp[\"TRAIN\"].get(\"output_weights_name\")\n",
    "epochs = cp[\"TRAIN\"].getint(\"epochs\")\n",
    "batch_size = cp[\"TRAIN\"].getint(\"batch_size\")\n",
    "initial_learning_rate = cp[\"TRAIN\"].getfloat(\"initial_learning_rate\")\n",
    "generator_workers = cp[\"TRAIN\"].getint(\"generator_workers\")\n",
    "image_dimension = cp[\"TRAIN\"].getint(\"image_dimension\")\n",
    "train_steps = cp[\"TRAIN\"].get(\"train_steps\")\n",
    "patience_reduce_lr = cp[\"TRAIN\"].getint(\"patience_reduce_lr\")\n",
    "min_lr = cp[\"TRAIN\"].getfloat(\"min_lr\")\n",
    "validation_steps = cp[\"TRAIN\"].get(\"validation_steps\")\n",
    "positive_weights_multiply = cp[\"TRAIN\"].getfloat(\"positive_weights_multiply\")\n",
    "dataset_csv_dir = cp[\"TRAIN\"].get(\"dataset_csv_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=1.0, alpha=0.5):\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(tf.cast(y_pred,tf.float32) - tf.cast(y_true,tf.float32)), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network1(dropout=0.08425517073874295, neuronPct=0.1767547775828121, neuronShrink=0.33180474398878285):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "    # Construct neural network\n",
    "    neuronCount = neuronCount * neuronShrink\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,1536)))\n",
    "    model.add(Flatten(name='flat1'))\n",
    "    model.add(Dense(neuronCount,name='dense1'))\n",
    "    model.add(Activation('relu',name='relu1'))\n",
    "    model.add(Dropout(dropout, name='dropout1'))\n",
    "    model.add(Dense(14, activation='sigmoid',name='midLayer1')) # Output\n",
    "    weights_path = None\n",
    "    if weights_path is not None:\n",
    "        print(f\"load model weights_path: {weights_path}\")\n",
    "        model.load_weights(weights_path)\n",
    "    model.layers.pop()\n",
    "    dr = model.layers[-2].output\n",
    "    model.trainable = False\n",
    "    left = Dense(14, activation=\"sigmoid\", name='leftLayer1')(dr)\n",
    "    right = Dense(14, activation=\"sigmoid\", name='rightLayer1')(dr)\n",
    "    model = Model(model.input, [left,model.output,right])\n",
    "    #model = Model(model.input, model.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network2(dropout=0.15672137551441198, neuronPct=0.2197894476507525, neuronShrink=0.3803316528497302, noisePct=0.282563134185142):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "    # Construct neural network\n",
    "    neuronCount = neuronCount * neuronShrink\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,1536)))\n",
    "    model.add(Flatten(name='flat2'))\n",
    "    model.add(Dense(neuronCount,name='dense2'))\n",
    "    model.add(GaussianNoise(noisePct))\n",
    "    model.add(Activation('relu',name='relu2'))\n",
    "    model.add(Dropout(dropout, name='dropout2'))\n",
    "    model.add(Dense(14, activation='sigmoid',name='midLayer2')) # Output\n",
    "    weights_path= None\n",
    "    if weights_path is not None:\n",
    "        print(f\"load model weights_path: {weights_path}\")\n",
    "        model.load_weights(weights_path)\n",
    "    #model.layers.pop()\n",
    "    dr = model.layers[-2].output\n",
    "    model.trainable = False\n",
    "    left = Dense(14, activation=\"sigmoid\", name='leftLayer2')(dr)\n",
    "    right = Dense(14, activation=\"sigmoid\", name='rightLayer2')(dr)\n",
    "    model = Model(model.input, [left,model.output,right])\n",
    "    #model = Model(model.input, model.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network(model1,model2):\n",
    "    model = Model([model1.input,model2.input], [model1.output,model2.output])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** compute class weights from training data **\n",
      "2226: 9261\n",
      "741: 9261\n",
      "9261: 9261\n",
      "2801: 9261\n",
      "865: 9261\n",
      "645: 9261\n",
      "195: 9261\n",
      "711: 9261\n",
      "896: 9261\n",
      "437: 9261\n",
      "265: 9261\n",
      "126: 9261\n",
      "594: 9261\n",
      "9: 9261\n",
      "** class_weights **\n",
      "[{0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}]\n"
     ]
    }
   ],
   "source": [
    "# compute steps\n",
    "train_counts, train_pos_counts = get_sample_counts(output_dir, \"train\"+oneClass, class_names)\n",
    "dev_counts, _ = get_sample_counts(output_dir, \"dev\"+oneClass, class_names)\n",
    "    \n",
    "if train_steps == \"auto\":\n",
    "    train_steps = int(train_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        train_steps = int(train_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"train_steps: {train_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "    print(f\"** train_steps: {train_steps} **\")\n",
    "\n",
    "if validation_steps == \"auto\":\n",
    "    validation_steps = int(dev_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        validation_steps = int(validation_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"validation_steps: {validation_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        print(f\"** validation_steps: {validation_steps} **\")\n",
    "\n",
    "        # compute class weights\n",
    "keras.backend.clear_session()\n",
    "print(\"** compute class weights from training data **\")\n",
    "class_weights = get_class_weights(train_counts,train_pos_counts,multiply=positive_weights_multiply,)\n",
    "print(\"** class_weights **\")\n",
    "print(class_weights)\n",
    "#print(str(train_steps))\n",
    "#print(str(train_counts))\n",
    "#print(str(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** test_steps: 22433 **\n"
     ]
    }
   ],
   "source": [
    "test_steps = cp[\"TEST\"].get(\"test_steps\")\n",
    "test_counts, _ = get_sample_counts(output_dir, \"test\", class_names)\n",
    "\n",
    "if test_steps == \"auto\":\n",
    "    test_steps = int(test_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        test_steps = int(test_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"test_steps: {test_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        \n",
    "print(f\"** test_steps: {test_steps} **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"train\"+oneClass+\".csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=train_steps,\n",
    "        )\n",
    "validation_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"dev\"+oneClass+\".csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=validation_steps,\n",
    "            shuffle_on_epoch_end=False,\n",
    ")\n",
    "\n",
    "test_sequence = AugmentedImageSequence(\n",
    "        dataset_csv_file=os.path.join(output_dir, \"test.csv\"),\n",
    "        class_names=class_names,\n",
    "        source_image_dir=image_source_dir,\n",
    "        batch_size=batch_size,\n",
    "        target_size=(image_dimension, image_dimension),\n",
    "        augmenter=None,\n",
    "        steps=test_steps,\n",
    "        shuffle_on_epoch_end=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(lr):\n",
    "    gc.collect()\n",
    "      # Define the Keras TensorBoard callback.\n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    model1 = construct_network1()\n",
    "    model2 = construct_network2()\n",
    "    \n",
    "    optimizer = SGD(lr=initial_learning_rate)\n",
    "    \n",
    "    alpha = 0.9340456763831478\n",
    "    gamma = 1.4195808780694898\n",
    "    model1.compile(optimizer=optimizer,loss={'leftLayer1':tf.keras.losses.Huber(),'midLayer1':focal_loss(gamma=gamma,alpha=alpha),'rightLayer1':euclidean_distance_loss})\n",
    "\n",
    "    alpha = 0.7297456293468533\n",
    "    gamma = 1.2700405014991505\n",
    "    model2.compile(optimizer=optimizer,loss={'leftLayer2':tf.keras.losses.Huber(),'midLayer2':focal_loss(gamma=gamma,alpha=alpha),'rightLayer2':euclidean_distance_loss})\n",
    "  \n",
    "    model = construct_network(model1=model1,model2=model2)\n",
    "    model.compile(optimizer=optimizer,loss={'leftLayer1':tf.keras.losses.Huber(),'midLayer1':focal_loss(gamma=gamma,alpha=alpha),'rightLayer1':euclidean_distance_loss,'leftLayer2':tf.keras.losses.Huber(),'midLayer2':focal_loss(gamma=gamma,alpha=alpha),'rightLayer2':euclidean_distance_loss})\n",
    "\n",
    "    output_weights_path = os.path.join(output_dir,  str(lr)+\"_\"+output_weights_name)\n",
    "    \n",
    "    print(f\"** set output weights path to: {output_weights_path} **\")\n",
    "                  \n",
    "    \n",
    "                  \n",
    "    checkpoint = ModelCheckpoint(\n",
    "                 output_weights_path,\n",
    "                 save_weights_only=True,\n",
    "                 save_best_only=True,\n",
    "                 verbose=1,\n",
    "            )\n",
    "    start_time = time.time()\n",
    "  \n",
    "    model.summary()\n",
    "  \n",
    "    callbacks = [\n",
    "            checkpoint,\n",
    "            #keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "            #TensorBoard(log_dir=os.path.join(output_dir, \"logs\"), batch_size=batch_size),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience_reduce_lr,\n",
    "                              verbose=1, mode=\"min\", min_lr=min_lr), \n",
    "            EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    history = model.fit_generator(\n",
    "            generator=train_sequence,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_sequence,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=[class_weights,class_weights,class_weights,class_weights,class_weights,class_weights],\n",
    "            workers=generator_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        \n",
    "    y_hat = model.predict_generator(test_sequence, verbose=1)\n",
    "    y = test_sequence.get_y_true()\n",
    "    \n",
    "    test_log_path = os.path.join(output_dir, str(lr)+\"_\"+\"test.log\")\n",
    "    print(f\"** write log to {test_log_path} **\")\n",
    "    aurocs = []\n",
    "    auprcs = []\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    threshold = dict()\n",
    "    with open(test_log_path, \"w\") as f:\n",
    "        for k in range(6):\n",
    "            for i in range(len(class_names)):\n",
    "                 if(class_names[i] == str(oneClass)):\n",
    "                \n",
    "                    try:\n",
    "                        score = roc_auc_score(y[:, i], y_hat[k][:, i])\n",
    "                        precision[i], recall[i], threshold[i] = precision_recall_curve(y[:, i], y_hat[k][:, i])\n",
    "                        tmp = auc(recall[i], precision[i])\n",
    "                        aurocs.append(score)\n",
    "                        auprcs.append(tmp) \n",
    "                    except ValueError:\n",
    "                        score = 0\n",
    "               \n",
    "                    print(f\"auroc {str(k)+class_names[i]}: {score}\\n\")\n",
    "                    print(f\"auprc {str(k)+class_names[i]}: {tmp}\\n\")\n",
    "                    f.write(f\"auroc {str(k)+class_names[i]}: {score}\\n\")\n",
    "                    f.write(f\"auprc {str(k)+class_names[i]}: {tmp}\\n\")\n",
    "        \n",
    "        mean_auroc = np.mean(aurocs)\n",
    "        mean_auprc = float(np.mean(auprcs))\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(f\"mean auroc: {mean_auroc}\\n\")\n",
    "        print(f\"mean auroc: {mean_auroc}\\n\")\n",
    "        f.write(f\"mean auprc: {mean_auprc}\\n\")\n",
    "        print(f\"mean auprc: {mean_auprc}\\n\")\n",
    "        \n",
    "        max_auroc = np.max(aurocs)\n",
    "        max_auprc = float(np.max(auprcs))\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(f\"max auroc: {max_auroc}\\n\")\n",
    "        print(f\"max auroc: {max_auroc}\\n\")\n",
    "        f.write(f\"max auprc: {max_auprc}\\n\")\n",
    "        print(f\"max auprc: {max_auprc}\\n\")\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "    time_took = time.time() - start_time\n",
    "    \n",
    "    return max_auroc, time_took\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** set output weights path to: ./experiments/0.009_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-15-3539473a5eed>:58: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 9261 steps, validate for 1292 steps\n",
      "Epoch 1/11\n",
      "9260/9261 [============================>.] - ETA: 0s - loss: 5.5953 - leftLayer1_loss: 0.1115 - midLayer1_loss: 1.3411 - rightLayer1_loss: 1.4371 - leftLayer2_loss: 0.1031 - midLayer2_loss: 1.4293 - rightLayer2_loss: 1.1732\n",
      "Epoch 00001: val_loss improved from inf to 5.12440, saving model to ./experiments/0.009_weights.h5\n",
      "9261/9261 [==============================] - 26s 3ms/step - loss: 5.5954 - leftLayer1_loss: 0.1115 - midLayer1_loss: 1.3411 - rightLayer1_loss: 1.4371 - leftLayer2_loss: 0.1031 - midLayer2_loss: 1.4293 - rightLayer2_loss: 1.1733 - val_loss: 5.1244 - val_leftLayer1_loss: 0.1025 - val_midLayer1_loss: 1.3331 - val_rightLayer1_loss: 1.1449 - val_leftLayer2_loss: 0.0953 - val_midLayer2_loss: 1.3388 - val_rightLayer2_loss: 1.1098\n",
      "Epoch 2/11\n",
      "9252/9261 [============================>.] - ETA: 0s - loss: 4.8968 - leftLayer1_loss: 0.0953 - midLayer1_loss: 1.3413 - rightLayer1_loss: 1.0461 - leftLayer2_loss: 0.0718 - midLayer2_loss: 1.4277 - rightLayer2_loss: 0.9146\n",
      "Epoch 00002: val_loss improved from 5.12440 to 4.83418, saving model to ./experiments/0.009_weights.h5\n",
      "9261/9261 [==============================] - 25s 3ms/step - loss: 4.8965 - leftLayer1_loss: 0.0953 - midLayer1_loss: 1.3413 - rightLayer1_loss: 1.0460 - leftLayer2_loss: 0.0718 - midLayer2_loss: 1.4276 - rightLayer2_loss: 0.9145 - val_loss: 4.8342 - val_leftLayer1_loss: 0.0885 - val_midLayer1_loss: 1.3331 - val_rightLayer1_loss: 0.9792 - val_leftLayer2_loss: 0.0792 - val_midLayer2_loss: 1.3388 - val_rightLayer2_loss: 1.0154\n",
      "Epoch 3/11\n",
      "9256/9261 [============================>.] - ETA: 0s - loss: 4.7534 - leftLayer1_loss: 0.0830 - midLayer1_loss: 1.3412 - rightLayer1_loss: 0.9532 - leftLayer2_loss: 0.0571 - midLayer2_loss: 1.4296 - rightLayer2_loss: 0.8894\n",
      "Epoch 00003: val_loss improved from 4.83418 to 4.72457, saving model to ./experiments/0.009_weights.h5\n",
      "9261/9261 [==============================] - 25s 3ms/step - loss: 4.7536 - leftLayer1_loss: 0.0830 - midLayer1_loss: 1.3412 - rightLayer1_loss: 0.9532 - leftLayer2_loss: 0.0571 - midLayer2_loss: 1.4296 - rightLayer2_loss: 0.8894 - val_loss: 4.7246 - val_leftLayer1_loss: 0.0779 - val_midLayer1_loss: 1.3331 - val_rightLayer1_loss: 0.9271 - val_leftLayer2_loss: 0.0698 - val_midLayer2_loss: 1.3388 - val_rightLayer2_loss: 0.9779\n",
      "Epoch 4/11\n",
      "9241/9261 [============================>.] - ETA: 0s - loss: 4.6911 - leftLayer1_loss: 0.0738 - midLayer1_loss: 1.3411 - rightLayer1_loss: 0.9191 - leftLayer2_loss: 0.0495 - midLayer2_loss: 1.4279 - rightLayer2_loss: 0.8796\n",
      "Epoch 00004: val_loss improved from 4.72457 to 4.66578, saving model to ./experiments/0.009_weights.h5\n",
      "9261/9261 [==============================] - 25s 3ms/step - loss: 4.6906 - leftLayer1_loss: 0.0738 - midLayer1_loss: 1.3411 - rightLayer1_loss: 0.9190 - leftLayer2_loss: 0.0495 - midLayer2_loss: 1.4279 - rightLayer2_loss: 0.8794 - val_loss: 4.6658 - val_leftLayer1_loss: 0.0699 - val_midLayer1_loss: 1.3331 - val_rightLayer1_loss: 0.9032 - val_leftLayer2_loss: 0.0638 - val_midLayer2_loss: 1.3388 - val_rightLayer2_loss: 0.9570\n",
      "Epoch 5/11\n",
      "9255/9261 [============================>.] - ETA: 0s - loss: 4.6590 - leftLayer1_loss: 0.0668 - midLayer1_loss: 1.3409 - rightLayer1_loss: 0.9012 - leftLayer2_loss: 0.0453 - midLayer2_loss: 1.4300 - rightLayer2_loss: 0.8750\n",
      "Epoch 00005: val_loss improved from 4.66578 to 4.62827, saving model to ./experiments/0.009_weights.h5\n",
      "9261/9261 [==============================] - 25s 3ms/step - loss: 4.6592 - leftLayer1_loss: 0.0668 - midLayer1_loss: 1.3409 - rightLayer1_loss: 0.9012 - leftLayer2_loss: 0.0453 - midLayer2_loss: 1.4299 - rightLayer2_loss: 0.8751 - val_loss: 4.6283 - val_leftLayer1_loss: 0.0638 - val_midLayer1_loss: 1.3331 - val_rightLayer1_loss: 0.8899 - val_leftLayer2_loss: 0.0597 - val_midLayer2_loss: 1.3388 - val_rightLayer2_loss: 0.9430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/11\n",
      "9240/9261 [============================>.] - ETA: 0s - loss: 4.6383 - leftLayer1_loss: 0.0615 - midLayer1_loss: 1.3414 - rightLayer1_loss: 0.8914 - leftLayer2_loss: 0.0425 - midLayer2_loss: 1.4301 - rightLayer2_loss: 0.8715\n",
      "Epoch 00006: val_loss improved from 4.62827 to 4.60214, saving model to ./experiments/0.009_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.6379 - leftLayer1_loss: 0.0615 - midLayer1_loss: 1.3414 - rightLayer1_loss: 0.8912 - leftLayer2_loss: 0.0425 - midLayer2_loss: 1.4300 - rightLayer2_loss: 0.8713 - val_loss: 4.6021 - val_leftLayer1_loss: 0.0591 - val_midLayer1_loss: 1.3331 - val_rightLayer1_loss: 0.8814 - val_leftLayer2_loss: 0.0566 - val_midLayer2_loss: 1.3388 - val_rightLayer2_loss: 0.9332\n",
      "Epoch 7/11\n",
      "9252/9261 [============================>.] - ETA: 0s - loss: 4.6220 - leftLayer1_loss: 0.0573 - midLayer1_loss: 1.3412 - rightLayer1_loss: 0.8841 - leftLayer2_loss: 0.0407 - midLayer2_loss: 1.4286 - rightLayer2_loss: 0.8701\n",
      "Epoch 00007: val_loss improved from 4.60214 to 4.58270, saving model to ./experiments/0.009_weights.h5\n",
      "9261/9261 [==============================] - 23s 2ms/step - loss: 4.6217 - leftLayer1_loss: 0.0573 - midLayer1_loss: 1.3412 - rightLayer1_loss: 0.8841 - leftLayer2_loss: 0.0407 - midLayer2_loss: 1.4285 - rightLayer2_loss: 0.8699 - val_loss: 4.5827 - val_leftLayer1_loss: 0.0554 - val_midLayer1_loss: 1.3331 - val_rightLayer1_loss: 0.8755 - val_leftLayer2_loss: 0.0543 - val_midLayer2_loss: 1.3388 - val_rightLayer2_loss: 0.9257\n",
      "Epoch 8/11\n",
      "9251/9261 [============================>.] - ETA: 0s - loss: 4.6130 - leftLayer1_loss: 0.0540 - midLayer1_loss: 1.3412 - rightLayer1_loss: 0.8790 - leftLayer2_loss: 0.0394 - midLayer2_loss: 1.4305 - rightLayer2_loss: 0.8689\n",
      "Epoch 00008: val_loss improved from 4.58270 to 4.56757, saving model to ./experiments/0.009_weights.h5\n",
      "9261/9261 [==============================] - 23s 2ms/step - loss: 4.6129 - leftLayer1_loss: 0.0540 - midLayer1_loss: 1.3412 - rightLayer1_loss: 0.8790 - leftLayer2_loss: 0.0394 - midLayer2_loss: 1.4305 - rightLayer2_loss: 0.8688 - val_loss: 4.5676 - val_leftLayer1_loss: 0.0524 - val_midLayer1_loss: 1.3331 - val_rightLayer1_loss: 0.8711 - val_leftLayer2_loss: 0.0525 - val_midLayer2_loss: 1.3388 - val_rightLayer2_loss: 0.9197\n",
      "Epoch 9/11\n",
      "9255/9261 [============================>.] - ETA: 0s - loss: 4.6059 - leftLayer1_loss: 0.0513 - midLayer1_loss: 1.3409 - rightLayer1_loss: 0.8751 - leftLayer2_loss: 0.0385 - midLayer2_loss: 1.4323 - rightLayer2_loss: 0.8677\n",
      "Epoch 00009: val_loss improved from 4.56757 to 4.55541, saving model to ./experiments/0.009_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.6062 - leftLayer1_loss: 0.0513 - midLayer1_loss: 1.3409 - rightLayer1_loss: 0.8752 - leftLayer2_loss: 0.0385 - midLayer2_loss: 1.4324 - rightLayer2_loss: 0.8678 - val_loss: 4.5554 - val_leftLayer1_loss: 0.0500 - val_midLayer1_loss: 1.3331 - val_rightLayer1_loss: 0.8678 - val_leftLayer2_loss: 0.0511 - val_midLayer2_loss: 1.3388 - val_rightLayer2_loss: 0.9147\n",
      "Epoch 10/11\n",
      "9245/9261 [============================>.] - ETA: 0s - loss: 4.5970 - leftLayer1_loss: 0.0492 - midLayer1_loss: 1.3414 - rightLayer1_loss: 0.8726 - leftLayer2_loss: 0.0378 - midLayer2_loss: 1.4301 - rightLayer2_loss: 0.8660\n",
      "Epoch 00010: val_loss improved from 4.55541 to 4.54560, saving model to ./experiments/0.009_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.5968 - leftLayer1_loss: 0.0492 - midLayer1_loss: 1.3414 - rightLayer1_loss: 0.8725 - leftLayer2_loss: 0.0378 - midLayer2_loss: 1.4301 - rightLayer2_loss: 0.8659 - val_loss: 4.5456 - val_leftLayer1_loss: 0.0480 - val_midLayer1_loss: 1.3331 - val_rightLayer1_loss: 0.8651 - val_leftLayer2_loss: 0.0499 - val_midLayer2_loss: 1.3388 - val_rightLayer2_loss: 0.9107\n",
      "Epoch 11/11\n",
      "9240/9261 [============================>.] - ETA: 0s - loss: 4.5914 - leftLayer1_loss: 0.0474 - midLayer1_loss: 1.3411 - rightLayer1_loss: 0.8702 - leftLayer2_loss: 0.0373 - midLayer2_loss: 1.4303 - rightLayer2_loss: 0.8652\n",
      "Epoch 00011: val_loss improved from 4.54560 to 4.53740, saving model to ./experiments/0.009_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.5911 - leftLayer1_loss: 0.0474 - midLayer1_loss: 1.3411 - rightLayer1_loss: 0.8700 - leftLayer2_loss: 0.0372 - midLayer2_loss: 1.4304 - rightLayer2_loss: 0.8650 - val_loss: 4.5374 - val_leftLayer1_loss: 0.0464 - val_midLayer1_loss: 1.3331 - val_rightLayer1_loss: 0.8629 - val_leftLayer2_loss: 0.0488 - val_midLayer2_loss: 1.3388 - val_rightLayer2_loss: 0.9074\n",
      "WARNING:tensorflow:From <ipython-input-15-3539473a5eed>:61: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.009_test.log **\n",
      "auroc 0Effusion: 0.8031153185829366\n",
      "\n",
      "auprc 0Effusion: 0.33999636181784687\n",
      "\n",
      "auroc 1Effusion: 0.3670276031245573\n",
      "\n",
      "auprc 1Effusion: 0.09120383853643595\n",
      "\n",
      "auroc 2Effusion: 0.6333064900070238\n",
      "\n",
      "auprc 2Effusion: 0.19113586510498232\n",
      "\n",
      "auroc 3Effusion: 0.5329991811567673\n",
      "\n",
      "auprc 3Effusion: 0.1362425474229007\n",
      "\n",
      "auroc 4Effusion: 0.3513337690115165\n",
      "\n",
      "auprc 4Effusion: 0.0871345962738808\n",
      "\n",
      "auroc 5Effusion: 0.42965146151283656\n",
      "\n",
      "auprc 5Effusion: 0.0991416193537123\n",
      "\n",
      "mean auroc: 0.519572303899273\n",
      "\n",
      "mean auprc: 0.15747580475162648\n",
      "\n",
      "max auroc: 0.8031153185829366\n",
      "\n",
      "max auprc: 0.33999636181784687\n",
      "\n",
      "293.7312891483307\n",
      "** set output weights path to: ./experiments/0.009999999999999998_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 9261 steps, validate for 1292 steps\n",
      "Epoch 1/11\n",
      "9253/9261 [============================>.] - ETA: 0s - loss: 5.6640 - leftLayer1_loss: 0.1127 - midLayer1_loss: 1.4261 - rightLayer1_loss: 1.4110 - leftLayer2_loss: 0.1013 - midLayer2_loss: 1.4653 - rightLayer2_loss: 1.1477\n",
      "Epoch 00001: val_loss improved from inf to 5.25270, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 5.6635 - leftLayer1_loss: 0.1126 - midLayer1_loss: 1.4261 - rightLayer1_loss: 1.4108 - leftLayer2_loss: 0.1013 - midLayer2_loss: 1.4653 - rightLayer2_loss: 1.1475 - val_loss: 5.2527 - val_leftLayer1_loss: 0.1031 - val_midLayer1_loss: 1.4216 - val_rightLayer1_loss: 1.1240 - val_leftLayer2_loss: 0.0935 - val_midLayer2_loss: 1.4056 - val_rightLayer2_loss: 1.1050\n",
      "Epoch 2/11\n",
      "9245/9261 [============================>.] - ETA: 0s - loss: 4.9941 - leftLayer1_loss: 0.0951 - midLayer1_loss: 1.4265 - rightLayer1_loss: 1.0312 - leftLayer2_loss: 0.0695 - midLayer2_loss: 1.4628 - rightLayer2_loss: 0.9091\n",
      "Epoch 00002: val_loss improved from 5.25270 to 4.97556, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.9938 - leftLayer1_loss: 0.0951 - midLayer1_loss: 1.4264 - rightLayer1_loss: 1.0310 - leftLayer2_loss: 0.0695 - midLayer2_loss: 1.4628 - rightLayer2_loss: 0.9089 - val_loss: 4.9756 - val_leftLayer1_loss: 0.0880 - val_midLayer1_loss: 1.4216 - val_rightLayer1_loss: 0.9699 - val_leftLayer2_loss: 0.0773 - val_midLayer2_loss: 1.4056 - val_rightLayer2_loss: 1.0132\n",
      "Epoch 3/11\n",
      "9259/9261 [============================>.] - ETA: 0s - loss: 4.8606 - leftLayer1_loss: 0.0821 - midLayer1_loss: 1.4268 - rightLayer1_loss: 0.9460 - leftLayer2_loss: 0.0553 - midLayer2_loss: 1.4639 - rightLayer2_loss: 0.8866\n",
      "Epoch 00003: val_loss improved from 4.97556 to 4.87070, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.8607 - leftLayer1_loss: 0.0821 - midLayer1_loss: 1.4269 - rightLayer1_loss: 0.9460 - leftLayer2_loss: 0.0553 - midLayer2_loss: 1.4639 - rightLayer2_loss: 0.8866 - val_loss: 4.8707 - val_leftLayer1_loss: 0.0769 - val_midLayer1_loss: 1.4216 - val_rightLayer1_loss: 0.9222 - val_leftLayer2_loss: 0.0680 - val_midLayer2_loss: 1.4056 - val_rightLayer2_loss: 0.9765\n",
      "Epoch 4/11\n",
      "9240/9261 [============================>.] - ETA: 0s - loss: 4.8029 - leftLayer1_loss: 0.0725 - midLayer1_loss: 1.4265 - rightLayer1_loss: 0.9149 - leftLayer2_loss: 0.0483 - midLayer2_loss: 1.4623 - rightLayer2_loss: 0.8784\n",
      "Epoch 00004: val_loss improved from 4.87070 to 4.81413, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.8026 - leftLayer1_loss: 0.0724 - midLayer1_loss: 1.4265 - rightLayer1_loss: 0.9148 - leftLayer2_loss: 0.0483 - midLayer2_loss: 1.4624 - rightLayer2_loss: 0.8782 - val_loss: 4.8141 - val_leftLayer1_loss: 0.0686 - val_midLayer1_loss: 1.4216 - val_rightLayer1_loss: 0.9004 - val_leftLayer2_loss: 0.0621 - val_midLayer2_loss: 1.4056 - val_rightLayer2_loss: 0.9558\n",
      "Epoch 5/11\n",
      "9255/9261 [============================>.] - ETA: 0s - loss: 4.7727 - leftLayer1_loss: 0.0653 - midLayer1_loss: 1.4263 - rightLayer1_loss: 0.8990 - leftLayer2_loss: 0.0442 - midLayer2_loss: 1.4640 - rightLayer2_loss: 0.8739\n",
      "Epoch 00005: val_loss improved from 4.81413 to 4.77833, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.7730 - leftLayer1_loss: 0.0653 - midLayer1_loss: 1.4263 - rightLayer1_loss: 0.8991 - leftLayer2_loss: 0.0442 - midLayer2_loss: 1.4641 - rightLayer2_loss: 0.8739 - val_loss: 4.7783 - val_leftLayer1_loss: 0.0624 - val_midLayer1_loss: 1.4216 - val_rightLayer1_loss: 0.8881 - val_leftLayer2_loss: 0.0581 - val_midLayer2_loss: 1.4056 - val_rightLayer2_loss: 0.9425\n",
      "Epoch 6/11\n",
      "9256/9261 [============================>.] - ETA: 0s - loss: 4.7531 - leftLayer1_loss: 0.0599 - midLayer1_loss: 1.4267 - rightLayer1_loss: 0.8896 - leftLayer2_loss: 0.0417 - midLayer2_loss: 1.4635 - rightLayer2_loss: 0.8717\n",
      "Epoch 00006: val_loss improved from 4.77833 to 4.75314, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.7533 - leftLayer1_loss: 0.0599 - midLayer1_loss: 1.4267 - rightLayer1_loss: 0.8897 - leftLayer2_loss: 0.0417 - midLayer2_loss: 1.4635 - rightLayer2_loss: 0.8718 - val_loss: 4.7531 - val_leftLayer1_loss: 0.0577 - val_midLayer1_loss: 1.4216 - val_rightLayer1_loss: 0.8803 - val_leftLayer2_loss: 0.0552 - val_midLayer2_loss: 1.4056 - val_rightLayer2_loss: 0.9328\n",
      "Epoch 7/11\n",
      "9242/9261 [============================>.] - ETA: 0s - loss: 4.7411 - leftLayer1_loss: 0.0558 - midLayer1_loss: 1.4272 - rightLayer1_loss: 0.8838 - leftLayer2_loss: 0.0400 - midLayer2_loss: 1.4650 - rightLayer2_loss: 0.8692\n",
      "Epoch 00007: val_loss improved from 4.75314 to 4.73463, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.7406 - leftLayer1_loss: 0.0558 - midLayer1_loss: 1.4272 - rightLayer1_loss: 0.8837 - leftLayer2_loss: 0.0400 - midLayer2_loss: 1.4650 - rightLayer2_loss: 0.8691 - val_loss: 4.7346 - val_leftLayer1_loss: 0.0540 - val_midLayer1_loss: 1.4216 - val_rightLayer1_loss: 0.8749 - val_leftLayer2_loss: 0.0530 - val_midLayer2_loss: 1.4056 - val_rightLayer2_loss: 0.9256\n",
      "Epoch 8/11\n",
      "9254/9261 [============================>.] - ETA: 0s - loss: 4.7298 - leftLayer1_loss: 0.0525 - midLayer1_loss: 1.4265 - rightLayer1_loss: 0.8789 - leftLayer2_loss: 0.0389 - midLayer2_loss: 1.4648 - rightLayer2_loss: 0.8683\n",
      "Epoch 00008: val_loss improved from 4.73463 to 4.72027, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.7298 - leftLayer1_loss: 0.0525 - midLayer1_loss: 1.4265 - rightLayer1_loss: 0.8789 - leftLayer2_loss: 0.0389 - midLayer2_loss: 1.4648 - rightLayer2_loss: 0.8683 - val_loss: 4.7203 - val_leftLayer1_loss: 0.0511 - val_midLayer1_loss: 1.4216 - val_rightLayer1_loss: 0.8709 - val_leftLayer2_loss: 0.0513 - val_midLayer2_loss: 1.4056 - val_rightLayer2_loss: 0.9198\n",
      "Epoch 9/11\n",
      "9242/9261 [============================>.] - ETA: 0s - loss: 4.7188 - leftLayer1_loss: 0.0499 - midLayer1_loss: 1.4261 - rightLayer1_loss: 0.8755 - leftLayer2_loss: 0.0380 - midLayer2_loss: 1.4625 - rightLayer2_loss: 0.8669\n",
      "Epoch 00009: val_loss improved from 4.72027 to 4.70890, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.7184 - leftLayer1_loss: 0.0499 - midLayer1_loss: 1.4260 - rightLayer1_loss: 0.8753 - leftLayer2_loss: 0.0380 - midLayer2_loss: 1.4624 - rightLayer2_loss: 0.8667 - val_loss: 4.7089 - val_leftLayer1_loss: 0.0488 - val_midLayer1_loss: 1.4216 - val_rightLayer1_loss: 0.8678 - val_leftLayer2_loss: 0.0499 - val_midLayer2_loss: 1.4056 - val_rightLayer2_loss: 0.9153\n",
      "Epoch 10/11\n",
      "9242/9261 [============================>.] - ETA: 0s - loss: 4.7165 - leftLayer1_loss: 0.0479 - midLayer1_loss: 1.4268 - rightLayer1_loss: 0.8730 - leftLayer2_loss: 0.0374 - midLayer2_loss: 1.4658 - rightLayer2_loss: 0.8657\n",
      "Epoch 00010: val_loss improved from 4.70890 to 4.69970, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.7161 - leftLayer1_loss: 0.0479 - midLayer1_loss: 1.4267 - rightLayer1_loss: 0.8728 - leftLayer2_loss: 0.0374 - midLayer2_loss: 1.4658 - rightLayer2_loss: 0.8656 - val_loss: 4.6997 - val_leftLayer1_loss: 0.0469 - val_midLayer1_loss: 1.4216 - val_rightLayer1_loss: 0.8653 - val_leftLayer2_loss: 0.0487 - val_midLayer2_loss: 1.4056 - val_rightLayer2_loss: 0.9116\n",
      "Epoch 11/11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9244/9261 [============================>.] - ETA: 0s - loss: 4.7078 - leftLayer1_loss: 0.0461 - midLayer1_loss: 1.4268 - rightLayer1_loss: 0.8707 - leftLayer2_loss: 0.0368 - midLayer2_loss: 1.4626 - rightLayer2_loss: 0.8647\n",
      "Epoch 00011: val_loss improved from 4.69970 to 4.69208, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.7073 - leftLayer1_loss: 0.0461 - midLayer1_loss: 1.4267 - rightLayer1_loss: 0.8705 - leftLayer2_loss: 0.0368 - midLayer2_loss: 1.4626 - rightLayer2_loss: 0.8645 - val_loss: 4.6921 - val_leftLayer1_loss: 0.0453 - val_midLayer1_loss: 1.4216 - val_rightLayer1_loss: 0.8633 - val_leftLayer2_loss: 0.0478 - val_midLayer2_loss: 1.4056 - val_rightLayer2_loss: 0.9085\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.009999999999999998_test.log **\n",
      "auroc 0Effusion: 0.48737823032806543\n",
      "\n",
      "auprc 0Effusion: 0.10955458332938339\n",
      "\n",
      "auroc 1Effusion: 0.7641201376500975\n",
      "\n",
      "auprc 1Effusion: 0.2579185623350266\n",
      "\n",
      "auroc 2Effusion: 0.5294849159068408\n",
      "\n",
      "auprc 2Effusion: 0.11925661483152925\n",
      "\n",
      "auroc 3Effusion: 0.4416261073748552\n",
      "\n",
      "auprc 3Effusion: 0.10190729650956838\n",
      "\n",
      "auroc 4Effusion: 0.5900382142833287\n",
      "\n",
      "auprc 4Effusion: 0.16021021037980054\n",
      "\n",
      "auroc 5Effusion: 0.5744038310895685\n",
      "\n",
      "auprc 5Effusion: 0.14029375922174245\n",
      "\n",
      "mean auroc: 0.564508572772126\n",
      "\n",
      "mean auprc: 0.1481901711011751\n",
      "\n",
      "max auroc: 0.7641201376500975\n",
      "\n",
      "max auprc: 0.2579185623350266\n",
      "\n",
      "287.96788263320923\n",
      "** set output weights path to: ./experiments/0.010999999999999998_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 9261 steps, validate for 1292 steps\n",
      "Epoch 1/11\n",
      "9250/9261 [============================>.] - ETA: 0s - loss: 5.4868 - leftLayer1_loss: 0.1149 - midLayer1_loss: 1.3536 - rightLayer1_loss: 1.3920 - leftLayer2_loss: 0.1029 - midLayer2_loss: 1.3868 - rightLayer2_loss: 1.1367\n",
      "Epoch 00001: val_loss improved from inf to 5.08934, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 5.4859 - leftLayer1_loss: 0.1149 - midLayer1_loss: 1.3536 - rightLayer1_loss: 1.3916 - leftLayer2_loss: 0.1028 - midLayer2_loss: 1.3867 - rightLayer2_loss: 1.1364 - val_loss: 5.0893 - val_leftLayer1_loss: 0.1037 - val_midLayer1_loss: 1.3470 - val_rightLayer1_loss: 1.0939 - val_leftLayer2_loss: 0.0955 - val_midLayer2_loss: 1.3327 - val_rightLayer2_loss: 1.1166\n",
      "Epoch 2/11\n",
      "9254/9261 [============================>.] - ETA: 0s - loss: 4.8254 - leftLayer1_loss: 0.0949 - midLayer1_loss: 1.3538 - rightLayer1_loss: 1.0106 - leftLayer2_loss: 0.0709 - midLayer2_loss: 1.3842 - rightLayer2_loss: 0.9111\n",
      "Epoch 00002: val_loss improved from 5.08934 to 4.82334, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.8253 - leftLayer1_loss: 0.0949 - midLayer1_loss: 1.3538 - rightLayer1_loss: 1.0105 - leftLayer2_loss: 0.0709 - midLayer2_loss: 1.3841 - rightLayer2_loss: 0.9111 - val_loss: 4.8233 - val_leftLayer1_loss: 0.0869 - val_midLayer1_loss: 1.3470 - val_rightLayer1_loss: 0.9540 - val_leftLayer2_loss: 0.0794 - val_midLayer2_loss: 1.3327 - val_rightLayer2_loss: 1.0234\n",
      "Epoch 3/11\n",
      "9252/9261 [============================>.] - ETA: 0s - loss: 4.6988 - leftLayer1_loss: 0.0807 - midLayer1_loss: 1.3537 - rightLayer1_loss: 0.9351 - leftLayer2_loss: 0.0563 - midLayer2_loss: 1.3844 - rightLayer2_loss: 0.8885\n",
      "Epoch 00003: val_loss improved from 4.82334 to 4.72193, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.6984 - leftLayer1_loss: 0.0807 - midLayer1_loss: 1.3537 - rightLayer1_loss: 0.9350 - leftLayer2_loss: 0.0563 - midLayer2_loss: 1.3844 - rightLayer2_loss: 0.8884 - val_loss: 4.7219 - val_leftLayer1_loss: 0.0749 - val_midLayer1_loss: 1.3470 - val_rightLayer1_loss: 0.9123 - val_leftLayer2_loss: 0.0701 - val_midLayer2_loss: 1.3327 - val_rightLayer2_loss: 0.9850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/11\n",
      "9259/9261 [============================>.] - ETA: 0s - loss: 4.6442 - leftLayer1_loss: 0.0705 - midLayer1_loss: 1.3535 - rightLayer1_loss: 0.9076 - leftLayer2_loss: 0.0490 - midLayer2_loss: 1.3845 - rightLayer2_loss: 0.8790\n",
      "Epoch 00004: val_loss improved from 4.72193 to 4.66680, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 23s 2ms/step - loss: 4.6443 - leftLayer1_loss: 0.0705 - midLayer1_loss: 1.3535 - rightLayer1_loss: 0.9076 - leftLayer2_loss: 0.0490 - midLayer2_loss: 1.3846 - rightLayer2_loss: 0.8790 - val_loss: 4.6668 - val_leftLayer1_loss: 0.0664 - val_midLayer1_loss: 1.3470 - val_rightLayer1_loss: 0.8934 - val_leftLayer2_loss: 0.0641 - val_midLayer2_loss: 1.3327 - val_rightLayer2_loss: 0.9633\n",
      "Epoch 5/11\n",
      "9246/9261 [============================>.] - ETA: 0s - loss: 4.6108 - leftLayer1_loss: 0.0632 - midLayer1_loss: 1.3542 - rightLayer1_loss: 0.8942 - leftLayer2_loss: 0.0447 - midLayer2_loss: 1.3801 - rightLayer2_loss: 0.8744\n",
      "Epoch 00005: val_loss improved from 4.66680 to 4.63174, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.6106 - leftLayer1_loss: 0.0632 - midLayer1_loss: 1.3542 - rightLayer1_loss: 0.8942 - leftLayer2_loss: 0.0447 - midLayer2_loss: 1.3800 - rightLayer2_loss: 0.8743 - val_loss: 4.6317 - val_leftLayer1_loss: 0.0602 - val_midLayer1_loss: 1.3470 - val_rightLayer1_loss: 0.8828 - val_leftLayer2_loss: 0.0599 - val_midLayer2_loss: 1.3327 - val_rightLayer2_loss: 0.9492\n",
      "Epoch 6/11\n",
      "9255/9261 [============================>.] - ETA: 0s - loss: 4.5958 - leftLayer1_loss: 0.0578 - midLayer1_loss: 1.3539 - rightLayer1_loss: 0.8855 - leftLayer2_loss: 0.0421 - midLayer2_loss: 1.3847 - rightLayer2_loss: 0.8718\n",
      "Epoch 00006: val_loss improved from 4.63174 to 4.60702, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.5960 - leftLayer1_loss: 0.0578 - midLayer1_loss: 1.3539 - rightLayer1_loss: 0.8856 - leftLayer2_loss: 0.0421 - midLayer2_loss: 1.3847 - rightLayer2_loss: 0.8719 - val_loss: 4.6070 - val_leftLayer1_loss: 0.0555 - val_midLayer1_loss: 1.3470 - val_rightLayer1_loss: 0.8760 - val_leftLayer2_loss: 0.0569 - val_midLayer2_loss: 1.3327 - val_rightLayer2_loss: 0.9390\n",
      "Epoch 7/11\n",
      "9241/9261 [============================>.] - ETA: 0s - loss: 4.5812 - leftLayer1_loss: 0.0538 - midLayer1_loss: 1.3541 - rightLayer1_loss: 0.8801 - leftLayer2_loss: 0.0404 - midLayer2_loss: 1.3827 - rightLayer2_loss: 0.8701\n",
      "Epoch 00007: val_loss improved from 4.60702 to 4.58875, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 23s 2ms/step - loss: 4.5805 - leftLayer1_loss: 0.0538 - midLayer1_loss: 1.3540 - rightLayer1_loss: 0.8799 - leftLayer2_loss: 0.0404 - midLayer2_loss: 1.3825 - rightLayer2_loss: 0.8698 - val_loss: 4.5887 - val_leftLayer1_loss: 0.0520 - val_midLayer1_loss: 1.3470 - val_rightLayer1_loss: 0.8713 - val_leftLayer2_loss: 0.0546 - val_midLayer2_loss: 1.3327 - val_rightLayer2_loss: 0.9312\n",
      "Epoch 8/11\n",
      "9260/9261 [============================>.] - ETA: 0s - loss: 4.5714 - leftLayer1_loss: 0.0507 - midLayer1_loss: 1.3540 - rightLayer1_loss: 0.8757 - leftLayer2_loss: 0.0391 - midLayer2_loss: 1.3841 - rightLayer2_loss: 0.8678\n",
      "Epoch 00008: val_loss improved from 4.58875 to 4.57466, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.5716 - leftLayer1_loss: 0.0507 - midLayer1_loss: 1.3540 - rightLayer1_loss: 0.8758 - leftLayer2_loss: 0.0392 - midLayer2_loss: 1.3841 - rightLayer2_loss: 0.8679 - val_loss: 4.5747 - val_leftLayer1_loss: 0.0492 - val_midLayer1_loss: 1.3470 - val_rightLayer1_loss: 0.8678 - val_leftLayer2_loss: 0.0528 - val_midLayer2_loss: 1.3327 - val_rightLayer2_loss: 0.9252\n",
      "Epoch 9/11\n",
      "9238/9261 [============================>.] - ETA: 0s - loss: 4.5624 - leftLayer1_loss: 0.0483 - midLayer1_loss: 1.3534 - rightLayer1_loss: 0.8731 - leftLayer2_loss: 0.0383 - midLayer2_loss: 1.3818 - rightLayer2_loss: 0.8675\n",
      "Epoch 00009: val_loss improved from 4.57466 to 4.56336, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.5620 - leftLayer1_loss: 0.0482 - midLayer1_loss: 1.3534 - rightLayer1_loss: 0.8730 - leftLayer2_loss: 0.0383 - midLayer2_loss: 1.3818 - rightLayer2_loss: 0.8674 - val_loss: 4.5634 - val_leftLayer1_loss: 0.0470 - val_midLayer1_loss: 1.3470 - val_rightLayer1_loss: 0.8651 - val_leftLayer2_loss: 0.0514 - val_midLayer2_loss: 1.3327 - val_rightLayer2_loss: 0.9202\n",
      "Epoch 10/11\n",
      "9243/9261 [============================>.] - ETA: 0s - loss: 4.5590 - leftLayer1_loss: 0.0463 - midLayer1_loss: 1.3542 - rightLayer1_loss: 0.8707 - leftLayer2_loss: 0.0376 - midLayer2_loss: 1.3843 - rightLayer2_loss: 0.8659\n",
      "Epoch 00010: val_loss improved from 4.56336 to 4.55418, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.5586 - leftLayer1_loss: 0.0463 - midLayer1_loss: 1.3541 - rightLayer1_loss: 0.8705 - leftLayer2_loss: 0.0376 - midLayer2_loss: 1.3843 - rightLayer2_loss: 0.8658 - val_loss: 4.5542 - val_leftLayer1_loss: 0.0453 - val_midLayer1_loss: 1.3470 - val_rightLayer1_loss: 0.8629 - val_leftLayer2_loss: 0.0501 - val_midLayer2_loss: 1.3327 - val_rightLayer2_loss: 0.9162\n",
      "Epoch 11/11\n",
      "9242/9261 [============================>.] - ETA: 0s - loss: 4.5534 - leftLayer1_loss: 0.0447 - midLayer1_loss: 1.3538 - rightLayer1_loss: 0.8686 - leftLayer2_loss: 0.0371 - midLayer2_loss: 1.3842 - rightLayer2_loss: 0.8650\n",
      "Epoch 00011: val_loss improved from 4.55418 to 4.54648, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.5531 - leftLayer1_loss: 0.0447 - midLayer1_loss: 1.3537 - rightLayer1_loss: 0.8684 - leftLayer2_loss: 0.0371 - midLayer2_loss: 1.3843 - rightLayer2_loss: 0.8648 - val_loss: 4.5465 - val_leftLayer1_loss: 0.0439 - val_midLayer1_loss: 1.3470 - val_rightLayer1_loss: 0.8612 - val_leftLayer2_loss: 0.0491 - val_midLayer2_loss: 1.3327 - val_rightLayer2_loss: 0.9127\n",
      "22433/22433 [==============================] - 27s 1ms/step\n",
      "** write log to ./experiments/0.010999999999999998_test.log **\n",
      "auroc 0Effusion: 0.6170484718364464\n",
      "\n",
      "auprc 0Effusion: 0.17056599272656736\n",
      "\n",
      "auroc 1Effusion: 0.7734774577133656\n",
      "\n",
      "auprc 1Effusion: 0.31485539188662115\n",
      "\n",
      "auroc 2Effusion: 0.6210946032403962\n",
      "\n",
      "auprc 2Effusion: 0.1824269173537582\n",
      "\n",
      "auroc 3Effusion: 0.41755087454295026\n",
      "\n",
      "auprc 3Effusion: 0.09846858089750364\n",
      "\n",
      "auroc 4Effusion: 0.6016468642703038\n",
      "\n",
      "auprc 4Effusion: 0.15216484726015048\n",
      "\n",
      "auroc 5Effusion: 0.3498570262591131\n",
      "\n",
      "auprc 5Effusion: 0.08714605668420287\n",
      "\n",
      "mean auroc: 0.563445882977096\n",
      "\n",
      "mean auprc: 0.16760463113480062\n",
      "\n",
      "max auroc: 0.7734774577133656\n",
      "\n",
      "max auprc: 0.31485539188662115\n",
      "\n",
      "280.11384320259094\n",
      "** set output weights path to: ./experiments/0.011999999999999997_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 9261 steps, validate for 1292 steps\n",
      "Epoch 1/11\n",
      "9240/9261 [============================>.] - ETA: 0s - loss: 5.6459 - leftLayer1_loss: 0.1143 - midLayer1_loss: 1.3694 - rightLayer1_loss: 1.4228 - leftLayer2_loss: 0.1045 - midLayer2_loss: 1.4750 - rightLayer2_loss: 1.1598\n",
      "Epoch 00001: val_loss improved from inf to 5.13608, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "9261/9261 [==============================] - 23s 2ms/step - loss: 5.6442 - leftLayer1_loss: 0.1143 - midLayer1_loss: 1.3693 - rightLayer1_loss: 1.4220 - leftLayer2_loss: 0.1045 - midLayer2_loss: 1.4749 - rightLayer2_loss: 1.1592 - val_loss: 5.1361 - val_leftLayer1_loss: 0.1039 - val_midLayer1_loss: 1.3663 - val_rightLayer1_loss: 1.1162 - val_leftLayer2_loss: 0.0950 - val_midLayer2_loss: 1.3575 - val_rightLayer2_loss: 1.0972\n",
      "Epoch 2/11\n",
      "9248/9261 [============================>.] - ETA: 0s - loss: 4.9514 - leftLayer1_loss: 0.0957 - midLayer1_loss: 1.3696 - rightLayer1_loss: 1.0278 - leftLayer2_loss: 0.0714 - midLayer2_loss: 1.4751 - rightLayer2_loss: 0.9118\n",
      "Epoch 00002: val_loss improved from 5.13608 to 4.86163, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.9512 - leftLayer1_loss: 0.0957 - midLayer1_loss: 1.3696 - rightLayer1_loss: 1.0276 - leftLayer2_loss: 0.0714 - midLayer2_loss: 1.4752 - rightLayer2_loss: 0.9116 - val_loss: 4.8616 - val_leftLayer1_loss: 0.0880 - val_midLayer1_loss: 1.3663 - val_rightLayer1_loss: 0.9642 - val_leftLayer2_loss: 0.0783 - val_midLayer2_loss: 1.3575 - val_rightLayer2_loss: 1.0074\n",
      "Epoch 3/11\n",
      "9238/9261 [============================>.] - ETA: 0s - loss: 4.8103 - leftLayer1_loss: 0.0821 - midLayer1_loss: 1.3696 - rightLayer1_loss: 0.9443 - leftLayer2_loss: 0.0564 - midLayer2_loss: 1.4701 - rightLayer2_loss: 0.8879\n",
      "Epoch 00003: val_loss improved from 4.86163 to 4.75878, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.8099 - leftLayer1_loss: 0.0821 - midLayer1_loss: 1.3695 - rightLayer1_loss: 0.9441 - leftLayer2_loss: 0.0564 - midLayer2_loss: 1.4699 - rightLayer2_loss: 0.8878 - val_loss: 4.7588 - val_leftLayer1_loss: 0.0764 - val_midLayer1_loss: 1.3663 - val_rightLayer1_loss: 0.9181 - val_leftLayer2_loss: 0.0689 - val_midLayer2_loss: 1.3575 - val_rightLayer2_loss: 0.9716\n",
      "Epoch 4/11\n",
      "9242/9261 [============================>.] - ETA: 0s - loss: 4.7574 - leftLayer1_loss: 0.0722 - midLayer1_loss: 1.3700 - rightLayer1_loss: 0.9137 - leftLayer2_loss: 0.0491 - midLayer2_loss: 1.4725 - rightLayer2_loss: 0.8799\n",
      "Epoch 00004: val_loss improved from 4.75878 to 4.70334, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "9261/9261 [==============================] - 26s 3ms/step - loss: 4.7570 - leftLayer1_loss: 0.0722 - midLayer1_loss: 1.3700 - rightLayer1_loss: 0.9136 - leftLayer2_loss: 0.0491 - midLayer2_loss: 1.4725 - rightLayer2_loss: 0.8797 - val_loss: 4.7033 - val_leftLayer1_loss: 0.0680 - val_midLayer1_loss: 1.3663 - val_rightLayer1_loss: 0.8972 - val_leftLayer2_loss: 0.0629 - val_midLayer2_loss: 1.3575 - val_rightLayer2_loss: 0.9515\n",
      "Epoch 5/11\n",
      "9244/9261 [============================>.] - ETA: 0s - loss: 4.7232 - leftLayer1_loss: 0.0649 - midLayer1_loss: 1.3699 - rightLayer1_loss: 0.8984 - leftLayer2_loss: 0.0448 - midLayer2_loss: 1.4703 - rightLayer2_loss: 0.8749\n",
      "Epoch 00005: val_loss improved from 4.70334 to 4.66817, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "9261/9261 [==============================] - 26s 3ms/step - loss: 4.7227 - leftLayer1_loss: 0.0649 - midLayer1_loss: 1.3698 - rightLayer1_loss: 0.8982 - leftLayer2_loss: 0.0448 - midLayer2_loss: 1.4703 - rightLayer2_loss: 0.8746 - val_loss: 4.6682 - val_leftLayer1_loss: 0.0617 - val_midLayer1_loss: 1.3663 - val_rightLayer1_loss: 0.8855 - val_leftLayer2_loss: 0.0588 - val_midLayer2_loss: 1.3575 - val_rightLayer2_loss: 0.9384\n",
      "Epoch 6/11\n",
      "9248/9261 [============================>.] - ETA: 0s - loss: 4.7075 - leftLayer1_loss: 0.0595 - midLayer1_loss: 1.3700 - rightLayer1_loss: 0.8891 - leftLayer2_loss: 0.0423 - midLayer2_loss: 1.4746 - rightLayer2_loss: 0.8721\n",
      "Epoch 00006: val_loss improved from 4.66817 to 4.64364, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "9261/9261 [==============================] - 26s 3ms/step - loss: 4.7071 - leftLayer1_loss: 0.0595 - midLayer1_loss: 1.3699 - rightLayer1_loss: 0.8890 - leftLayer2_loss: 0.0423 - midLayer2_loss: 1.4745 - rightLayer2_loss: 0.8720 - val_loss: 4.6436 - val_leftLayer1_loss: 0.0570 - val_midLayer1_loss: 1.3663 - val_rightLayer1_loss: 0.8780 - val_leftLayer2_loss: 0.0558 - val_midLayer2_loss: 1.3575 - val_rightLayer2_loss: 0.9291\n",
      "Epoch 7/11\n",
      "9242/9261 [============================>.] - ETA: 0s - loss: 4.6900 - leftLayer1_loss: 0.0554 - midLayer1_loss: 1.3698 - rightLayer1_loss: 0.8829 - leftLayer2_loss: 0.0405 - midLayer2_loss: 1.4715 - rightLayer2_loss: 0.8700\n",
      "Epoch 00007: val_loss improved from 4.64364 to 4.62543, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "9261/9261 [==============================] - 25s 3ms/step - loss: 4.6893 - leftLayer1_loss: 0.0554 - midLayer1_loss: 1.3697 - rightLayer1_loss: 0.8828 - leftLayer2_loss: 0.0405 - midLayer2_loss: 1.4712 - rightLayer2_loss: 0.8698 - val_loss: 4.6254 - val_leftLayer1_loss: 0.0533 - val_midLayer1_loss: 1.3663 - val_rightLayer1_loss: 0.8728 - val_leftLayer2_loss: 0.0535 - val_midLayer2_loss: 1.3575 - val_rightLayer2_loss: 0.9220\n",
      "Epoch 8/11\n",
      "9247/9261 [============================>.] - ETA: 0s - loss: 4.6761 - leftLayer1_loss: 0.0522 - midLayer1_loss: 1.3692 - rightLayer1_loss: 0.8783 - leftLayer2_loss: 0.0393 - midLayer2_loss: 1.4685 - rightLayer2_loss: 0.8686\n",
      "Epoch 00008: val_loss improved from 4.62543 to 4.61145, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "9261/9261 [==============================] - 25s 3ms/step - loss: 4.6759 - leftLayer1_loss: 0.0522 - midLayer1_loss: 1.3692 - rightLayer1_loss: 0.8783 - leftLayer2_loss: 0.0393 - midLayer2_loss: 1.4685 - rightLayer2_loss: 0.8685 - val_loss: 4.6114 - val_leftLayer1_loss: 0.0505 - val_midLayer1_loss: 1.3663 - val_rightLayer1_loss: 0.8690 - val_leftLayer2_loss: 0.0518 - val_midLayer2_loss: 1.3575 - val_rightLayer2_loss: 0.9164\n",
      "Epoch 9/11\n",
      "9251/9261 [============================>.] - ETA: 0s - loss: 4.6723 - leftLayer1_loss: 0.0497 - midLayer1_loss: 1.3698 - rightLayer1_loss: 0.8748 - leftLayer2_loss: 0.0384 - midLayer2_loss: 1.4717 - rightLayer2_loss: 0.8678\n",
      "Epoch 00009: val_loss improved from 4.61145 to 4.60020, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "9261/9261 [==============================] - 25s 3ms/step - loss: 4.6720 - leftLayer1_loss: 0.0497 - midLayer1_loss: 1.3698 - rightLayer1_loss: 0.8748 - leftLayer2_loss: 0.0384 - midLayer2_loss: 1.4716 - rightLayer2_loss: 0.8678 - val_loss: 4.6002 - val_leftLayer1_loss: 0.0482 - val_midLayer1_loss: 1.3663 - val_rightLayer1_loss: 0.8660 - val_leftLayer2_loss: 0.0503 - val_midLayer2_loss: 1.3575 - val_rightLayer2_loss: 0.9119\n",
      "Epoch 10/11\n",
      "9259/9261 [============================>.] - ETA: 0s - loss: 4.6665 - leftLayer1_loss: 0.0476 - midLayer1_loss: 1.3694 - rightLayer1_loss: 0.8723 - leftLayer2_loss: 0.0377 - midLayer2_loss: 1.4727 - rightLayer2_loss: 0.8668\n",
      "Epoch 00010: val_loss improved from 4.60020 to 4.59099, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "9261/9261 [==============================] - 26s 3ms/step - loss: 4.6665 - leftLayer1_loss: 0.0476 - midLayer1_loss: 1.3694 - rightLayer1_loss: 0.8723 - leftLayer2_loss: 0.0377 - midLayer2_loss: 1.4727 - rightLayer2_loss: 0.8668 - val_loss: 4.5910 - val_leftLayer1_loss: 0.0464 - val_midLayer1_loss: 1.3663 - val_rightLayer1_loss: 0.8637 - val_leftLayer2_loss: 0.0491 - val_midLayer2_loss: 1.3575 - val_rightLayer2_loss: 0.9080\n",
      "Epoch 11/11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9259/9261 [============================>.] - ETA: 0s - loss: 4.6582 - leftLayer1_loss: 0.0459 - midLayer1_loss: 1.3694 - rightLayer1_loss: 0.8700 - leftLayer2_loss: 0.0372 - midLayer2_loss: 1.4702 - rightLayer2_loss: 0.8654\n",
      "Epoch 00011: val_loss improved from 4.59099 to 4.58340, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "9261/9261 [==============================] - 23s 2ms/step - loss: 4.6582 - leftLayer1_loss: 0.0460 - midLayer1_loss: 1.3694 - rightLayer1_loss: 0.8700 - leftLayer2_loss: 0.0372 - midLayer2_loss: 1.4702 - rightLayer2_loss: 0.8655 - val_loss: 4.5834 - val_leftLayer1_loss: 0.0448 - val_midLayer1_loss: 1.3663 - val_rightLayer1_loss: 0.8617 - val_leftLayer2_loss: 0.0482 - val_midLayer2_loss: 1.3575 - val_rightLayer2_loss: 0.9049\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.011999999999999997_test.log **\n",
      "auroc 0Effusion: 0.6624430128249766\n",
      "\n",
      "auprc 0Effusion: 0.22407131562057594\n",
      "\n",
      "auroc 1Effusion: 0.5804963417387929\n",
      "\n",
      "auprc 1Effusion: 0.1390461261366621\n",
      "\n",
      "auroc 2Effusion: 0.5601147325245572\n",
      "\n",
      "auprc 2Effusion: 0.15124090449219071\n",
      "\n",
      "auroc 3Effusion: 0.5033780650759136\n",
      "\n",
      "auprc 3Effusion: 0.11341989292942219\n",
      "\n",
      "auroc 4Effusion: 0.6681966144860302\n",
      "\n",
      "auprc 4Effusion: 0.17878474332120337\n",
      "\n",
      "auroc 5Effusion: 0.4351027067217512\n",
      "\n",
      "auprc 5Effusion: 0.1021392883936755\n",
      "\n",
      "mean auroc: 0.5682885788953369\n",
      "\n",
      "mean auprc: 0.15145037848228832\n",
      "\n",
      "max auroc: 0.6681966144860302\n",
      "\n",
      "max auprc: 0.22407131562057594\n",
      "\n",
      "298.5454773902893\n",
      "** set output weights path to: ./experiments/0.012999999999999996_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 9261 steps, validate for 1292 steps\n",
      "Epoch 1/11\n",
      "9238/9261 [============================>.] - ETA: 0s - loss: 5.6052 - leftLayer1_loss: 0.1166 - midLayer1_loss: 1.3632 - rightLayer1_loss: 1.4050 - leftLayer2_loss: 0.1095 - midLayer2_loss: 1.4523 - rightLayer2_loss: 1.1586\n",
      "Epoch 00001: val_loss improved from inf to 5.18651, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 5.6037 - leftLayer1_loss: 0.1166 - midLayer1_loss: 1.3632 - rightLayer1_loss: 1.4043 - leftLayer2_loss: 0.1094 - midLayer2_loss: 1.4522 - rightLayer2_loss: 1.1580 - val_loss: 5.1865 - val_leftLayer1_loss: 0.1060 - val_midLayer1_loss: 1.3579 - val_rightLayer1_loss: 1.1136 - val_leftLayer2_loss: 0.0987 - val_midLayer2_loss: 1.3866 - val_rightLayer2_loss: 1.1237\n",
      "Epoch 2/11\n",
      "9258/9261 [============================>.] - ETA: 0s - loss: 4.9240 - leftLayer1_loss: 0.0977 - midLayer1_loss: 1.3630 - rightLayer1_loss: 1.0266 - leftLayer2_loss: 0.0740 - midLayer2_loss: 1.4511 - rightLayer2_loss: 0.9116\n",
      "Epoch 00002: val_loss improved from 5.18651 to 4.90811, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.9241 - leftLayer1_loss: 0.0977 - midLayer1_loss: 1.3630 - rightLayer1_loss: 1.0266 - leftLayer2_loss: 0.0740 - midLayer2_loss: 1.4511 - rightLayer2_loss: 0.9117 - val_loss: 4.9081 - val_leftLayer1_loss: 0.0898 - val_midLayer1_loss: 1.3579 - val_rightLayer1_loss: 0.9640 - val_leftLayer2_loss: 0.0816 - val_midLayer2_loss: 1.3866 - val_rightLayer2_loss: 1.0283\n",
      "Epoch 3/11\n",
      "9247/9261 [============================>.] - ETA: 0s - loss: 4.7915 - leftLayer1_loss: 0.0838 - midLayer1_loss: 1.3626 - rightLayer1_loss: 0.9437 - leftLayer2_loss: 0.0581 - midLayer2_loss: 1.4560 - rightLayer2_loss: 0.8873\n",
      "Epoch 00003: val_loss improved from 4.90811 to 4.80135, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.7914 - leftLayer1_loss: 0.0838 - midLayer1_loss: 1.3626 - rightLayer1_loss: 0.9437 - leftLayer2_loss: 0.0580 - midLayer2_loss: 1.4560 - rightLayer2_loss: 0.8873 - val_loss: 4.8014 - val_leftLayer1_loss: 0.0779 - val_midLayer1_loss: 1.3579 - val_rightLayer1_loss: 0.9181 - val_leftLayer2_loss: 0.0717 - val_midLayer2_loss: 1.3866 - val_rightLayer2_loss: 0.9892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/11\n",
      "9260/9261 [============================>.] - ETA: 0s - loss: 4.7298 - leftLayer1_loss: 0.0736 - midLayer1_loss: 1.3627 - rightLayer1_loss: 0.9129 - leftLayer2_loss: 0.0500 - midLayer2_loss: 1.4512 - rightLayer2_loss: 0.8794\n",
      "Epoch 00004: val_loss improved from 4.80135 to 4.74331, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "9261/9261 [==============================] - 23s 2ms/step - loss: 4.7300 - leftLayer1_loss: 0.0736 - midLayer1_loss: 1.3627 - rightLayer1_loss: 0.9129 - leftLayer2_loss: 0.0500 - midLayer2_loss: 1.4512 - rightLayer2_loss: 0.8795 - val_loss: 4.7433 - val_leftLayer1_loss: 0.0692 - val_midLayer1_loss: 1.3579 - val_rightLayer1_loss: 0.8973 - val_leftLayer2_loss: 0.0654 - val_midLayer2_loss: 1.3866 - val_rightLayer2_loss: 0.9670\n",
      "Epoch 5/11\n",
      "9240/9261 [============================>.] - ETA: 0s - loss: 4.6985 - leftLayer1_loss: 0.0661 - midLayer1_loss: 1.3630 - rightLayer1_loss: 0.8978 - leftLayer2_loss: 0.0454 - midLayer2_loss: 1.4516 - rightLayer2_loss: 0.8745\n",
      "Epoch 00005: val_loss improved from 4.74331 to 4.70622, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.6979 - leftLayer1_loss: 0.0661 - midLayer1_loss: 1.3630 - rightLayer1_loss: 0.8977 - leftLayer2_loss: 0.0453 - midLayer2_loss: 1.4515 - rightLayer2_loss: 0.8744 - val_loss: 4.7062 - val_leftLayer1_loss: 0.0627 - val_midLayer1_loss: 1.3579 - val_rightLayer1_loss: 0.8856 - val_leftLayer2_loss: 0.0611 - val_midLayer2_loss: 1.3866 - val_rightLayer2_loss: 0.9524\n",
      "Epoch 6/11\n",
      "9255/9261 [============================>.] - ETA: 0s - loss: 4.6769 - leftLayer1_loss: 0.0605 - midLayer1_loss: 1.3633 - rightLayer1_loss: 0.8883 - leftLayer2_loss: 0.0426 - midLayer2_loss: 1.4509 - rightLayer2_loss: 0.8714\n",
      "Epoch 00006: val_loss improved from 4.70622 to 4.68033, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.6772 - leftLayer1_loss: 0.0605 - midLayer1_loss: 1.3633 - rightLayer1_loss: 0.8884 - leftLayer2_loss: 0.0426 - midLayer2_loss: 1.4510 - rightLayer2_loss: 0.8714 - val_loss: 4.6803 - val_leftLayer1_loss: 0.0578 - val_midLayer1_loss: 1.3579 - val_rightLayer1_loss: 0.8781 - val_leftLayer2_loss: 0.0579 - val_midLayer2_loss: 1.3866 - val_rightLayer2_loss: 0.9420\n",
      "Epoch 7/11\n",
      "9239/9261 [============================>.] - ETA: 0s - loss: 4.6660 - leftLayer1_loss: 0.0562 - midLayer1_loss: 1.3629 - rightLayer1_loss: 0.8819 - leftLayer2_loss: 0.0408 - midLayer2_loss: 1.4543 - rightLayer2_loss: 0.8699\n",
      "Epoch 00007: val_loss improved from 4.68033 to 4.66108, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.6656 - leftLayer1_loss: 0.0562 - midLayer1_loss: 1.3628 - rightLayer1_loss: 0.8818 - leftLayer2_loss: 0.0408 - midLayer2_loss: 1.4543 - rightLayer2_loss: 0.8697 - val_loss: 4.6611 - val_leftLayer1_loss: 0.0541 - val_midLayer1_loss: 1.3579 - val_rightLayer1_loss: 0.8729 - val_leftLayer2_loss: 0.0555 - val_midLayer2_loss: 1.3866 - val_rightLayer2_loss: 0.9341\n",
      "Epoch 8/11\n",
      "9255/9261 [============================>.] - ETA: 0s - loss: 4.6540 - leftLayer1_loss: 0.0529 - midLayer1_loss: 1.3624 - rightLayer1_loss: 0.8775 - leftLayer2_loss: 0.0395 - midLayer2_loss: 1.4536 - rightLayer2_loss: 0.8681\n",
      "Epoch 00008: val_loss improved from 4.66108 to 4.64616, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.6542 - leftLayer1_loss: 0.0529 - midLayer1_loss: 1.3624 - rightLayer1_loss: 0.8776 - leftLayer2_loss: 0.0395 - midLayer2_loss: 1.4537 - rightLayer2_loss: 0.8682 - val_loss: 4.6462 - val_leftLayer1_loss: 0.0511 - val_midLayer1_loss: 1.3579 - val_rightLayer1_loss: 0.8691 - val_leftLayer2_loss: 0.0536 - val_midLayer2_loss: 1.3866 - val_rightLayer2_loss: 0.9278\n",
      "Epoch 9/11\n",
      "9240/9261 [============================>.] - ETA: 0s - loss: 4.6465 - leftLayer1_loss: 0.0503 - midLayer1_loss: 1.3634 - rightLayer1_loss: 0.8745 - leftLayer2_loss: 0.0385 - midLayer2_loss: 1.4526 - rightLayer2_loss: 0.8672\n",
      "Epoch 00009: val_loss improved from 4.64616 to 4.63423, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.6458 - leftLayer1_loss: 0.0503 - midLayer1_loss: 1.3633 - rightLayer1_loss: 0.8743 - leftLayer2_loss: 0.0385 - midLayer2_loss: 1.4524 - rightLayer2_loss: 0.8670 - val_loss: 4.6342 - val_leftLayer1_loss: 0.0487 - val_midLayer1_loss: 1.3579 - val_rightLayer1_loss: 0.8661 - val_leftLayer2_loss: 0.0521 - val_midLayer2_loss: 1.3866 - val_rightLayer2_loss: 0.9227\n",
      "Epoch 10/11\n",
      "9259/9261 [============================>.] - ETA: 0s - loss: 4.6371 - leftLayer1_loss: 0.0481 - midLayer1_loss: 1.3631 - rightLayer1_loss: 0.8715 - leftLayer2_loss: 0.0378 - midLayer2_loss: 1.4500 - rightLayer2_loss: 0.8664\n",
      "Epoch 00010: val_loss improved from 4.63423 to 4.62440, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.6371 - leftLayer1_loss: 0.0481 - midLayer1_loss: 1.3632 - rightLayer1_loss: 0.8715 - leftLayer2_loss: 0.0378 - midLayer2_loss: 1.4500 - rightLayer2_loss: 0.8665 - val_loss: 4.6244 - val_leftLayer1_loss: 0.0468 - val_midLayer1_loss: 1.3579 - val_rightLayer1_loss: 0.8638 - val_leftLayer2_loss: 0.0508 - val_midLayer2_loss: 1.3866 - val_rightLayer2_loss: 0.9184\n",
      "Epoch 11/11\n",
      "9245/9261 [============================>.] - ETA: 0s - loss: 4.6360 - leftLayer1_loss: 0.0464 - midLayer1_loss: 1.3628 - rightLayer1_loss: 0.8695 - leftLayer2_loss: 0.0372 - midLayer2_loss: 1.4548 - rightLayer2_loss: 0.8652\n",
      "Epoch 00011: val_loss improved from 4.62440 to 4.61637, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.6357 - leftLayer1_loss: 0.0464 - midLayer1_loss: 1.3628 - rightLayer1_loss: 0.8694 - leftLayer2_loss: 0.0372 - midLayer2_loss: 1.4548 - rightLayer2_loss: 0.8651 - val_loss: 4.6164 - val_leftLayer1_loss: 0.0453 - val_midLayer1_loss: 1.3579 - val_rightLayer1_loss: 0.8618 - val_leftLayer2_loss: 0.0498 - val_midLayer2_loss: 1.3866 - val_rightLayer2_loss: 0.9150\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.012999999999999996_test.log **\n",
      "auroc 0Effusion: 0.6783267319194939\n",
      "\n",
      "auprc 0Effusion: 0.22988866395158777\n",
      "\n",
      "auroc 1Effusion: 0.353092073310401\n",
      "\n",
      "auprc 1Effusion: 0.08683509582415914\n",
      "\n",
      "auroc 2Effusion: 0.6591244171199016\n",
      "\n",
      "auprc 2Effusion: 0.19602997322653654\n",
      "\n",
      "auroc 3Effusion: 0.4703229480216295\n",
      "\n",
      "auprc 3Effusion: 0.10532140783972663\n",
      "\n",
      "auroc 4Effusion: 0.5081328101061986\n",
      "\n",
      "auprc 4Effusion: 0.11658247672867894\n",
      "\n",
      "auroc 5Effusion: 0.45649559046516486\n",
      "\n",
      "auprc 5Effusion: 0.10399980736450926\n",
      "\n",
      "mean auroc: 0.5209157618237983\n",
      "\n",
      "mean auprc: 0.13977623748919973\n",
      "\n",
      "max auroc: 0.6783267319194939\n",
      "\n",
      "max auprc: 0.22988866395158777\n",
      "\n",
      "286.5914843082428\n",
      "** set output weights path to: ./experiments/0.013999999999999995_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 9261 steps, validate for 1292 steps\n",
      "Epoch 1/11\n",
      "9258/9261 [============================>.] - ETA: 0s - loss: 5.5784 - leftLayer1_loss: 0.1113 - midLayer1_loss: 1.3449 - rightLayer1_loss: 1.4275 - leftLayer2_loss: 0.1029 - midLayer2_loss: 1.4234 - rightLayer2_loss: 1.1684\n",
      "Epoch 00001: val_loss improved from inf to 5.19395, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 5.5783 - leftLayer1_loss: 0.1113 - midLayer1_loss: 1.3449 - rightLayer1_loss: 1.4274 - leftLayer2_loss: 0.1029 - midLayer2_loss: 1.4234 - rightLayer2_loss: 1.1683 - val_loss: 5.1939 - val_leftLayer1_loss: 0.1026 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 1.1454 - val_leftLayer2_loss: 0.0994 - val_midLayer2_loss: 1.3500 - val_rightLayer2_loss: 1.1546\n",
      "Epoch 2/11\n",
      "9240/9261 [============================>.] - ETA: 0s - loss: 4.9032 - leftLayer1_loss: 0.0955 - midLayer1_loss: 1.3445 - rightLayer1_loss: 1.0487 - leftLayer2_loss: 0.0722 - midLayer2_loss: 1.4269 - rightLayer2_loss: 0.9154\n",
      "Epoch 00002: val_loss improved from 5.19395 to 4.89871, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.9026 - leftLayer1_loss: 0.0955 - midLayer1_loss: 1.3444 - rightLayer1_loss: 1.0485 - leftLayer2_loss: 0.0722 - midLayer2_loss: 1.4269 - rightLayer2_loss: 0.9152 - val_loss: 4.8987 - val_leftLayer1_loss: 0.0888 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.9821 - val_leftLayer2_loss: 0.0839 - val_midLayer2_loss: 1.3500 - val_rightLayer2_loss: 1.0520\n",
      "Epoch 3/11\n",
      "9255/9261 [============================>.] - ETA: 0s - loss: 4.7563 - leftLayer1_loss: 0.0834 - midLayer1_loss: 1.3446 - rightLayer1_loss: 0.9559 - leftLayer2_loss: 0.0576 - midLayer2_loss: 1.4254 - rightLayer2_loss: 0.8894\n",
      "Epoch 00003: val_loss improved from 4.89871 to 4.78405, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.7564 - leftLayer1_loss: 0.0834 - midLayer1_loss: 1.3446 - rightLayer1_loss: 0.9560 - leftLayer2_loss: 0.0576 - midLayer2_loss: 1.4254 - rightLayer2_loss: 0.8894 - val_loss: 4.7840 - val_leftLayer1_loss: 0.0783 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.9298 - val_leftLayer2_loss: 0.0745 - val_midLayer2_loss: 1.3500 - val_rightLayer2_loss: 1.0094\n",
      "Epoch 4/11\n",
      "9245/9261 [============================>.] - ETA: 0s - loss: 4.6949 - leftLayer1_loss: 0.0743 - midLayer1_loss: 1.3449 - rightLayer1_loss: 0.9215 - leftLayer2_loss: 0.0501 - midLayer2_loss: 1.4243 - rightLayer2_loss: 0.8798\n",
      "Epoch 00004: val_loss improved from 4.78405 to 4.72174, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 4.6944 - leftLayer1_loss: 0.0743 - midLayer1_loss: 1.3449 - rightLayer1_loss: 0.9214 - leftLayer2_loss: 0.0501 - midLayer2_loss: 1.4242 - rightLayer2_loss: 0.8796 - val_loss: 4.7217 - val_leftLayer1_loss: 0.0703 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.9057 - val_leftLayer2_loss: 0.0684 - val_midLayer2_loss: 1.3500 - val_rightLayer2_loss: 0.9853\n",
      "Epoch 5/11\n",
      "9255/9261 [============================>.] - ETA: 0s - loss: 4.6589 - leftLayer1_loss: 0.0674 - midLayer1_loss: 1.3451 - rightLayer1_loss: 0.9035 - leftLayer2_loss: 0.0457 - midLayer2_loss: 1.4221 - rightLayer2_loss: 0.8752\n",
      "Epoch 00005: val_loss improved from 4.72174 to 4.68159, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.6592 - leftLayer1_loss: 0.0674 - midLayer1_loss: 1.3451 - rightLayer1_loss: 0.9036 - leftLayer2_loss: 0.0457 - midLayer2_loss: 1.4222 - rightLayer2_loss: 0.8752 - val_loss: 4.6816 - val_leftLayer1_loss: 0.0642 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.8921 - val_leftLayer2_loss: 0.0641 - val_midLayer2_loss: 1.3500 - val_rightLayer2_loss: 0.9691\n",
      "Epoch 6/11\n",
      "9251/9261 [============================>.] - ETA: 0s - loss: 4.6426 - leftLayer1_loss: 0.0620 - midLayer1_loss: 1.3449 - rightLayer1_loss: 0.8932 - leftLayer2_loss: 0.0429 - midLayer2_loss: 1.4277 - rightLayer2_loss: 0.8719\n",
      "Epoch 00006: val_loss improved from 4.68159 to 4.65339, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "9261/9261 [==============================] - 24s 3ms/step - loss: 4.6425 - leftLayer1_loss: 0.0620 - midLayer1_loss: 1.3449 - rightLayer1_loss: 0.8931 - leftLayer2_loss: 0.0429 - midLayer2_loss: 1.4277 - rightLayer2_loss: 0.8719 - val_loss: 4.6534 - val_leftLayer1_loss: 0.0595 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.8835 - val_leftLayer2_loss: 0.0609 - val_midLayer2_loss: 1.3500 - val_rightLayer2_loss: 0.9575\n",
      "Epoch 7/11\n",
      "9254/9261 [============================>.] - ETA: 0s - loss: 4.6251 - leftLayer1_loss: 0.0578 - midLayer1_loss: 1.3451 - rightLayer1_loss: 0.8861 - leftLayer2_loss: 0.0411 - midLayer2_loss: 1.4252 - rightLayer2_loss: 0.8698\n",
      "Epoch 00007: val_loss improved from 4.65339 to 4.63227, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "9261/9261 [==============================] - 23s 2ms/step - loss: 4.6251 - leftLayer1_loss: 0.0578 - midLayer1_loss: 1.3451 - rightLayer1_loss: 0.8861 - leftLayer2_loss: 0.0411 - midLayer2_loss: 1.4252 - rightLayer2_loss: 0.8698 - val_loss: 4.6323 - val_leftLayer1_loss: 0.0558 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.8775 - val_leftLayer2_loss: 0.0584 - val_midLayer2_loss: 1.3500 - val_rightLayer2_loss: 0.9486\n",
      "Epoch 8/11\n",
      "9248/9261 [============================>.] - ETA: 0s - loss: 4.6125 - leftLayer1_loss: 0.0544 - midLayer1_loss: 1.3447 - rightLayer1_loss: 0.8808 - leftLayer2_loss: 0.0397 - midLayer2_loss: 1.4246 - rightLayer2_loss: 0.8682\n",
      "Epoch 00008: val_loss improved from 4.63227 to 4.61593, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.6122 - leftLayer1_loss: 0.0544 - midLayer1_loss: 1.3447 - rightLayer1_loss: 0.8807 - leftLayer2_loss: 0.0397 - midLayer2_loss: 1.4246 - rightLayer2_loss: 0.8681 - val_loss: 4.6159 - val_leftLayer1_loss: 0.0528 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.8730 - val_leftLayer2_loss: 0.0564 - val_midLayer2_loss: 1.3500 - val_rightLayer2_loss: 0.9417\n",
      "Epoch 9/11\n",
      "9247/9261 [============================>.] - ETA: 0s - loss: 4.6040 - leftLayer1_loss: 0.0518 - midLayer1_loss: 1.3446 - rightLayer1_loss: 0.8770 - leftLayer2_loss: 0.0388 - midLayer2_loss: 1.4244 - rightLayer2_loss: 0.8674\n",
      "Epoch 00009: val_loss improved from 4.61593 to 4.60269, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.6039 - leftLayer1_loss: 0.0518 - midLayer1_loss: 1.3446 - rightLayer1_loss: 0.8770 - leftLayer2_loss: 0.0388 - midLayer2_loss: 1.4245 - rightLayer2_loss: 0.8673 - val_loss: 4.6027 - val_leftLayer1_loss: 0.0503 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.8696 - val_leftLayer2_loss: 0.0548 - val_midLayer2_loss: 1.3500 - val_rightLayer2_loss: 0.9359\n",
      "Epoch 10/11\n",
      "9250/9261 [============================>.] - ETA: 0s - loss: 4.5984 - leftLayer1_loss: 0.0496 - midLayer1_loss: 1.3448 - rightLayer1_loss: 0.8739 - leftLayer2_loss: 0.0381 - midLayer2_loss: 1.4260 - rightLayer2_loss: 0.8661\n",
      "Epoch 00010: val_loss improved from 4.60269 to 4.59183, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.5981 - leftLayer1_loss: 0.0496 - midLayer1_loss: 1.3448 - rightLayer1_loss: 0.8738 - leftLayer2_loss: 0.0381 - midLayer2_loss: 1.4259 - rightLayer2_loss: 0.8660 - val_loss: 4.5918 - val_leftLayer1_loss: 0.0483 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.8669 - val_leftLayer2_loss: 0.0535 - val_midLayer2_loss: 1.3500 - val_rightLayer2_loss: 0.9311\n",
      "Epoch 11/11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9255/9261 [============================>.] - ETA: 0s - loss: 4.5913 - leftLayer1_loss: 0.0478 - midLayer1_loss: 1.3448 - rightLayer1_loss: 0.8713 - leftLayer2_loss: 0.0374 - midLayer2_loss: 1.4250 - rightLayer2_loss: 0.8650\n",
      "Epoch 00011: val_loss improved from 4.59183 to 4.58275, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.5916 - leftLayer1_loss: 0.0478 - midLayer1_loss: 1.3448 - rightLayer1_loss: 0.8714 - leftLayer2_loss: 0.0374 - midLayer2_loss: 1.4251 - rightLayer2_loss: 0.8651 - val_loss: 4.5828 - val_leftLayer1_loss: 0.0467 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.8646 - val_leftLayer2_loss: 0.0524 - val_midLayer2_loss: 1.3500 - val_rightLayer2_loss: 0.9270\n",
      "22433/22433 [==============================] - 27s 1ms/step\n",
      "** write log to ./experiments/0.013999999999999995_test.log **\n",
      "auroc 0Effusion: 0.41544430631608265\n",
      "\n",
      "auprc 0Effusion: 0.09688982707317102\n",
      "\n",
      "auroc 1Effusion: 0.4192298094658927\n",
      "\n",
      "auprc 1Effusion: 0.09637534738261386\n",
      "\n",
      "auroc 2Effusion: 0.5902711006202934\n",
      "\n",
      "auprc 2Effusion: 0.15166167168330538\n",
      "\n",
      "auroc 3Effusion: 0.463000594915127\n",
      "\n",
      "auprc 3Effusion: 0.10504698500873796\n",
      "\n",
      "auroc 4Effusion: 0.28690159337689447\n",
      "\n",
      "auprc 4Effusion: 0.0796332184029697\n",
      "\n",
      "auroc 5Effusion: 0.4170123196992189\n",
      "\n",
      "auprc 5Effusion: 0.09561146964295676\n",
      "\n",
      "mean auroc: 0.4319766207322515\n",
      "\n",
      "mean auprc: 0.10420308653229245\n",
      "\n",
      "max auroc: 0.5902711006202934\n",
      "\n",
      "max auprc: 0.15166167168330538\n",
      "\n",
      "279.84462571144104\n",
      "** set output weights path to: ./experiments/0.014999999999999994_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 9261 steps, validate for 1292 steps\n",
      "Epoch 1/11\n",
      "9244/9261 [============================>.] - ETA: 0s - loss: 5.5089 - leftLayer1_loss: 0.1179 - midLayer1_loss: 1.3849 - rightLayer1_loss: 1.3871 - leftLayer2_loss: 0.1013 - midLayer2_loss: 1.3749 - rightLayer2_loss: 1.1428\n",
      "Epoch 00001: val_loss improved from inf to 5.16552, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "9261/9261 [==============================] - 23s 3ms/step - loss: 5.5077 - leftLayer1_loss: 0.1179 - midLayer1_loss: 1.3849 - rightLayer1_loss: 1.3865 - leftLayer2_loss: 0.1013 - midLayer2_loss: 1.3748 - rightLayer2_loss: 1.1424 - val_loss: 5.1655 - val_leftLayer1_loss: 0.1066 - val_midLayer1_loss: 1.3763 - val_rightLayer1_loss: 1.1023 - val_leftLayer2_loss: 0.0966 - val_midLayer2_loss: 1.3365 - val_rightLayer2_loss: 1.1472\n",
      "Epoch 2/11\n",
      "9247/9261 [============================>.] - ETA: 0s - loss: 4.8573 - leftLayer1_loss: 0.0980 - midLayer1_loss: 1.3851 - rightLayer1_loss: 1.0149 - leftLayer2_loss: 0.0711 - midLayer2_loss: 1.3753 - rightLayer2_loss: 0.9130\n",
      "Epoch 00002: val_loss improved from 5.16552 to 4.89104, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.8571 - leftLayer1_loss: 0.0980 - midLayer1_loss: 1.3850 - rightLayer1_loss: 1.0148 - leftLayer2_loss: 0.0711 - midLayer2_loss: 1.3753 - rightLayer2_loss: 0.9129 - val_loss: 4.8910 - val_leftLayer1_loss: 0.0897 - val_midLayer1_loss: 1.3763 - val_rightLayer1_loss: 0.9585 - val_leftLayer2_loss: 0.0814 - val_midLayer2_loss: 1.3365 - val_rightLayer2_loss: 1.0487\n",
      "Epoch 3/11\n",
      "9238/9261 [============================>.] - ETA: 0s - loss: 4.7276 - leftLayer1_loss: 0.0835 - midLayer1_loss: 1.3854 - rightLayer1_loss: 0.9369 - leftLayer2_loss: 0.0568 - midLayer2_loss: 1.3759 - rightLayer2_loss: 0.8892\n",
      "Epoch 00003: val_loss improved from 4.89104 to 4.78406, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.7273 - leftLayer1_loss: 0.0834 - midLayer1_loss: 1.3853 - rightLayer1_loss: 0.9368 - leftLayer2_loss: 0.0568 - midLayer2_loss: 1.3759 - rightLayer2_loss: 0.8891 - val_loss: 4.7841 - val_leftLayer1_loss: 0.0775 - val_midLayer1_loss: 1.3763 - val_rightLayer1_loss: 0.9144 - val_leftLayer2_loss: 0.0723 - val_midLayer2_loss: 1.3365 - val_rightLayer2_loss: 1.0071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/11\n",
      "9246/9261 [============================>.] - ETA: 0s - loss: 4.6706 - leftLayer1_loss: 0.0730 - midLayer1_loss: 1.3857 - rightLayer1_loss: 0.9083 - leftLayer2_loss: 0.0494 - midLayer2_loss: 1.3744 - rightLayer2_loss: 0.8798\n",
      "Epoch 00004: val_loss improved from 4.78406 to 4.72535, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.6703 - leftLayer1_loss: 0.0730 - midLayer1_loss: 1.3857 - rightLayer1_loss: 0.9082 - leftLayer2_loss: 0.0494 - midLayer2_loss: 1.3742 - rightLayer2_loss: 0.8798 - val_loss: 4.7254 - val_leftLayer1_loss: 0.0686 - val_midLayer1_loss: 1.3763 - val_rightLayer1_loss: 0.8943 - val_leftLayer2_loss: 0.0663 - val_midLayer2_loss: 1.3365 - val_rightLayer2_loss: 0.9833\n",
      "Epoch 5/11\n",
      "9237/9261 [============================>.] - ETA: 0s - loss: 4.6395 - leftLayer1_loss: 0.0653 - midLayer1_loss: 1.3854 - rightLayer1_loss: 0.8936 - leftLayer2_loss: 0.0450 - midLayer2_loss: 1.3746 - rightLayer2_loss: 0.8754\n",
      "Epoch 00005: val_loss improved from 4.72535 to 4.68740, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.6393 - leftLayer1_loss: 0.0653 - midLayer1_loss: 1.3854 - rightLayer1_loss: 0.8936 - leftLayer2_loss: 0.0450 - midLayer2_loss: 1.3746 - rightLayer2_loss: 0.8754 - val_loss: 4.6874 - val_leftLayer1_loss: 0.0621 - val_midLayer1_loss: 1.3763 - val_rightLayer1_loss: 0.8830 - val_leftLayer2_loss: 0.0622 - val_midLayer2_loss: 1.3365 - val_rightLayer2_loss: 0.9674\n",
      "Epoch 6/11\n",
      "9241/9261 [============================>.] - ETA: 0s - loss: 4.6180 - leftLayer1_loss: 0.0597 - midLayer1_loss: 1.3856 - rightLayer1_loss: 0.8850 - leftLayer2_loss: 0.0425 - midLayer2_loss: 1.3727 - rightLayer2_loss: 0.8726\n",
      "Epoch 00006: val_loss improved from 4.68740 to 4.66063, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.6175 - leftLayer1_loss: 0.0597 - midLayer1_loss: 1.3855 - rightLayer1_loss: 0.8848 - leftLayer2_loss: 0.0425 - midLayer2_loss: 1.3726 - rightLayer2_loss: 0.8724 - val_loss: 4.6606 - val_leftLayer1_loss: 0.0572 - val_midLayer1_loss: 1.3763 - val_rightLayer1_loss: 0.8757 - val_leftLayer2_loss: 0.0591 - val_midLayer2_loss: 1.3365 - val_rightLayer2_loss: 0.9558\n",
      "Epoch 7/11\n",
      "9239/9261 [============================>.] - ETA: 0s - loss: 4.6046 - leftLayer1_loss: 0.0554 - midLayer1_loss: 1.3852 - rightLayer1_loss: 0.8792 - leftLayer2_loss: 0.0406 - midLayer2_loss: 1.3735 - rightLayer2_loss: 0.8706\n",
      "Epoch 00007: val_loss improved from 4.66063 to 4.64078, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.6043 - leftLayer1_loss: 0.0554 - midLayer1_loss: 1.3852 - rightLayer1_loss: 0.8791 - leftLayer2_loss: 0.0406 - midLayer2_loss: 1.3735 - rightLayer2_loss: 0.8705 - val_loss: 4.6408 - val_leftLayer1_loss: 0.0534 - val_midLayer1_loss: 1.3763 - val_rightLayer1_loss: 0.8707 - val_leftLayer2_loss: 0.0568 - val_midLayer2_loss: 1.3365 - val_rightLayer2_loss: 0.9471\n",
      "Epoch 8/11\n",
      "9259/9261 [============================>.] - ETA: 0s - loss: 4.5947 - leftLayer1_loss: 0.0521 - midLayer1_loss: 1.3855 - rightLayer1_loss: 0.8746 - leftLayer2_loss: 0.0394 - midLayer2_loss: 1.3748 - rightLayer2_loss: 0.8682\n",
      "Epoch 00008: val_loss improved from 4.64078 to 4.62551, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "9261/9261 [==============================] - 23s 2ms/step - loss: 4.5948 - leftLayer1_loss: 0.0521 - midLayer1_loss: 1.3855 - rightLayer1_loss: 0.8746 - leftLayer2_loss: 0.0394 - midLayer2_loss: 1.3749 - rightLayer2_loss: 0.8682 - val_loss: 4.6255 - val_leftLayer1_loss: 0.0505 - val_midLayer1_loss: 1.3763 - val_rightLayer1_loss: 0.8669 - val_leftLayer2_loss: 0.0549 - val_midLayer2_loss: 1.3365 - val_rightLayer2_loss: 0.9404\n",
      "Epoch 9/11\n",
      "9238/9261 [============================>.] - ETA: 0s - loss: 4.5841 - leftLayer1_loss: 0.0495 - midLayer1_loss: 1.3851 - rightLayer1_loss: 0.8713 - leftLayer2_loss: 0.0385 - midLayer2_loss: 1.3721 - rightLayer2_loss: 0.8677\n",
      "Epoch 00009: val_loss improved from 4.62551 to 4.61309, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "9261/9261 [==============================] - 23s 2ms/step - loss: 4.5837 - leftLayer1_loss: 0.0495 - midLayer1_loss: 1.3851 - rightLayer1_loss: 0.8712 - leftLayer2_loss: 0.0385 - midLayer2_loss: 1.3718 - rightLayer2_loss: 0.8676 - val_loss: 4.6131 - val_leftLayer1_loss: 0.0482 - val_midLayer1_loss: 1.3763 - val_rightLayer1_loss: 0.8640 - val_leftLayer2_loss: 0.0533 - val_midLayer2_loss: 1.3365 - val_rightLayer2_loss: 0.9347\n",
      "Epoch 10/11\n",
      "9252/9261 [============================>.] - ETA: 0s - loss: 4.5799 - leftLayer1_loss: 0.0474 - midLayer1_loss: 1.3852 - rightLayer1_loss: 0.8688 - leftLayer2_loss: 0.0378 - midLayer2_loss: 1.3741 - rightLayer2_loss: 0.8666\n",
      "Epoch 00010: val_loss improved from 4.61309 to 4.60299, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.5796 - leftLayer1_loss: 0.0474 - midLayer1_loss: 1.3852 - rightLayer1_loss: 0.8687 - leftLayer2_loss: 0.0378 - midLayer2_loss: 1.3740 - rightLayer2_loss: 0.8665 - val_loss: 4.6030 - val_leftLayer1_loss: 0.0463 - val_midLayer1_loss: 1.3763 - val_rightLayer1_loss: 0.8617 - val_leftLayer2_loss: 0.0521 - val_midLayer2_loss: 1.3365 - val_rightLayer2_loss: 0.9301\n",
      "Epoch 11/11\n",
      "9250/9261 [============================>.] - ETA: 0s - loss: 4.5756 - leftLayer1_loss: 0.0458 - midLayer1_loss: 1.3857 - rightLayer1_loss: 0.8665 - leftLayer2_loss: 0.0372 - midLayer2_loss: 1.3748 - rightLayer2_loss: 0.8656\n",
      "Epoch 00011: val_loss improved from 4.60299 to 4.59455, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "9261/9261 [==============================] - 22s 2ms/step - loss: 4.5753 - leftLayer1_loss: 0.0457 - midLayer1_loss: 1.3856 - rightLayer1_loss: 0.8664 - leftLayer2_loss: 0.0372 - midLayer2_loss: 1.3748 - rightLayer2_loss: 0.8655 - val_loss: 4.5946 - val_leftLayer1_loss: 0.0448 - val_midLayer1_loss: 1.3763 - val_rightLayer1_loss: 0.8598 - val_leftLayer2_loss: 0.0510 - val_midLayer2_loss: 1.3365 - val_rightLayer2_loss: 0.9262\n",
      "22433/22433 [==============================] - 27s 1ms/step\n",
      "** write log to ./experiments/0.014999999999999994_test.log **\n",
      "auroc 0Effusion: 0.739541721610793\n",
      "\n",
      "auprc 0Effusion: 0.2529105842733651\n",
      "\n",
      "auroc 1Effusion: 0.7838526856408465\n",
      "\n",
      "auprc 1Effusion: 0.331150091598834\n",
      "\n",
      "auroc 2Effusion: 0.6465890745447733\n",
      "\n",
      "auprc 2Effusion: 0.1744651169099754\n",
      "\n",
      "auroc 3Effusion: 0.600184845861037\n",
      "\n",
      "auprc 3Effusion: 0.1431689684338039\n",
      "\n",
      "auroc 4Effusion: 0.46251363985282595\n",
      "\n",
      "auprc 4Effusion: 0.10847006071876159\n",
      "\n",
      "auroc 5Effusion: 0.44206760702447856\n",
      "\n",
      "auprc 5Effusion: 0.10067375598950473\n",
      "\n",
      "mean auroc: 0.6124582624224589\n",
      "\n",
      "mean auprc: 0.1851397629873741\n",
      "\n",
      "max auroc: 0.7838526856408465\n",
      "\n",
      "max auprc: 0.331150091598834\n",
      "\n",
      "274.6512825489044\n"
     ]
    }
   ],
   "source": [
    "step = np.arange(0.009, 0.0151, 0.001)\n",
    "maxi = []\n",
    "for k in np.nditer(step):\n",
    "    opn, daTime = optimize_network(k)\n",
    "    print(daTime)\n",
    "    maxi.append(opn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8031153185829366\n"
     ]
    }
   ],
   "source": [
    "print(np.max(maxi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
