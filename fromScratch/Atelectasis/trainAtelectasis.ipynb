{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import pickle\n",
    "from callback import MultipleClassAUROC, MultiGPUModelCheckpoint\n",
    "from configparser import ConfigParser\n",
    "from generator import AugmentedImageSequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from utility import get_sample_counts\n",
    "from weights import get_class_weights\n",
    "from augmenter import augmenter\n",
    "from tensorflow.keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer, Flatten, Input, GaussianNoise\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_radam import RAdam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#print(\"TensorFlow version: \", tf.__version__)\n",
    "#assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "#    \"This notebook requires TensorFlow 2.0 or above.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "# UPDATED: import from tensorflow.keras instead of keras\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics\n",
    "import gc\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneClass = \"Atelectasis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"./config.ini\"\n",
    "cp = ConfigParser()\n",
    "cp.read(config_file)\n",
    "\n",
    "    # default config\n",
    "output_dir = cp[\"DEFAULT\"].get(\"output_dir\")\n",
    "image_source_dir = cp[\"DEFAULT\"].get(\"image_source_dir\")\n",
    "base_model_name = cp[\"DEFAULT\"].get(\"base_model_name\")\n",
    "class_names = cp[\"DEFAULT\"].get(\"class_names\").split(\",\")\n",
    "\n",
    "    # train config\n",
    "use_base_model_weights = cp[\"TRAIN\"].getboolean(\"use_base_model_weights\")\n",
    "use_trained_model_weights = cp[\"TRAIN\"].getboolean(\"use_trained_model_weights\")\n",
    "use_best_weights = cp[\"TRAIN\"].getboolean(\"use_best_weights\")\n",
    "output_weights_name = cp[\"TRAIN\"].get(\"output_weights_name\")\n",
    "epochs = cp[\"TRAIN\"].getint(\"epochs\")\n",
    "batch_size = cp[\"TRAIN\"].getint(\"batch_size\")\n",
    "initial_learning_rate = cp[\"TRAIN\"].getfloat(\"initial_learning_rate\")\n",
    "generator_workers = cp[\"TRAIN\"].getint(\"generator_workers\")\n",
    "image_dimension = cp[\"TRAIN\"].getint(\"image_dimension\")\n",
    "train_steps = cp[\"TRAIN\"].get(\"train_steps\")\n",
    "patience_reduce_lr = cp[\"TRAIN\"].getint(\"patience_reduce_lr\")\n",
    "min_lr = cp[\"TRAIN\"].getfloat(\"min_lr\")\n",
    "validation_steps = cp[\"TRAIN\"].get(\"validation_steps\")\n",
    "positive_weights_multiply = cp[\"TRAIN\"].getfloat(\"positive_weights_multiply\")\n",
    "dataset_csv_dir = cp[\"TRAIN\"].get(\"dataset_csv_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=1.0, alpha=0.5):\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(tf.cast(y_pred,tf.float32) - tf.cast(y_true,tf.float32)), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network1(dropout=0.08425517073874295, neuronPct=0.1767547775828121, neuronShrink=0.33180474398878285):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "    # Construct neural network\n",
    "    neuronCount = neuronCount * neuronShrink\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,1536)))\n",
    "    model.add(Flatten(name='flat1'))\n",
    "    model.add(Dense(neuronCount,name='dense1'))\n",
    "    model.add(Activation('relu',name='relu1'))\n",
    "    model.add(Dropout(dropout, name='dropout1'))\n",
    "    model.add(Dense(14, activation='sigmoid',name='midLayer1')) # Output\n",
    "    weights_path= None\n",
    "    if weights_path is not None:\n",
    "        print(f\"load model weights_path: {weights_path}\")\n",
    "        model.load_weights(weights_path)\n",
    "    model.layers.pop()\n",
    "    dr = model.layers[-2].output\n",
    "    model.trainable = False\n",
    "    left = Dense(14, activation=\"sigmoid\", name='leftLayer1')(dr)\n",
    "    right = Dense(14, activation=\"sigmoid\", name='rightLayer1')(dr)\n",
    "    model = Model(model.input, [left,model.output,right])\n",
    "    #model = Model(model.input, model.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network2(dropout=0.15672137551441198, neuronPct=0.2197894476507525, neuronShrink=0.3803316528497302, noisePct=0.282563134185142):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "    # Construct neural network\n",
    "    neuronCount = neuronCount * neuronShrink\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,1536)))\n",
    "    model.add(Flatten(name='flat2'))\n",
    "    model.add(Dense(neuronCount,name='dense2'))\n",
    "    model.add(GaussianNoise(noisePct))\n",
    "    model.add(Activation('relu',name='relu2'))\n",
    "    model.add(Dropout(dropout, name='dropout2'))\n",
    "    model.add(Dense(14, activation='sigmoid',name='midLayer2')) # Output\n",
    "    weights_path= None\n",
    "    if weights_path is not None:\n",
    "        print(f\"load model weights_path: {weights_path}\")\n",
    "        model.load_weights(weights_path)\n",
    "    #model.layers.pop()\n",
    "    dr = model.layers[-2].output\n",
    "    model.trainable = False\n",
    "    left = Dense(14, activation=\"sigmoid\", name='leftLayer2')(dr)\n",
    "    right = Dense(14, activation=\"sigmoid\", name='rightLayer2')(dr)\n",
    "    model = Model(model.input, [left,model.output,right])\n",
    "    #model = Model(model.input, model.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network(model1,model2):\n",
    "    model = Model([model1.input,model2.input], [model1.output,model2.output])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** compute class weights from training data **\n",
      "7996: 7996\n",
      "264: 7996\n",
      "2226: 7996\n",
      "2281: 7996\n",
      "503: 7996\n",
      "405: 7996\n",
      "188: 7996\n",
      "522: 7996\n",
      "813: 7996\n",
      "163: 7996\n",
      "316: 7996\n",
      "161: 7996\n",
      "323: 7996\n",
      "23: 7996\n",
      "** class_weights **\n",
      "[{0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}]\n"
     ]
    }
   ],
   "source": [
    "# compute steps\n",
    "train_counts, train_pos_counts = get_sample_counts(output_dir, \"train\"+oneClass, class_names)\n",
    "dev_counts, _ = get_sample_counts(output_dir, \"dev\"+oneClass, class_names)\n",
    "    \n",
    "if train_steps == \"auto\":\n",
    "    train_steps = int(train_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        train_steps = int(train_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"train_steps: {train_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "    print(f\"** train_steps: {train_steps} **\")\n",
    "\n",
    "if validation_steps == \"auto\":\n",
    "    validation_steps = int(dev_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        validation_steps = int(validation_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"validation_steps: {validation_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        print(f\"** validation_steps: {validation_steps} **\")\n",
    "\n",
    "        # compute class weights\n",
    "keras.backend.clear_session()\n",
    "print(\"** compute class weights from training data **\")\n",
    "class_weights = get_class_weights(train_counts,train_pos_counts,multiply=positive_weights_multiply,)\n",
    "print(\"** class_weights **\")\n",
    "print(class_weights)\n",
    "#print(str(train_steps))\n",
    "#print(str(train_counts))\n",
    "#print(str(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** test_steps: 22433 **\n"
     ]
    }
   ],
   "source": [
    "test_steps = cp[\"TEST\"].get(\"test_steps\")\n",
    "test_counts, _ = get_sample_counts(output_dir, \"test\", class_names)\n",
    "\n",
    "if test_steps == \"auto\":\n",
    "    test_steps = int(test_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        test_steps = int(test_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"test_steps: {test_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        \n",
    "print(f\"** test_steps: {test_steps} **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"train\"+oneClass+\".csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=train_steps,\n",
    "        )\n",
    "validation_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"dev\"+oneClass+\".csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=validation_steps,\n",
    "            shuffle_on_epoch_end=False,\n",
    ")\n",
    "\n",
    "test_sequence = AugmentedImageSequence(\n",
    "        dataset_csv_file=os.path.join(output_dir, \"test.csv\"),\n",
    "        class_names=class_names,\n",
    "        source_image_dir=image_source_dir,\n",
    "        batch_size=batch_size,\n",
    "        target_size=(image_dimension, image_dimension),\n",
    "        augmenter=None,\n",
    "        steps=test_steps,\n",
    "        shuffle_on_epoch_end=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(lr):\n",
    "    gc.collect()\n",
    "      # Define the Keras TensorBoard callback.\n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    model1 = construct_network1()\n",
    "    model2 = construct_network2()\n",
    "    \n",
    "    optimizer = SGD(lr=initial_learning_rate)\n",
    "    \n",
    "    alpha = 0.9340456763831478\n",
    "    gamma = 1.4195808780694898\n",
    "    model1.compile(optimizer=optimizer,loss={'leftLayer1':tf.keras.losses.Huber(),'midLayer1':focal_loss(gamma=gamma,alpha=alpha),'rightLayer1':euclidean_distance_loss})\n",
    "\n",
    "    alpha = 0.7297456293468533\n",
    "    gamma = 1.2700405014991505\n",
    "    model2.compile(optimizer=optimizer,loss={'leftLayer2':tf.keras.losses.Huber(),'midLayer2':focal_loss(gamma=gamma,alpha=alpha),'rightLayer2':euclidean_distance_loss})\n",
    "  \n",
    "    model = construct_network(model1=model1,model2=model2)\n",
    "    model.compile(optimizer=optimizer,loss={'leftLayer1':tf.keras.losses.Huber(),'midLayer1':focal_loss(gamma=gamma,alpha=alpha),'rightLayer1':euclidean_distance_loss,'leftLayer2':tf.keras.losses.Huber(),'midLayer2':focal_loss(gamma=gamma,alpha=alpha),'rightLayer2':euclidean_distance_loss})\n",
    "\n",
    "    output_weights_path = os.path.join(output_dir,  str(lr)+\"_\"+output_weights_name)\n",
    "    \n",
    "    print(f\"** set output weights path to: {output_weights_path} **\")\n",
    "                  \n",
    "    \n",
    "                  \n",
    "    checkpoint = ModelCheckpoint(\n",
    "                 output_weights_path,\n",
    "                 save_weights_only=True,\n",
    "                 save_best_only=True,\n",
    "                 verbose=1,\n",
    "            )\n",
    "    start_time = time.time()\n",
    "  \n",
    "    model.summary()\n",
    "  \n",
    "    callbacks = [\n",
    "            checkpoint,\n",
    "            #keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "            #TensorBoard(log_dir=os.path.join(output_dir, \"logs\"), batch_size=batch_size),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience_reduce_lr,\n",
    "                              verbose=1, mode=\"min\", min_lr=min_lr), \n",
    "            EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    history = model.fit_generator(\n",
    "            generator=train_sequence,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_sequence,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=[class_weights,class_weights,class_weights,class_weights,class_weights,class_weights],\n",
    "            workers=generator_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        \n",
    "    y_hat = model.predict_generator(test_sequence, verbose=1)\n",
    "    y = test_sequence.get_y_true()\n",
    "    \n",
    "    test_log_path = os.path.join(output_dir, str(lr)+\"_\"+\"test.log\")\n",
    "    print(f\"** write log to {test_log_path} **\")\n",
    "    aurocs = []\n",
    "    auprcs = []\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    threshold = dict()\n",
    "    with open(test_log_path, \"w\") as f:\n",
    "        for k in range(6):\n",
    "            for i in range(len(class_names)):\n",
    "                 if(class_names[i] == str(oneClass)):\n",
    "                \n",
    "                    try:\n",
    "                        score = roc_auc_score(y[:, i], y_hat[k][:, i])\n",
    "                        precision[i], recall[i], threshold[i] = precision_recall_curve(y[:, i], y_hat[k][:, i])\n",
    "                        tmp = auc(recall[i], precision[i])\n",
    "                        aurocs.append(score)\n",
    "                        auprcs.append(tmp) \n",
    "                    except ValueError:\n",
    "                        score = 0\n",
    "               \n",
    "                    print(f\"auroc {str(k)+class_names[i]}: {score}\\n\")\n",
    "                    print(f\"auprc {str(k)+class_names[i]}: {tmp}\\n\")\n",
    "                    f.write(f\"auroc {str(k)+class_names[i]}: {score}\\n\")\n",
    "                    f.write(f\"auprc {str(k)+class_names[i]}: {tmp}\\n\")\n",
    "        \n",
    "        mean_auroc = np.mean(aurocs)\n",
    "        mean_auprc = float(np.mean(auprcs))\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(f\"mean auroc: {mean_auroc}\\n\")\n",
    "        print(f\"mean auroc: {mean_auroc}\\n\")\n",
    "        f.write(f\"mean auprc: {mean_auprc}\\n\")\n",
    "        print(f\"mean auprc: {mean_auprc}\\n\")\n",
    "        \n",
    "        max_auroc = np.max(aurocs)\n",
    "        max_auprc = float(np.max(auprcs))\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(f\"max auroc: {max_auroc}\\n\")\n",
    "        print(f\"max auroc: {max_auroc}\\n\")\n",
    "        f.write(f\"max auprc: {max_auprc}\\n\")\n",
    "        print(f\"max auprc: {max_auprc}\\n\")\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "    time_took = time.time() - start_time\n",
    "    \n",
    "    return max_auroc, time_took\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** set output weights path to: ./experiments/0.009_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-15-3539473a5eed>:58: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 7996 steps, validate for 1119 steps\n",
      "Epoch 1/11\n",
      "7982/7996 [============================>.] - ETA: 0s - loss: 5.7280 - leftLayer1_loss: 0.1096 - midLayer1_loss: 1.4425 - rightLayer1_loss: 1.4705 - leftLayer2_loss: 0.1054 - midLayer2_loss: 1.4733 - rightLayer2_loss: 1.1267\n",
      "Epoch 00001: val_loss improved from inf to 5.22038, saving model to ./experiments/0.009_weights.h5\n",
      "7996/7996 [==============================] - 22s 3ms/step - loss: 5.7269 - leftLayer1_loss: 0.1095 - midLayer1_loss: 1.4424 - rightLayer1_loss: 1.4699 - leftLayer2_loss: 0.1054 - midLayer2_loss: 1.4734 - rightLayer2_loss: 1.1262 - val_loss: 5.2204 - val_leftLayer1_loss: 0.1021 - val_midLayer1_loss: 1.4317 - val_rightLayer1_loss: 1.1610 - val_leftLayer2_loss: 0.0995 - val_midLayer2_loss: 1.3520 - val_rightLayer2_loss: 1.0741\n",
      "Epoch 2/11\n",
      "7984/7996 [============================>.] - ETA: 0s - loss: 4.9809 - leftLayer1_loss: 0.0956 - midLayer1_loss: 1.4423 - rightLayer1_loss: 1.0464 - leftLayer2_loss: 0.0730 - midLayer2_loss: 1.4723 - rightLayer2_loss: 0.8513\n",
      "Epoch 00002: val_loss improved from 5.22038 to 4.85445, saving model to ./experiments/0.009_weights.h5\n",
      "7996/7996 [==============================] - 21s 3ms/step - loss: 4.9804 - leftLayer1_loss: 0.0956 - midLayer1_loss: 1.4422 - rightLayer1_loss: 1.0462 - leftLayer2_loss: 0.0730 - midLayer2_loss: 1.4723 - rightLayer2_loss: 0.8511 - val_loss: 4.8545 - val_leftLayer1_loss: 0.0895 - val_midLayer1_loss: 1.4317 - val_rightLayer1_loss: 0.9434 - val_leftLayer2_loss: 0.0820 - val_midLayer2_loss: 1.3520 - val_rightLayer2_loss: 0.9559\n",
      "Epoch 3/11\n",
      "7989/7996 [============================>.] - ETA: 0s - loss: 4.7911 - leftLayer1_loss: 0.0845 - midLayer1_loss: 1.4425 - rightLayer1_loss: 0.9195 - leftLayer2_loss: 0.0571 - midLayer2_loss: 1.4703 - rightLayer2_loss: 0.8172\n",
      "Epoch 00003: val_loss improved from 4.85445 to 4.70643, saving model to ./experiments/0.009_weights.h5\n",
      "7996/7996 [==============================] - 21s 3ms/step - loss: 4.7906 - leftLayer1_loss: 0.0845 - midLayer1_loss: 1.4424 - rightLayer1_loss: 0.9193 - leftLayer2_loss: 0.0571 - midLayer2_loss: 1.4703 - rightLayer2_loss: 0.8170 - val_loss: 4.7064 - val_leftLayer1_loss: 0.0796 - val_midLayer1_loss: 1.4317 - val_rightLayer1_loss: 0.8655 - val_leftLayer2_loss: 0.0714 - val_midLayer2_loss: 1.3520 - val_rightLayer2_loss: 0.9062\n",
      "Epoch 4/11\n",
      "7979/7996 [============================>.] - ETA: 0s - loss: 4.7078 - leftLayer1_loss: 0.0758 - midLayer1_loss: 1.4422 - rightLayer1_loss: 0.8680 - leftLayer2_loss: 0.0486 - midLayer2_loss: 1.4691 - rightLayer2_loss: 0.8042\n",
      "Epoch 00004: val_loss improved from 4.70643 to 4.62510, saving model to ./experiments/0.009_weights.h5\n",
      "7996/7996 [==============================] - 21s 3ms/step - loss: 4.7075 - leftLayer1_loss: 0.0758 - midLayer1_loss: 1.4421 - rightLayer1_loss: 0.8679 - leftLayer2_loss: 0.0486 - midLayer2_loss: 1.4690 - rightLayer2_loss: 0.8041 - val_loss: 4.6251 - val_leftLayer1_loss: 0.0717 - val_midLayer1_loss: 1.4317 - val_rightLayer1_loss: 0.8277 - val_leftLayer2_loss: 0.0645 - val_midLayer2_loss: 1.3520 - val_rightLayer2_loss: 0.8776\n",
      "Epoch 5/11\n",
      "7986/7996 [============================>.] - ETA: 0s - loss: 4.6616 - leftLayer1_loss: 0.0688 - midLayer1_loss: 1.4421 - rightLayer1_loss: 0.8412 - leftLayer2_loss: 0.0437 - midLayer2_loss: 1.4702 - rightLayer2_loss: 0.7957\n",
      "Epoch 00005: val_loss improved from 4.62510 to 4.57255, saving model to ./experiments/0.009_weights.h5\n",
      "7996/7996 [==============================] - 23s 3ms/step - loss: 4.6610 - leftLayer1_loss: 0.0688 - midLayer1_loss: 1.4420 - rightLayer1_loss: 0.8410 - leftLayer2_loss: 0.0437 - midLayer2_loss: 1.4701 - rightLayer2_loss: 0.7955 - val_loss: 4.5725 - val_leftLayer1_loss: 0.0654 - val_midLayer1_loss: 1.4317 - val_rightLayer1_loss: 0.8054 - val_leftLayer2_loss: 0.0597 - val_midLayer2_loss: 1.3520 - val_rightLayer2_loss: 0.8584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/11\n",
      "7987/7996 [============================>.] - ETA: 0s - loss: 4.6327 - leftLayer1_loss: 0.0633 - midLayer1_loss: 1.4423 - rightLayer1_loss: 0.8240 - leftLayer2_loss: 0.0405 - midLayer2_loss: 1.4716 - rightLayer2_loss: 0.7911\n",
      "Epoch 00006: val_loss improved from 4.57255 to 4.53509, saving model to ./experiments/0.009_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.6321 - leftLayer1_loss: 0.0633 - midLayer1_loss: 1.4422 - rightLayer1_loss: 0.8237 - leftLayer2_loss: 0.0405 - midLayer2_loss: 1.4715 - rightLayer2_loss: 0.7908 - val_loss: 4.5351 - val_leftLayer1_loss: 0.0603 - val_midLayer1_loss: 1.4317 - val_rightLayer1_loss: 0.7906 - val_leftLayer2_loss: 0.0561 - val_midLayer2_loss: 1.3520 - val_rightLayer2_loss: 0.8443\n",
      "Epoch 7/11\n",
      "7977/7996 [============================>.] - ETA: 0s - loss: 4.6109 - leftLayer1_loss: 0.0588 - midLayer1_loss: 1.4419 - rightLayer1_loss: 0.8124 - leftLayer2_loss: 0.0385 - midLayer2_loss: 1.4723 - rightLayer2_loss: 0.7871\n",
      "Epoch 00007: val_loss improved from 4.53509 to 4.50667, saving model to ./experiments/0.009_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.6106 - leftLayer1_loss: 0.0588 - midLayer1_loss: 1.4419 - rightLayer1_loss: 0.8122 - leftLayer2_loss: 0.0384 - midLayer2_loss: 1.4723 - rightLayer2_loss: 0.7869 - val_loss: 4.5067 - val_leftLayer1_loss: 0.0562 - val_midLayer1_loss: 1.4317 - val_rightLayer1_loss: 0.7800 - val_leftLayer2_loss: 0.0534 - val_midLayer2_loss: 1.3520 - val_rightLayer2_loss: 0.8333\n",
      "Epoch 8/11\n",
      "7975/7996 [============================>.] - ETA: 0s - loss: 4.5931 - leftLayer1_loss: 0.0552 - midLayer1_loss: 1.4417 - rightLayer1_loss: 0.8033 - leftLayer2_loss: 0.0370 - midLayer2_loss: 1.4720 - rightLayer2_loss: 0.7838\n",
      "Epoch 00008: val_loss improved from 4.50667 to 4.48420, saving model to ./experiments/0.009_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.5926 - leftLayer1_loss: 0.0552 - midLayer1_loss: 1.4417 - rightLayer1_loss: 0.8031 - leftLayer2_loss: 0.0370 - midLayer2_loss: 1.4720 - rightLayer2_loss: 0.7836 - val_loss: 4.4842 - val_leftLayer1_loss: 0.0528 - val_midLayer1_loss: 1.4317 - val_rightLayer1_loss: 0.7718 - val_leftLayer2_loss: 0.0513 - val_midLayer2_loss: 1.3520 - val_rightLayer2_loss: 0.8246\n",
      "Epoch 9/11\n",
      "7982/7996 [============================>.] - ETA: 0s - loss: 4.5771 - leftLayer1_loss: 0.0522 - midLayer1_loss: 1.4424 - rightLayer1_loss: 0.7961 - leftLayer2_loss: 0.0358 - midLayer2_loss: 1.4693 - rightLayer2_loss: 0.7814\n",
      "Epoch 00009: val_loss improved from 4.48420 to 4.46570, saving model to ./experiments/0.009_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.5767 - leftLayer1_loss: 0.0522 - midLayer1_loss: 1.4423 - rightLayer1_loss: 0.7960 - leftLayer2_loss: 0.0358 - midLayer2_loss: 1.4692 - rightLayer2_loss: 0.7813 - val_loss: 4.4657 - val_leftLayer1_loss: 0.0500 - val_midLayer1_loss: 1.4317 - val_rightLayer1_loss: 0.7653 - val_leftLayer2_loss: 0.0495 - val_midLayer2_loss: 1.3520 - val_rightLayer2_loss: 0.8171\n",
      "Epoch 10/11\n",
      "7983/7996 [============================>.] - ETA: 0s - loss: 4.5661 - leftLayer1_loss: 0.0497 - midLayer1_loss: 1.4421 - rightLayer1_loss: 0.7903 - leftLayer2_loss: 0.0350 - midLayer2_loss: 1.4701 - rightLayer2_loss: 0.7788\n",
      "Epoch 00010: val_loss improved from 4.46570 to 4.45021, saving model to ./experiments/0.009_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.5657 - leftLayer1_loss: 0.0497 - midLayer1_loss: 1.4421 - rightLayer1_loss: 0.7902 - leftLayer2_loss: 0.0350 - midLayer2_loss: 1.4702 - rightLayer2_loss: 0.7786 - val_loss: 4.4502 - val_leftLayer1_loss: 0.0477 - val_midLayer1_loss: 1.4317 - val_rightLayer1_loss: 0.7599 - val_leftLayer2_loss: 0.0480 - val_midLayer2_loss: 1.3520 - val_rightLayer2_loss: 0.8109\n",
      "Epoch 11/11\n",
      "7980/7996 [============================>.] - ETA: 0s - loss: 4.5581 - leftLayer1_loss: 0.0476 - midLayer1_loss: 1.4424 - rightLayer1_loss: 0.7856 - leftLayer2_loss: 0.0344 - midLayer2_loss: 1.4714 - rightLayer2_loss: 0.7766\n",
      "Epoch 00011: val_loss improved from 4.45021 to 4.43690, saving model to ./experiments/0.009_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.5580 - leftLayer1_loss: 0.0476 - midLayer1_loss: 1.4424 - rightLayer1_loss: 0.7856 - leftLayer2_loss: 0.0344 - midLayer2_loss: 1.4714 - rightLayer2_loss: 0.7766 - val_loss: 4.4369 - val_leftLayer1_loss: 0.0457 - val_midLayer1_loss: 1.4317 - val_rightLayer1_loss: 0.7553 - val_leftLayer2_loss: 0.0468 - val_midLayer2_loss: 1.3520 - val_rightLayer2_loss: 0.8054\n",
      "WARNING:tensorflow:From <ipython-input-15-3539473a5eed>:61: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.009_test.log **\n",
      "auroc 0Atelectasis: 0.5739898611357163\n",
      "\n",
      "auprc 0Atelectasis: 0.12614596044268214\n",
      "\n",
      "auroc 1Atelectasis: 0.4176151100957931\n",
      "\n",
      "auprc 1Atelectasis: 0.08807766719614753\n",
      "\n",
      "auroc 2Atelectasis: 0.382687926401558\n",
      "\n",
      "auprc 2Atelectasis: 0.07986096607359569\n",
      "\n",
      "auroc 3Atelectasis: 0.45536127756627615\n",
      "\n",
      "auprc 3Atelectasis: 0.09120285828918252\n",
      "\n",
      "auroc 4Atelectasis: 0.667759262264652\n",
      "\n",
      "auprc 4Atelectasis: 0.16174317439883035\n",
      "\n",
      "auroc 5Atelectasis: 0.4796155536091623\n",
      "\n",
      "auprc 5Atelectasis: 0.09745804113478233\n",
      "\n",
      "mean auroc: 0.496171498512193\n",
      "\n",
      "mean auprc: 0.10741477792253674\n",
      "\n",
      "max auroc: 0.667759262264652\n",
      "\n",
      "max auprc: 0.16174317439883035\n",
      "\n",
      "256.4504482746124\n",
      "** set output weights path to: ./experiments/0.009999999999999998_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 7996 steps, validate for 1119 steps\n",
      "Epoch 1/11\n",
      "7979/7996 [============================>.] - ETA: 0s - loss: 5.5507 - leftLayer1_loss: 0.1181 - midLayer1_loss: 1.4091 - rightLayer1_loss: 1.4232 - leftLayer2_loss: 0.1009 - midLayer2_loss: 1.3178 - rightLayer2_loss: 1.1817\n",
      "Epoch 00001: val_loss improved from inf to 5.11639, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 21s 3ms/step - loss: 5.5492 - leftLayer1_loss: 0.1181 - midLayer1_loss: 1.4091 - rightLayer1_loss: 1.4225 - leftLayer2_loss: 0.1008 - midLayer2_loss: 1.3177 - rightLayer2_loss: 1.1810 - val_loss: 5.1164 - val_leftLayer1_loss: 0.1086 - val_midLayer1_loss: 1.3939 - val_rightLayer1_loss: 1.1135 - val_leftLayer2_loss: 0.0976 - val_midLayer2_loss: 1.2851 - val_rightLayer2_loss: 1.1176\n",
      "Epoch 2/11\n",
      "7992/7996 [============================>.] - ETA: 0s - loss: 4.7710 - leftLayer1_loss: 0.1010 - midLayer1_loss: 1.4095 - rightLayer1_loss: 1.0100 - leftLayer2_loss: 0.0730 - midLayer2_loss: 1.3163 - rightLayer2_loss: 0.8612\n",
      "Epoch 00002: val_loss improved from 5.11639 to 4.75822, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.7707 - leftLayer1_loss: 0.1010 - midLayer1_loss: 1.4094 - rightLayer1_loss: 1.0099 - leftLayer2_loss: 0.0730 - midLayer2_loss: 1.3163 - rightLayer2_loss: 0.8611 - val_loss: 4.7582 - val_leftLayer1_loss: 0.0934 - val_midLayer1_loss: 1.3939 - val_rightLayer1_loss: 0.9158 - val_leftLayer2_loss: 0.0827 - val_midLayer2_loss: 1.2851 - val_rightLayer2_loss: 0.9874\n",
      "Epoch 3/11\n",
      "7987/7996 [============================>.] - ETA: 0s - loss: 4.5904 - leftLayer1_loss: 0.0877 - midLayer1_loss: 1.4093 - rightLayer1_loss: 0.8965 - leftLayer2_loss: 0.0584 - midLayer2_loss: 1.3161 - rightLayer2_loss: 0.8225\n",
      "Epoch 00003: val_loss improved from 4.75822 to 4.61292, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.5897 - leftLayer1_loss: 0.0877 - midLayer1_loss: 1.4092 - rightLayer1_loss: 0.8963 - leftLayer2_loss: 0.0584 - midLayer2_loss: 1.3160 - rightLayer2_loss: 0.8222 - val_loss: 4.6129 - val_leftLayer1_loss: 0.0816 - val_midLayer1_loss: 1.3939 - val_rightLayer1_loss: 0.8471 - val_leftLayer2_loss: 0.0732 - val_midLayer2_loss: 1.2851 - val_rightLayer2_loss: 0.9321\n",
      "Epoch 4/11\n",
      "7993/7996 [============================>.] - ETA: 0s - loss: 4.5121 - leftLayer1_loss: 0.0774 - midLayer1_loss: 1.4092 - rightLayer1_loss: 0.8510 - leftLayer2_loss: 0.0501 - midLayer2_loss: 1.3175 - rightLayer2_loss: 0.8069\n",
      "Epoch 00004: val_loss improved from 4.61292 to 4.53192, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.5120 - leftLayer1_loss: 0.0774 - midLayer1_loss: 1.4092 - rightLayer1_loss: 0.8510 - leftLayer2_loss: 0.0501 - midLayer2_loss: 1.3174 - rightLayer2_loss: 0.8068 - val_loss: 4.5319 - val_leftLayer1_loss: 0.0725 - val_midLayer1_loss: 1.3939 - val_rightLayer1_loss: 0.8136 - val_leftLayer2_loss: 0.0667 - val_midLayer2_loss: 1.2851 - val_rightLayer2_loss: 0.9001\n",
      "Epoch 5/11\n",
      "7994/7996 [============================>.] - ETA: 0s - loss: 4.4640 - leftLayer1_loss: 0.0694 - midLayer1_loss: 1.4096 - rightLayer1_loss: 0.8263 - leftLayer2_loss: 0.0450 - midLayer2_loss: 1.3161 - rightLayer2_loss: 0.7976\n",
      "Epoch 00005: val_loss improved from 4.53192 to 4.47902, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.4641 - leftLayer1_loss: 0.0694 - midLayer1_loss: 1.4096 - rightLayer1_loss: 0.8263 - leftLayer2_loss: 0.0450 - midLayer2_loss: 1.3161 - rightLayer2_loss: 0.7977 - val_loss: 4.4790 - val_leftLayer1_loss: 0.0654 - val_midLayer1_loss: 1.3939 - val_rightLayer1_loss: 0.7937 - val_leftLayer2_loss: 0.0621 - val_midLayer2_loss: 1.2851 - val_rightLayer2_loss: 0.8788\n",
      "Epoch 6/11\n",
      "7978/7996 [============================>.] - ETA: 0s - loss: 4.4359 - leftLayer1_loss: 0.0633 - midLayer1_loss: 1.4090 - rightLayer1_loss: 0.8113 - leftLayer2_loss: 0.0419 - midLayer2_loss: 1.3177 - rightLayer2_loss: 0.7926\n",
      "Epoch 00006: val_loss improved from 4.47902 to 4.44093, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.4354 - leftLayer1_loss: 0.0633 - midLayer1_loss: 1.4089 - rightLayer1_loss: 0.8112 - leftLayer2_loss: 0.0419 - midLayer2_loss: 1.3176 - rightLayer2_loss: 0.7925 - val_loss: 4.4409 - val_leftLayer1_loss: 0.0598 - val_midLayer1_loss: 1.3939 - val_rightLayer1_loss: 0.7804 - val_leftLayer2_loss: 0.0586 - val_midLayer2_loss: 1.2851 - val_rightLayer2_loss: 0.8631\n",
      "Epoch 7/11\n",
      "7993/7996 [============================>.] - ETA: 0s - loss: 4.4103 - leftLayer1_loss: 0.0584 - midLayer1_loss: 1.4092 - rightLayer1_loss: 0.8002 - leftLayer2_loss: 0.0396 - midLayer2_loss: 1.3157 - rightLayer2_loss: 0.7873\n",
      "Epoch 00007: val_loss improved from 4.44093 to 4.41199, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.4102 - leftLayer1_loss: 0.0584 - midLayer1_loss: 1.4092 - rightLayer1_loss: 0.8002 - leftLayer2_loss: 0.0396 - midLayer2_loss: 1.3157 - rightLayer2_loss: 0.7873 - val_loss: 4.4120 - val_leftLayer1_loss: 0.0554 - val_midLayer1_loss: 1.3939 - val_rightLayer1_loss: 0.7707 - val_leftLayer2_loss: 0.0559 - val_midLayer2_loss: 1.2851 - val_rightLayer2_loss: 0.8510\n",
      "Epoch 8/11\n",
      "7994/7996 [============================>.] - ETA: 0s - loss: 4.3921 - leftLayer1_loss: 0.0545 - midLayer1_loss: 1.4096 - rightLayer1_loss: 0.7917 - leftLayer2_loss: 0.0379 - midLayer2_loss: 1.3135 - rightLayer2_loss: 0.7848\n",
      "Epoch 00008: val_loss improved from 4.41199 to 4.38899, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.3922 - leftLayer1_loss: 0.0545 - midLayer1_loss: 1.4096 - rightLayer1_loss: 0.7918 - leftLayer2_loss: 0.0379 - midLayer2_loss: 1.3135 - rightLayer2_loss: 0.7849 - val_loss: 4.3890 - val_leftLayer1_loss: 0.0519 - val_midLayer1_loss: 1.3939 - val_rightLayer1_loss: 0.7632 - val_leftLayer2_loss: 0.0537 - val_midLayer2_loss: 1.2851 - val_rightLayer2_loss: 0.8412\n",
      "Epoch 9/11\n",
      "7979/7996 [============================>.] - ETA: 0s - loss: 4.3808 - leftLayer1_loss: 0.0513 - midLayer1_loss: 1.4092 - rightLayer1_loss: 0.7851 - leftLayer2_loss: 0.0367 - midLayer2_loss: 1.3172 - rightLayer2_loss: 0.7812\n",
      "Epoch 00009: val_loss improved from 4.38899 to 4.37030, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 21s 3ms/step - loss: 4.3806 - leftLayer1_loss: 0.0513 - midLayer1_loss: 1.4092 - rightLayer1_loss: 0.7851 - leftLayer2_loss: 0.0367 - midLayer2_loss: 1.3171 - rightLayer2_loss: 0.7812 - val_loss: 4.3703 - val_leftLayer1_loss: 0.0490 - val_midLayer1_loss: 1.3939 - val_rightLayer1_loss: 0.7572 - val_leftLayer2_loss: 0.0519 - val_midLayer2_loss: 1.2851 - val_rightLayer2_loss: 0.8333\n",
      "Epoch 10/11\n",
      "7981/7996 [============================>.] - ETA: 0s - loss: 4.3688 - leftLayer1_loss: 0.0488 - midLayer1_loss: 1.4094 - rightLayer1_loss: 0.7799 - leftLayer2_loss: 0.0358 - midLayer2_loss: 1.3155 - rightLayer2_loss: 0.7794\n",
      "Epoch 00010: val_loss improved from 4.37030 to 4.35441, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 21s 3ms/step - loss: 4.3683 - leftLayer1_loss: 0.0488 - midLayer1_loss: 1.4094 - rightLayer1_loss: 0.7799 - leftLayer2_loss: 0.0358 - midLayer2_loss: 1.3152 - rightLayer2_loss: 0.7794 - val_loss: 4.3544 - val_leftLayer1_loss: 0.0466 - val_midLayer1_loss: 1.3939 - val_rightLayer1_loss: 0.7521 - val_leftLayer2_loss: 0.0504 - val_midLayer2_loss: 1.2851 - val_rightLayer2_loss: 0.8263\n",
      "Epoch 11/11\n",
      "7994/7996 [============================>.] - ETA: 0s - loss: 4.3600 - leftLayer1_loss: 0.0466 - midLayer1_loss: 1.4095 - rightLayer1_loss: 0.7747 - leftLayer2_loss: 0.0350 - midLayer2_loss: 1.3178 - rightLayer2_loss: 0.7764\n",
      "Epoch 00011: val_loss improved from 4.35441 to 4.34090, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.3601 - leftLayer1_loss: 0.0466 - midLayer1_loss: 1.4095 - rightLayer1_loss: 0.7748 - leftLayer2_loss: 0.0350 - midLayer2_loss: 1.3177 - rightLayer2_loss: 0.7765 - val_loss: 4.3409 - val_leftLayer1_loss: 0.0446 - val_midLayer1_loss: 1.3939 - val_rightLayer1_loss: 0.7478 - val_leftLayer2_loss: 0.0491 - val_midLayer2_loss: 1.2851 - val_rightLayer2_loss: 0.8205\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.009999999999999998_test.log **\n",
      "auroc 0Atelectasis: 0.38866628220582244\n",
      "\n",
      "auprc 0Atelectasis: 0.0813312201528039\n",
      "\n",
      "auroc 1Atelectasis: 0.3508793148090105\n",
      "\n",
      "auprc 1Atelectasis: 0.0765507918465675\n",
      "\n",
      "auroc 2Atelectasis: 0.44375454508288625\n",
      "\n",
      "auprc 2Atelectasis: 0.09117301915713016\n",
      "\n",
      "auroc 3Atelectasis: 0.3925018985593248\n",
      "\n",
      "auprc 3Atelectasis: 0.08208717370425926\n",
      "\n",
      "auroc 4Atelectasis: 0.4317169046731195\n",
      "\n",
      "auprc 4Atelectasis: 0.08705984047015258\n",
      "\n",
      "auroc 5Atelectasis: 0.4094452552122112\n",
      "\n",
      "auprc 5Atelectasis: 0.08383435596323044\n",
      "\n",
      "mean auroc: 0.4028273667570625\n",
      "\n",
      "mean auprc: 0.08367273354902398\n",
      "\n",
      "max auroc: 0.44375454508288625\n",
      "\n",
      "max auprc: 0.09117301915713016\n",
      "\n",
      "252.7267026901245\n",
      "** set output weights path to: ./experiments/0.010999999999999998_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 7996 steps, validate for 1119 steps\n",
      "Epoch 1/11\n",
      "7986/7996 [============================>.] - ETA: 0s - loss: 5.4494 - leftLayer1_loss: 0.1176 - midLayer1_loss: 1.3209 - rightLayer1_loss: 1.4026 - leftLayer2_loss: 0.0999 - midLayer2_loss: 1.3717 - rightLayer2_loss: 1.1367\n",
      "Epoch 00001: val_loss improved from inf to 4.99787, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 21s 3ms/step - loss: 5.4483 - leftLayer1_loss: 0.1176 - midLayer1_loss: 1.3208 - rightLayer1_loss: 1.4021 - leftLayer2_loss: 0.0998 - midLayer2_loss: 1.3716 - rightLayer2_loss: 1.1362 - val_loss: 4.9979 - val_leftLayer1_loss: 0.1080 - val_midLayer1_loss: 1.3096 - val_rightLayer1_loss: 1.0940 - val_leftLayer2_loss: 0.0933 - val_midLayer2_loss: 1.3226 - val_rightLayer2_loss: 1.0704\n",
      "Epoch 2/11\n",
      "7982/7996 [============================>.] - ETA: 0s - loss: 4.7095 - leftLayer1_loss: 0.1002 - midLayer1_loss: 1.3206 - rightLayer1_loss: 0.9981 - leftLayer2_loss: 0.0696 - midLayer2_loss: 1.3702 - rightLayer2_loss: 0.8508\n",
      "Epoch 00002: val_loss improved from 4.99787 to 4.65845, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 21s 3ms/step - loss: 4.7090 - leftLayer1_loss: 0.1002 - midLayer1_loss: 1.3205 - rightLayer1_loss: 0.9979 - leftLayer2_loss: 0.0696 - midLayer2_loss: 1.3701 - rightLayer2_loss: 0.8507 - val_loss: 4.6584 - val_leftLayer1_loss: 0.0925 - val_midLayer1_loss: 1.3096 - val_rightLayer1_loss: 0.9042 - val_leftLayer2_loss: 0.0773 - val_midLayer2_loss: 1.3226 - val_rightLayer2_loss: 0.9523\n",
      "Epoch 3/11\n",
      "7979/7996 [============================>.] - ETA: 0s - loss: 4.5399 - leftLayer1_loss: 0.0867 - midLayer1_loss: 1.3207 - rightLayer1_loss: 0.8899 - leftLayer2_loss: 0.0551 - midLayer2_loss: 1.3704 - rightLayer2_loss: 0.8171\n",
      "Epoch 00003: val_loss improved from 4.65845 to 4.52266, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 21s 3ms/step - loss: 4.5397 - leftLayer1_loss: 0.0867 - midLayer1_loss: 1.3207 - rightLayer1_loss: 0.8898 - leftLayer2_loss: 0.0551 - midLayer2_loss: 1.3704 - rightLayer2_loss: 0.8170 - val_loss: 4.5227 - val_leftLayer1_loss: 0.0806 - val_midLayer1_loss: 1.3096 - val_rightLayer1_loss: 0.8391 - val_leftLayer2_loss: 0.0676 - val_midLayer2_loss: 1.3226 - val_rightLayer2_loss: 0.9032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/11\n",
      "7984/7996 [============================>.] - ETA: 0s - loss: 4.4671 - leftLayer1_loss: 0.0764 - midLayer1_loss: 1.3204 - rightLayer1_loss: 0.8470 - leftLayer2_loss: 0.0472 - midLayer2_loss: 1.3721 - rightLayer2_loss: 0.8041\n",
      "Epoch 00004: val_loss improved from 4.52266 to 4.44738, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.4665 - leftLayer1_loss: 0.0764 - midLayer1_loss: 1.3203 - rightLayer1_loss: 0.8468 - leftLayer2_loss: 0.0472 - midLayer2_loss: 1.3720 - rightLayer2_loss: 0.8039 - val_loss: 4.4474 - val_leftLayer1_loss: 0.0714 - val_midLayer1_loss: 1.3096 - val_rightLayer1_loss: 0.8077 - val_leftLayer2_loss: 0.0613 - val_midLayer2_loss: 1.3226 - val_rightLayer2_loss: 0.8748\n",
      "Epoch 5/11\n",
      "7987/7996 [============================>.] - ETA: 0s - loss: 4.4256 - leftLayer1_loss: 0.0684 - midLayer1_loss: 1.3204 - rightLayer1_loss: 0.8243 - leftLayer2_loss: 0.0428 - midLayer2_loss: 1.3738 - rightLayer2_loss: 0.7959\n",
      "Epoch 00005: val_loss improved from 4.44738 to 4.39836, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.4248 - leftLayer1_loss: 0.0684 - midLayer1_loss: 1.3203 - rightLayer1_loss: 0.8240 - leftLayer2_loss: 0.0428 - midLayer2_loss: 1.3737 - rightLayer2_loss: 0.7956 - val_loss: 4.3984 - val_leftLayer1_loss: 0.0644 - val_midLayer1_loss: 1.3096 - val_rightLayer1_loss: 0.7891 - val_leftLayer2_loss: 0.0569 - val_midLayer2_loss: 1.3226 - val_rightLayer2_loss: 0.8558\n",
      "Epoch 6/11\n",
      "7984/7996 [============================>.] - ETA: 0s - loss: 4.3977 - leftLayer1_loss: 0.0623 - midLayer1_loss: 1.3206 - rightLayer1_loss: 0.8100 - leftLayer2_loss: 0.0398 - midLayer2_loss: 1.3733 - rightLayer2_loss: 0.7918\n",
      "Epoch 00006: val_loss improved from 4.39836 to 4.36327, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.3972 - leftLayer1_loss: 0.0623 - midLayer1_loss: 1.3205 - rightLayer1_loss: 0.8098 - leftLayer2_loss: 0.0398 - midLayer2_loss: 1.3732 - rightLayer2_loss: 0.7917 - val_loss: 4.3633 - val_leftLayer1_loss: 0.0588 - val_midLayer1_loss: 1.3096 - val_rightLayer1_loss: 0.7767 - val_leftLayer2_loss: 0.0536 - val_midLayer2_loss: 1.3226 - val_rightLayer2_loss: 0.8419\n",
      "Epoch 7/11\n",
      "7993/7996 [============================>.] - ETA: 0s - loss: 4.3742 - leftLayer1_loss: 0.0574 - midLayer1_loss: 1.3207 - rightLayer1_loss: 0.7998 - leftLayer2_loss: 0.0378 - midLayer2_loss: 1.3713 - rightLayer2_loss: 0.7871\n",
      "Epoch 00007: val_loss improved from 4.36327 to 4.33676, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.3741 - leftLayer1_loss: 0.0574 - midLayer1_loss: 1.3207 - rightLayer1_loss: 0.7998 - leftLayer2_loss: 0.0378 - midLayer2_loss: 1.3713 - rightLayer2_loss: 0.7871 - val_loss: 4.3368 - val_leftLayer1_loss: 0.0545 - val_midLayer1_loss: 1.3096 - val_rightLayer1_loss: 0.7677 - val_leftLayer2_loss: 0.0511 - val_midLayer2_loss: 1.3226 - val_rightLayer2_loss: 0.8313\n",
      "Epoch 8/11\n",
      "7989/7996 [============================>.] - ETA: 0s - loss: 4.3608 - leftLayer1_loss: 0.0536 - midLayer1_loss: 1.3208 - rightLayer1_loss: 0.7921 - leftLayer2_loss: 0.0363 - midLayer2_loss: 1.3728 - rightLayer2_loss: 0.7852\n",
      "Epoch 00008: val_loss improved from 4.33676 to 4.31562, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.3601 - leftLayer1_loss: 0.0536 - midLayer1_loss: 1.3207 - rightLayer1_loss: 0.7919 - leftLayer2_loss: 0.0363 - midLayer2_loss: 1.3727 - rightLayer2_loss: 0.7849 - val_loss: 4.3156 - val_leftLayer1_loss: 0.0510 - val_midLayer1_loss: 1.3096 - val_rightLayer1_loss: 0.7607 - val_leftLayer2_loss: 0.0491 - val_midLayer2_loss: 1.3226 - val_rightLayer2_loss: 0.8226\n",
      "Epoch 9/11\n",
      "7993/7996 [============================>.] - ETA: 0s - loss: 4.3458 - leftLayer1_loss: 0.0505 - midLayer1_loss: 1.3208 - rightLayer1_loss: 0.7859 - leftLayer2_loss: 0.0353 - midLayer2_loss: 1.3713 - rightLayer2_loss: 0.7821\n",
      "Epoch 00009: val_loss improved from 4.31562 to 4.29827, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.3458 - leftLayer1_loss: 0.0505 - midLayer1_loss: 1.3208 - rightLayer1_loss: 0.7858 - leftLayer2_loss: 0.0353 - midLayer2_loss: 1.3713 - rightLayer2_loss: 0.7820 - val_loss: 4.2983 - val_leftLayer1_loss: 0.0481 - val_midLayer1_loss: 1.3096 - val_rightLayer1_loss: 0.7551 - val_leftLayer2_loss: 0.0475 - val_midLayer2_loss: 1.3226 - val_rightLayer2_loss: 0.8153\n",
      "Epoch 10/11\n",
      "7993/7996 [============================>.] - ETA: 0s - loss: 4.3354 - leftLayer1_loss: 0.0480 - midLayer1_loss: 1.3210 - rightLayer1_loss: 0.7808 - leftLayer2_loss: 0.0346 - midLayer2_loss: 1.3710 - rightLayer2_loss: 0.7800\n",
      "Epoch 00010: val_loss improved from 4.29827 to 4.28364, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.3354 - leftLayer1_loss: 0.0480 - midLayer1_loss: 1.3209 - rightLayer1_loss: 0.7808 - leftLayer2_loss: 0.0346 - midLayer2_loss: 1.3711 - rightLayer2_loss: 0.7800 - val_loss: 4.2836 - val_leftLayer1_loss: 0.0458 - val_midLayer1_loss: 1.3096 - val_rightLayer1_loss: 0.7504 - val_leftLayer2_loss: 0.0462 - val_midLayer2_loss: 1.3226 - val_rightLayer2_loss: 0.8091\n",
      "Epoch 11/11\n",
      "7986/7996 [============================>.] - ETA: 0s - loss: 4.3246 - leftLayer1_loss: 0.0459 - midLayer1_loss: 1.3207 - rightLayer1_loss: 0.7764 - leftLayer2_loss: 0.0339 - midLayer2_loss: 1.3688 - rightLayer2_loss: 0.7789\n",
      "Epoch 00011: val_loss improved from 4.28364 to 4.27099, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "7996/7996 [==============================] - 19s 2ms/step - loss: 4.3239 - leftLayer1_loss: 0.0459 - midLayer1_loss: 1.3205 - rightLayer1_loss: 0.7761 - leftLayer2_loss: 0.0339 - midLayer2_loss: 1.3688 - rightLayer2_loss: 0.7787 - val_loss: 4.2710 - val_leftLayer1_loss: 0.0438 - val_midLayer1_loss: 1.3096 - val_rightLayer1_loss: 0.7463 - val_leftLayer2_loss: 0.0450 - val_midLayer2_loss: 1.3226 - val_rightLayer2_loss: 0.8036\n",
      "22433/22433 [==============================] - 27s 1ms/step\n",
      "** write log to ./experiments/0.010999999999999998_test.log **\n",
      "auroc 0Atelectasis: 0.3907062888461343\n",
      "\n",
      "auprc 0Atelectasis: 0.08131212465314928\n",
      "\n",
      "auroc 1Atelectasis: 0.6083918077216752\n",
      "\n",
      "auprc 1Atelectasis: 0.1538760997823332\n",
      "\n",
      "auroc 2Atelectasis: 0.39445473045825996\n",
      "\n",
      "auprc 2Atelectasis: 0.08153863815529246\n",
      "\n",
      "auroc 3Atelectasis: 0.5111992287657651\n",
      "\n",
      "auprc 3Atelectasis: 0.10655455656526013\n",
      "\n",
      "auroc 4Atelectasis: 0.6824147155588537\n",
      "\n",
      "auprc 4Atelectasis: 0.19323565270633214\n",
      "\n",
      "auroc 5Atelectasis: 0.3574350845504141\n",
      "\n",
      "auprc 5Atelectasis: 0.07665057454206187\n",
      "\n",
      "mean auroc: 0.490766975983517\n",
      "\n",
      "mean auprc: 0.11552794106740483\n",
      "\n",
      "max auroc: 0.6824147155588537\n",
      "\n",
      "max auprc: 0.19323565270633214\n",
      "\n",
      "250.58568811416626\n",
      "** set output weights path to: ./experiments/0.011999999999999997_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 7996 steps, validate for 1119 steps\n",
      "Epoch 1/11\n",
      "7975/7996 [============================>.] - ETA: 0s - loss: 5.5864 - leftLayer1_loss: 0.1164 - midLayer1_loss: 1.3733 - rightLayer1_loss: 1.4235 - leftLayer2_loss: 0.1090 - midLayer2_loss: 1.4308 - rightLayer2_loss: 1.1334\n",
      "Epoch 00001: val_loss improved from inf to 5.08257, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 5.5845 - leftLayer1_loss: 0.1164 - midLayer1_loss: 1.3732 - rightLayer1_loss: 1.4226 - leftLayer2_loss: 0.1090 - midLayer2_loss: 1.4308 - rightLayer2_loss: 1.1326 - val_loss: 5.0826 - val_leftLayer1_loss: 0.1072 - val_midLayer1_loss: 1.3576 - val_rightLayer1_loss: 1.1115 - val_leftLayer2_loss: 0.0981 - val_midLayer2_loss: 1.3463 - val_rightLayer2_loss: 1.0617\n",
      "Epoch 2/11\n",
      "7974/7996 [============================>.] - ETA: 0s - loss: 4.8392 - leftLayer1_loss: 0.0994 - midLayer1_loss: 1.3735 - rightLayer1_loss: 1.0101 - leftLayer2_loss: 0.0751 - midLayer2_loss: 1.4323 - rightLayer2_loss: 0.8489\n",
      "Epoch 00002: val_loss improved from 5.08257 to 4.73689, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.8385 - leftLayer1_loss: 0.0994 - midLayer1_loss: 1.3734 - rightLayer1_loss: 1.0097 - leftLayer2_loss: 0.0750 - midLayer2_loss: 1.4322 - rightLayer2_loss: 0.8487 - val_loss: 4.7369 - val_leftLayer1_loss: 0.0921 - val_midLayer1_loss: 1.3576 - val_rightLayer1_loss: 0.9165 - val_leftLayer2_loss: 0.0804 - val_midLayer2_loss: 1.3463 - val_rightLayer2_loss: 0.9440\n",
      "Epoch 3/11\n",
      "7979/7996 [============================>.] - ETA: 0s - loss: 4.6625 - leftLayer1_loss: 0.0862 - midLayer1_loss: 1.3730 - rightLayer1_loss: 0.8993 - leftLayer2_loss: 0.0580 - midLayer2_loss: 1.4323 - rightLayer2_loss: 0.8137\n",
      "Epoch 00003: val_loss improved from 4.73689 to 4.59879, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.6621 - leftLayer1_loss: 0.0862 - midLayer1_loss: 1.3730 - rightLayer1_loss: 0.8991 - leftLayer2_loss: 0.0580 - midLayer2_loss: 1.4322 - rightLayer2_loss: 0.8136 - val_loss: 4.5988 - val_leftLayer1_loss: 0.0804 - val_midLayer1_loss: 1.3576 - val_rightLayer1_loss: 0.8491 - val_leftLayer2_loss: 0.0698 - val_midLayer2_loss: 1.3463 - val_rightLayer2_loss: 0.8956\n",
      "Epoch 4/11\n",
      "7976/7996 [============================>.] - ETA: 0s - loss: 4.5804 - leftLayer1_loss: 0.0761 - midLayer1_loss: 1.3732 - rightLayer1_loss: 0.8550 - leftLayer2_loss: 0.0490 - midLayer2_loss: 1.4281 - rightLayer2_loss: 0.7990\n",
      "Epoch 00004: val_loss improved from 4.59879 to 4.52254, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.5798 - leftLayer1_loss: 0.0760 - midLayer1_loss: 1.3731 - rightLayer1_loss: 0.8548 - leftLayer2_loss: 0.0490 - midLayer2_loss: 1.4281 - rightLayer2_loss: 0.7988 - val_loss: 4.5225 - val_leftLayer1_loss: 0.0714 - val_midLayer1_loss: 1.3576 - val_rightLayer1_loss: 0.8164 - val_leftLayer2_loss: 0.0629 - val_midLayer2_loss: 1.3463 - val_rightLayer2_loss: 0.8679\n",
      "Epoch 5/11\n",
      "7987/7996 [============================>.] - ETA: 0s - loss: 4.5383 - leftLayer1_loss: 0.0683 - midLayer1_loss: 1.3731 - rightLayer1_loss: 0.8314 - leftLayer2_loss: 0.0440 - midLayer2_loss: 1.4291 - rightLayer2_loss: 0.7924\n",
      "Epoch 00005: val_loss improved from 4.52254 to 4.47255, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.5375 - leftLayer1_loss: 0.0683 - midLayer1_loss: 1.3730 - rightLayer1_loss: 0.8312 - leftLayer2_loss: 0.0440 - midLayer2_loss: 1.4290 - rightLayer2_loss: 0.7921 - val_loss: 4.4726 - val_leftLayer1_loss: 0.0645 - val_midLayer1_loss: 1.3576 - val_rightLayer1_loss: 0.7969 - val_leftLayer2_loss: 0.0581 - val_midLayer2_loss: 1.3463 - val_rightLayer2_loss: 0.8491\n",
      "Epoch 6/11\n",
      "7981/7996 [============================>.] - ETA: 0s - loss: 4.5107 - leftLayer1_loss: 0.0623 - midLayer1_loss: 1.3731 - rightLayer1_loss: 0.8166 - leftLayer2_loss: 0.0407 - midLayer2_loss: 1.4314 - rightLayer2_loss: 0.7867\n",
      "Epoch 00006: val_loss improved from 4.47255 to 4.43681, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.5104 - leftLayer1_loss: 0.0623 - midLayer1_loss: 1.3730 - rightLayer1_loss: 0.8166 - leftLayer2_loss: 0.0407 - midLayer2_loss: 1.4313 - rightLayer2_loss: 0.7866 - val_loss: 4.4368 - val_leftLayer1_loss: 0.0590 - val_midLayer1_loss: 1.3576 - val_rightLayer1_loss: 0.7839 - val_leftLayer2_loss: 0.0546 - val_midLayer2_loss: 1.3463 - val_rightLayer2_loss: 0.8352\n",
      "Epoch 7/11\n",
      "7985/7996 [============================>.] - ETA: 0s - loss: 4.4884 - leftLayer1_loss: 0.0575 - midLayer1_loss: 1.3730 - rightLayer1_loss: 0.8064 - leftLayer2_loss: 0.0385 - midLayer2_loss: 1.4301 - rightLayer2_loss: 0.7829\n",
      "Epoch 00007: val_loss improved from 4.43681 to 4.40967, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.4877 - leftLayer1_loss: 0.0575 - midLayer1_loss: 1.3729 - rightLayer1_loss: 0.8062 - leftLayer2_loss: 0.0385 - midLayer2_loss: 1.4299 - rightLayer2_loss: 0.7827 - val_loss: 4.4097 - val_leftLayer1_loss: 0.0547 - val_midLayer1_loss: 1.3576 - val_rightLayer1_loss: 0.7745 - val_leftLayer2_loss: 0.0520 - val_midLayer2_loss: 1.3463 - val_rightLayer2_loss: 0.8245\n",
      "Epoch 8/11\n",
      "7976/7996 [============================>.] - ETA: 0s - loss: 4.4721 - leftLayer1_loss: 0.0537 - midLayer1_loss: 1.3731 - rightLayer1_loss: 0.7984 - leftLayer2_loss: 0.0370 - midLayer2_loss: 1.4307 - rightLayer2_loss: 0.7792\n",
      "Epoch 00008: val_loss improved from 4.40967 to 4.38827, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.4715 - leftLayer1_loss: 0.0537 - midLayer1_loss: 1.3730 - rightLayer1_loss: 0.7982 - leftLayer2_loss: 0.0370 - midLayer2_loss: 1.4306 - rightLayer2_loss: 0.7790 - val_loss: 4.3883 - val_leftLayer1_loss: 0.0512 - val_midLayer1_loss: 1.3576 - val_rightLayer1_loss: 0.7672 - val_leftLayer2_loss: 0.0499 - val_midLayer2_loss: 1.3463 - val_rightLayer2_loss: 0.8160\n",
      "Epoch 9/11\n",
      "7988/7996 [============================>.] - ETA: 0s - loss: 4.4569 - leftLayer1_loss: 0.0506 - midLayer1_loss: 1.3729 - rightLayer1_loss: 0.7920 - leftLayer2_loss: 0.0357 - midLayer2_loss: 1.4301 - rightLayer2_loss: 0.7756\n",
      "Epoch 00009: val_loss improved from 4.38827 to 4.37074, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.4563 - leftLayer1_loss: 0.0506 - midLayer1_loss: 1.3728 - rightLayer1_loss: 0.7919 - leftLayer2_loss: 0.0357 - midLayer2_loss: 1.4300 - rightLayer2_loss: 0.7754 - val_loss: 4.3707 - val_leftLayer1_loss: 0.0484 - val_midLayer1_loss: 1.3576 - val_rightLayer1_loss: 0.7613 - val_leftLayer2_loss: 0.0482 - val_midLayer2_loss: 1.3463 - val_rightLayer2_loss: 0.8089\n",
      "Epoch 10/11\n",
      "7975/7996 [============================>.] - ETA: 0s - loss: 4.4457 - leftLayer1_loss: 0.0481 - midLayer1_loss: 1.3736 - rightLayer1_loss: 0.7866 - leftLayer2_loss: 0.0349 - midLayer2_loss: 1.4286 - rightLayer2_loss: 0.7739\n",
      "Epoch 00010: val_loss improved from 4.37074 to 4.35585, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.4452 - leftLayer1_loss: 0.0481 - midLayer1_loss: 1.3735 - rightLayer1_loss: 0.7864 - leftLayer2_loss: 0.0349 - midLayer2_loss: 1.4286 - rightLayer2_loss: 0.7737 - val_loss: 4.3559 - val_leftLayer1_loss: 0.0461 - val_midLayer1_loss: 1.3576 - val_rightLayer1_loss: 0.7563 - val_leftLayer2_loss: 0.0467 - val_midLayer2_loss: 1.3463 - val_rightLayer2_loss: 0.8027\n",
      "Epoch 11/11\n",
      "7974/7996 [============================>.] - ETA: 0s - loss: 4.4349 - leftLayer1_loss: 0.0461 - midLayer1_loss: 1.3732 - rightLayer1_loss: 0.7821 - leftLayer2_loss: 0.0342 - midLayer2_loss: 1.4286 - rightLayer2_loss: 0.7707\n",
      "Epoch 00011: val_loss improved from 4.35585 to 4.34323, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.4346 - leftLayer1_loss: 0.0461 - midLayer1_loss: 1.3731 - rightLayer1_loss: 0.7820 - leftLayer2_loss: 0.0342 - midLayer2_loss: 1.4287 - rightLayer2_loss: 0.7705 - val_loss: 4.3432 - val_leftLayer1_loss: 0.0442 - val_midLayer1_loss: 1.3576 - val_rightLayer1_loss: 0.7521 - val_leftLayer2_loss: 0.0455 - val_midLayer2_loss: 1.3463 - val_rightLayer2_loss: 0.7975\n",
      "22433/22433 [==============================] - 27s 1ms/step\n",
      "** write log to ./experiments/0.011999999999999997_test.log **\n",
      "auroc 0Atelectasis: 0.4890100153908224\n",
      "\n",
      "auprc 0Atelectasis: 0.10288795034582816\n",
      "\n",
      "auroc 1Atelectasis: 0.45610919018340557\n",
      "\n",
      "auprc 1Atelectasis: 0.09323030086804543\n",
      "\n",
      "auroc 2Atelectasis: 0.5234100623850696\n",
      "\n",
      "auprc 2Atelectasis: 0.11243139324783195\n",
      "\n",
      "auroc 3Atelectasis: 0.5356992954579524\n",
      "\n",
      "auprc 3Atelectasis: 0.1194789947238577\n",
      "\n",
      "auroc 4Atelectasis: 0.6054274122646727\n",
      "\n",
      "auprc 4Atelectasis: 0.13488159864473534\n",
      "\n",
      "auroc 5Atelectasis: 0.4220023307164393\n",
      "\n",
      "auprc 5Atelectasis: 0.08654079891462665\n",
      "\n",
      "mean auroc: 0.505276384399727\n",
      "\n",
      "mean auprc: 0.10824183945748754\n",
      "\n",
      "max auroc: 0.6054274122646727\n",
      "\n",
      "max auprc: 0.13488159864473534\n",
      "\n",
      "243.44431924819946\n",
      "** set output weights path to: ./experiments/0.012999999999999996_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 7996 steps, validate for 1119 steps\n",
      "Epoch 1/11\n",
      "7982/7996 [============================>.] - ETA: 0s - loss: 5.5646 - leftLayer1_loss: 0.1153 - midLayer1_loss: 1.3544 - rightLayer1_loss: 1.4307 - leftLayer2_loss: 0.1076 - midLayer2_loss: 1.4074 - rightLayer2_loss: 1.1493\n",
      "Epoch 00001: val_loss improved from inf to 5.05826, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 5.5635 - leftLayer1_loss: 0.1153 - midLayer1_loss: 1.3543 - rightLayer1_loss: 1.4301 - leftLayer2_loss: 0.1076 - midLayer2_loss: 1.4075 - rightLayer2_loss: 1.1487 - val_loss: 5.0583 - val_leftLayer1_loss: 0.1062 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 1.1197 - val_leftLayer2_loss: 0.0990 - val_midLayer2_loss: 1.3015 - val_rightLayer2_loss: 1.0898\n",
      "Epoch 2/11\n",
      "7986/7996 [============================>.] - ETA: 0s - loss: 4.8031 - leftLayer1_loss: 0.0991 - midLayer1_loss: 1.3538 - rightLayer1_loss: 1.0159 - leftLayer2_loss: 0.0752 - midLayer2_loss: 1.4084 - rightLayer2_loss: 0.8507\n",
      "Epoch 00002: val_loss improved from 5.05826 to 4.70156, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.8025 - leftLayer1_loss: 0.0991 - midLayer1_loss: 1.3538 - rightLayer1_loss: 1.0157 - leftLayer2_loss: 0.0752 - midLayer2_loss: 1.4083 - rightLayer2_loss: 0.8504 - val_loss: 4.7016 - val_leftLayer1_loss: 0.0918 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.9185 - val_leftLayer2_loss: 0.0823 - val_midLayer2_loss: 1.3015 - val_rightLayer2_loss: 0.9655\n",
      "Epoch 3/11\n",
      "7990/7996 [============================>.] - ETA: 0s - loss: 4.6233 - leftLayer1_loss: 0.0864 - midLayer1_loss: 1.3540 - rightLayer1_loss: 0.9008 - leftLayer2_loss: 0.0586 - midLayer2_loss: 1.4087 - rightLayer2_loss: 0.8147\n",
      "Epoch 00003: val_loss improved from 4.70156 to 4.55806, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.6227 - leftLayer1_loss: 0.0864 - midLayer1_loss: 1.3540 - rightLayer1_loss: 0.9006 - leftLayer2_loss: 0.0586 - midLayer2_loss: 1.4086 - rightLayer2_loss: 0.8145 - val_loss: 4.5581 - val_leftLayer1_loss: 0.0805 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.8485 - val_leftLayer2_loss: 0.0720 - val_midLayer2_loss: 1.3015 - val_rightLayer2_loss: 0.9135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/11\n",
      "7976/7996 [============================>.] - ETA: 0s - loss: 4.5440 - leftLayer1_loss: 0.0766 - midLayer1_loss: 1.3538 - rightLayer1_loss: 0.8544 - leftLayer2_loss: 0.0498 - midLayer2_loss: 1.4089 - rightLayer2_loss: 0.8004\n",
      "Epoch 00004: val_loss improved from 4.55806 to 4.47850, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.5435 - leftLayer1_loss: 0.0766 - midLayer1_loss: 1.3537 - rightLayer1_loss: 0.8543 - leftLayer2_loss: 0.0498 - midLayer2_loss: 1.4089 - rightLayer2_loss: 0.8002 - val_loss: 4.4785 - val_leftLayer1_loss: 0.0718 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.8146 - val_leftLayer2_loss: 0.0652 - val_midLayer2_loss: 1.3015 - val_rightLayer2_loss: 0.8835\n",
      "Epoch 5/11\n",
      "7980/7996 [============================>.] - ETA: 0s - loss: 4.4955 - leftLayer1_loss: 0.0690 - midLayer1_loss: 1.3536 - rightLayer1_loss: 0.8298 - leftLayer2_loss: 0.0445 - midLayer2_loss: 1.4059 - rightLayer2_loss: 0.7928\n",
      "Epoch 00005: val_loss improved from 4.47850 to 4.42654, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.4955 - leftLayer1_loss: 0.0690 - midLayer1_loss: 1.3536 - rightLayer1_loss: 0.8297 - leftLayer2_loss: 0.0445 - midLayer2_loss: 1.4059 - rightLayer2_loss: 0.7928 - val_loss: 4.4265 - val_leftLayer1_loss: 0.0649 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.7945 - val_leftLayer2_loss: 0.0604 - val_midLayer2_loss: 1.3015 - val_rightLayer2_loss: 0.8632\n",
      "Epoch 6/11\n",
      "7993/7996 [============================>.] - ETA: 0s - loss: 4.4643 - leftLayer1_loss: 0.0630 - midLayer1_loss: 1.3540 - rightLayer1_loss: 0.8142 - leftLayer2_loss: 0.0412 - midLayer2_loss: 1.4053 - rightLayer2_loss: 0.7865\n",
      "Epoch 00006: val_loss improved from 4.42654 to 4.38936, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.4641 - leftLayer1_loss: 0.0630 - midLayer1_loss: 1.3540 - rightLayer1_loss: 0.8142 - leftLayer2_loss: 0.0412 - midLayer2_loss: 1.4052 - rightLayer2_loss: 0.7865 - val_loss: 4.3894 - val_leftLayer1_loss: 0.0595 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.7810 - val_leftLayer2_loss: 0.0569 - val_midLayer2_loss: 1.3015 - val_rightLayer2_loss: 0.8484\n",
      "Epoch 7/11\n",
      "7972/7996 [============================>.] - ETA: 0s - loss: 4.4463 - leftLayer1_loss: 0.0582 - midLayer1_loss: 1.3540 - rightLayer1_loss: 0.8035 - leftLayer2_loss: 0.0390 - midLayer2_loss: 1.4072 - rightLayer2_loss: 0.7843\n",
      "Epoch 00007: val_loss improved from 4.38936 to 4.36098, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.4457 - leftLayer1_loss: 0.0582 - midLayer1_loss: 1.3539 - rightLayer1_loss: 0.8033 - leftLayer2_loss: 0.0390 - midLayer2_loss: 1.4073 - rightLayer2_loss: 0.7840 - val_loss: 4.3610 - val_leftLayer1_loss: 0.0552 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.7712 - val_leftLayer2_loss: 0.0541 - val_midLayer2_loss: 1.3015 - val_rightLayer2_loss: 0.8369\n",
      "Epoch 8/11\n",
      "7988/7996 [============================>.] - ETA: 0s - loss: 4.4266 - leftLayer1_loss: 0.0544 - midLayer1_loss: 1.3540 - rightLayer1_loss: 0.7946 - leftLayer2_loss: 0.0374 - midLayer2_loss: 1.4056 - rightLayer2_loss: 0.7807\n",
      "Epoch 00008: val_loss improved from 4.36098 to 4.33837, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.4261 - leftLayer1_loss: 0.0544 - midLayer1_loss: 1.3539 - rightLayer1_loss: 0.7944 - leftLayer2_loss: 0.0374 - midLayer2_loss: 1.4056 - rightLayer2_loss: 0.7805 - val_loss: 4.3384 - val_leftLayer1_loss: 0.0517 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.7637 - val_leftLayer2_loss: 0.0520 - val_midLayer2_loss: 1.3015 - val_rightLayer2_loss: 0.8275\n",
      "Epoch 9/11\n",
      "7985/7996 [============================>.] - ETA: 0s - loss: 4.4141 - leftLayer1_loss: 0.0513 - midLayer1_loss: 1.3536 - rightLayer1_loss: 0.7884 - leftLayer2_loss: 0.0362 - midLayer2_loss: 1.4062 - rightLayer2_loss: 0.7783\n",
      "Epoch 00009: val_loss improved from 4.33837 to 4.31982, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.4135 - leftLayer1_loss: 0.0513 - midLayer1_loss: 1.3535 - rightLayer1_loss: 0.7882 - leftLayer2_loss: 0.0362 - midLayer2_loss: 1.4062 - rightLayer2_loss: 0.7781 - val_loss: 4.3198 - val_leftLayer1_loss: 0.0489 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.7575 - val_leftLayer2_loss: 0.0502 - val_midLayer2_loss: 1.3015 - val_rightLayer2_loss: 0.8197\n",
      "Epoch 10/11\n",
      "7989/7996 [============================>.] - ETA: 0s - loss: 4.4035 - leftLayer1_loss: 0.0487 - midLayer1_loss: 1.3535 - rightLayer1_loss: 0.7824 - leftLayer2_loss: 0.0353 - midLayer2_loss: 1.4082 - rightLayer2_loss: 0.7753\n",
      "Epoch 00010: val_loss improved from 4.31982 to 4.30432, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.4028 - leftLayer1_loss: 0.0487 - midLayer1_loss: 1.3535 - rightLayer1_loss: 0.7822 - leftLayer2_loss: 0.0353 - midLayer2_loss: 1.4081 - rightLayer2_loss: 0.7750 - val_loss: 4.3043 - val_leftLayer1_loss: 0.0465 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.7524 - val_leftLayer2_loss: 0.0487 - val_midLayer2_loss: 1.3015 - val_rightLayer2_loss: 0.8132\n",
      "Epoch 11/11\n",
      "7986/7996 [============================>.] - ETA: 0s - loss: 4.3930 - leftLayer1_loss: 0.0466 - midLayer1_loss: 1.3542 - rightLayer1_loss: 0.7778 - leftLayer2_loss: 0.0346 - midLayer2_loss: 1.4069 - rightLayer2_loss: 0.7728\n",
      "Epoch 00011: val_loss improved from 4.30432 to 4.29087, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.3925 - leftLayer1_loss: 0.0466 - midLayer1_loss: 1.3542 - rightLayer1_loss: 0.7775 - leftLayer2_loss: 0.0346 - midLayer2_loss: 1.4069 - rightLayer2_loss: 0.7726 - val_loss: 4.2909 - val_leftLayer1_loss: 0.0445 - val_midLayer1_loss: 1.3420 - val_rightLayer1_loss: 0.7480 - val_leftLayer2_loss: 0.0475 - val_midLayer2_loss: 1.3015 - val_rightLayer2_loss: 0.8074\n",
      "22433/22433 [==============================] - 27s 1ms/step\n",
      "** write log to ./experiments/0.012999999999999996_test.log **\n",
      "auroc 0Atelectasis: 0.5430779084504163\n",
      "\n",
      "auprc 0Atelectasis: 0.1163028251975271\n",
      "\n",
      "auroc 1Atelectasis: 0.5167284240450319\n",
      "\n",
      "auprc 1Atelectasis: 0.10495221659221413\n",
      "\n",
      "auroc 2Atelectasis: 0.35489546464219746\n",
      "\n",
      "auprc 2Atelectasis: 0.07669997423863734\n",
      "\n",
      "auroc 3Atelectasis: 0.3246716390544493\n",
      "\n",
      "auprc 3Atelectasis: 0.07327489895096911\n",
      "\n",
      "auroc 4Atelectasis: 0.3875665941105224\n",
      "\n",
      "auprc 4Atelectasis: 0.08021723713882166\n",
      "\n",
      "auroc 5Atelectasis: 0.3083657089833757\n",
      "\n",
      "auprc 5Atelectasis: 0.0715233047963099\n",
      "\n",
      "mean auroc: 0.4058842898809989\n",
      "\n",
      "mean auprc: 0.08716174281907989\n",
      "\n",
      "max auroc: 0.5430779084504163\n",
      "\n",
      "max auprc: 0.1163028251975271\n",
      "\n",
      "243.36531043052673\n",
      "** set output weights path to: ./experiments/0.013999999999999995_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 7996 steps, validate for 1119 steps\n",
      "Epoch 1/11\n",
      "7992/7996 [============================>.] - ETA: 0s - loss: 5.8526 - leftLayer1_loss: 0.1158 - midLayer1_loss: 1.3588 - rightLayer1_loss: 1.4564 - leftLayer2_loss: 0.1070 - midLayer2_loss: 1.6626 - rightLayer2_loss: 1.1519\n",
      "Epoch 00001: val_loss improved from inf to 5.27027, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 5.8521 - leftLayer1_loss: 0.1158 - midLayer1_loss: 1.3588 - rightLayer1_loss: 1.4562 - leftLayer2_loss: 0.1070 - midLayer2_loss: 1.6625 - rightLayer2_loss: 1.1517 - val_loss: 5.2703 - val_leftLayer1_loss: 0.1071 - val_midLayer1_loss: 1.3461 - val_rightLayer1_loss: 1.1395 - val_leftLayer2_loss: 0.0966 - val_midLayer2_loss: 1.4816 - val_rightLayer2_loss: 1.0994\n",
      "Epoch 2/11\n",
      "7982/7996 [============================>.] - ETA: 0s - loss: 5.0839 - leftLayer1_loss: 0.1001 - midLayer1_loss: 1.3593 - rightLayer1_loss: 1.0268 - leftLayer2_loss: 0.0751 - midLayer2_loss: 1.6658 - rightLayer2_loss: 0.8568\n",
      "Epoch 00002: val_loss improved from 5.27027 to 4.90126, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 5.0834 - leftLayer1_loss: 0.1001 - midLayer1_loss: 1.3592 - rightLayer1_loss: 1.0266 - leftLayer2_loss: 0.0751 - midLayer2_loss: 1.6657 - rightLayer2_loss: 0.8567 - val_loss: 4.9013 - val_leftLayer1_loss: 0.0930 - val_midLayer1_loss: 1.3461 - val_rightLayer1_loss: 0.9254 - val_leftLayer2_loss: 0.0805 - val_midLayer2_loss: 1.4816 - val_rightLayer2_loss: 0.9747\n",
      "Epoch 3/11\n",
      "7994/7996 [============================>.] - ETA: 0s - loss: 4.8942 - leftLayer1_loss: 0.0877 - midLayer1_loss: 1.3590 - rightLayer1_loss: 0.9036 - leftLayer2_loss: 0.0589 - midLayer2_loss: 1.6657 - rightLayer2_loss: 0.8193\n",
      "Epoch 00003: val_loss improved from 4.90126 to 4.75305, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "7996/7996 [==============================] - 20s 2ms/step - loss: 4.8942 - leftLayer1_loss: 0.0877 - midLayer1_loss: 1.3590 - rightLayer1_loss: 0.9036 - leftLayer2_loss: 0.0589 - midLayer2_loss: 1.6656 - rightLayer2_loss: 0.8193 - val_loss: 4.7531 - val_leftLayer1_loss: 0.0819 - val_midLayer1_loss: 1.3461 - val_rightLayer1_loss: 0.8509 - val_leftLayer2_loss: 0.0705 - val_midLayer2_loss: 1.4816 - val_rightLayer2_loss: 0.9221\n",
      "Epoch 4/11\n",
      "7991/7996 [============================>.] - ETA: 0s - loss: 4.8080 - leftLayer1_loss: 0.0780 - midLayer1_loss: 1.3589 - rightLayer1_loss: 0.8547 - leftLayer2_loss: 0.0499 - midLayer2_loss: 1.6618 - rightLayer2_loss: 0.8047\n",
      "Epoch 00004: val_loss improved from 4.75305 to 4.67159, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.8075 - leftLayer1_loss: 0.0780 - midLayer1_loss: 1.3588 - rightLayer1_loss: 0.8546 - leftLayer2_loss: 0.0499 - midLayer2_loss: 1.6617 - rightLayer2_loss: 0.8045 - val_loss: 4.6716 - val_leftLayer1_loss: 0.0732 - val_midLayer1_loss: 1.3461 - val_rightLayer1_loss: 0.8152 - val_leftLayer2_loss: 0.0639 - val_midLayer2_loss: 1.4816 - val_rightLayer2_loss: 0.8916\n",
      "Epoch 5/11\n",
      "7989/7996 [============================>.] - ETA: 0s - loss: 4.7660 - leftLayer1_loss: 0.0704 - midLayer1_loss: 1.3592 - rightLayer1_loss: 0.8290 - leftLayer2_loss: 0.0448 - midLayer2_loss: 1.6664 - rightLayer2_loss: 0.7962\n",
      "Epoch 00005: val_loss improved from 4.67159 to 4.61881, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "7996/7996 [==============================] - 22s 3ms/step - loss: 4.7654 - leftLayer1_loss: 0.0704 - midLayer1_loss: 1.3591 - rightLayer1_loss: 0.8288 - leftLayer2_loss: 0.0448 - midLayer2_loss: 1.6664 - rightLayer2_loss: 0.7960 - val_loss: 4.6188 - val_leftLayer1_loss: 0.0664 - val_midLayer1_loss: 1.3461 - val_rightLayer1_loss: 0.7944 - val_leftLayer2_loss: 0.0592 - val_midLayer2_loss: 1.4816 - val_rightLayer2_loss: 0.8711\n",
      "Epoch 6/11\n",
      "7976/7996 [============================>.] - ETA: 0s - loss: 4.7369 - leftLayer1_loss: 0.0644 - midLayer1_loss: 1.3585 - rightLayer1_loss: 0.8132 - leftLayer2_loss: 0.0414 - midLayer2_loss: 1.6686 - rightLayer2_loss: 0.7908\n",
      "Epoch 00006: val_loss improved from 4.61881 to 4.58107, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "7996/7996 [==============================] - 22s 3ms/step - loss: 4.7365 - leftLayer1_loss: 0.0644 - midLayer1_loss: 1.3584 - rightLayer1_loss: 0.8130 - leftLayer2_loss: 0.0414 - midLayer2_loss: 1.6687 - rightLayer2_loss: 0.7906 - val_loss: 4.5811 - val_leftLayer1_loss: 0.0609 - val_midLayer1_loss: 1.3461 - val_rightLayer1_loss: 0.7806 - val_leftLayer2_loss: 0.0558 - val_midLayer2_loss: 1.4816 - val_rightLayer2_loss: 0.8561\n",
      "Epoch 7/11\n",
      "7992/7996 [============================>.] - ETA: 0s - loss: 4.7099 - leftLayer1_loss: 0.0596 - midLayer1_loss: 1.3590 - rightLayer1_loss: 0.8024 - leftLayer2_loss: 0.0391 - midLayer2_loss: 1.6631 - rightLayer2_loss: 0.7868\n",
      "Epoch 00007: val_loss improved from 4.58107 to 4.55234, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "7996/7996 [==============================] - 23s 3ms/step - loss: 4.7097 - leftLayer1_loss: 0.0596 - midLayer1_loss: 1.3589 - rightLayer1_loss: 0.8023 - leftLayer2_loss: 0.0391 - midLayer2_loss: 1.6631 - rightLayer2_loss: 0.7868 - val_loss: 4.5523 - val_leftLayer1_loss: 0.0565 - val_midLayer1_loss: 1.3461 - val_rightLayer1_loss: 0.7708 - val_leftLayer2_loss: 0.0531 - val_midLayer2_loss: 1.4816 - val_rightLayer2_loss: 0.8443\n",
      "Epoch 8/11\n",
      "7985/7996 [============================>.] - ETA: 0s - loss: 4.6929 - leftLayer1_loss: 0.0556 - midLayer1_loss: 1.3588 - rightLayer1_loss: 0.7941 - leftLayer2_loss: 0.0376 - midLayer2_loss: 1.6635 - rightLayer2_loss: 0.7834\n",
      "Epoch 00008: val_loss improved from 4.55234 to 4.52964, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "7996/7996 [==============================] - 23s 3ms/step - loss: 4.6924 - leftLayer1_loss: 0.0556 - midLayer1_loss: 1.3587 - rightLayer1_loss: 0.7939 - leftLayer2_loss: 0.0376 - midLayer2_loss: 1.6635 - rightLayer2_loss: 0.7831 - val_loss: 4.5296 - val_leftLayer1_loss: 0.0530 - val_midLayer1_loss: 1.3461 - val_rightLayer1_loss: 0.7632 - val_leftLayer2_loss: 0.0510 - val_midLayer2_loss: 1.4816 - val_rightLayer2_loss: 0.8348\n",
      "Epoch 9/11\n",
      "7988/7996 [============================>.] - ETA: 0s - loss: 4.6803 - leftLayer1_loss: 0.0525 - midLayer1_loss: 1.3589 - rightLayer1_loss: 0.7878 - leftLayer2_loss: 0.0362 - midLayer2_loss: 1.6647 - rightLayer2_loss: 0.7802\n",
      "Epoch 00009: val_loss improved from 4.52964 to 4.51108, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "7996/7996 [==============================] - 22s 3ms/step - loss: 4.6796 - leftLayer1_loss: 0.0525 - midLayer1_loss: 1.3588 - rightLayer1_loss: 0.7876 - leftLayer2_loss: 0.0362 - midLayer2_loss: 1.6645 - rightLayer2_loss: 0.7800 - val_loss: 4.5111 - val_leftLayer1_loss: 0.0500 - val_midLayer1_loss: 1.3461 - val_rightLayer1_loss: 0.7572 - val_leftLayer2_loss: 0.0493 - val_midLayer2_loss: 1.4816 - val_rightLayer2_loss: 0.8269\n",
      "Epoch 10/11\n",
      "7978/7996 [============================>.] - ETA: 0s - loss: 4.6658 - leftLayer1_loss: 0.0498 - midLayer1_loss: 1.3591 - rightLayer1_loss: 0.7823 - leftLayer2_loss: 0.0354 - midLayer2_loss: 1.6616 - rightLayer2_loss: 0.7777\n",
      "Epoch 00010: val_loss improved from 4.51108 to 4.49550, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "7996/7996 [==============================] - 23s 3ms/step - loss: 4.6653 - leftLayer1_loss: 0.0498 - midLayer1_loss: 1.3590 - rightLayer1_loss: 0.7821 - leftLayer2_loss: 0.0354 - midLayer2_loss: 1.6615 - rightLayer2_loss: 0.7775 - val_loss: 4.4955 - val_leftLayer1_loss: 0.0476 - val_midLayer1_loss: 1.3461 - val_rightLayer1_loss: 0.7523 - val_leftLayer2_loss: 0.0479 - val_midLayer2_loss: 1.4816 - val_rightLayer2_loss: 0.8202\n",
      "Epoch 11/11\n",
      "7992/7996 [============================>.] - ETA: 0s - loss: 4.6541 - leftLayer1_loss: 0.0477 - midLayer1_loss: 1.3587 - rightLayer1_loss: 0.7778 - leftLayer2_loss: 0.0346 - midLayer2_loss: 1.6612 - rightLayer2_loss: 0.7741\n",
      "Epoch 00011: val_loss improved from 4.49550 to 4.48225, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.6538 - leftLayer1_loss: 0.0477 - midLayer1_loss: 1.3587 - rightLayer1_loss: 0.7777 - leftLayer2_loss: 0.0346 - midLayer2_loss: 1.6611 - rightLayer2_loss: 0.7740 - val_loss: 4.4822 - val_leftLayer1_loss: 0.0455 - val_midLayer1_loss: 1.3461 - val_rightLayer1_loss: 0.7480 - val_leftLayer2_loss: 0.0466 - val_midLayer2_loss: 1.4816 - val_rightLayer2_loss: 0.8144\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.013999999999999995_test.log **\n",
      "auroc 0Atelectasis: 0.28430124757750436\n",
      "\n",
      "auprc 0Atelectasis: 0.06936246826069674\n",
      "\n",
      "auroc 1Atelectasis: 0.4739091491357065\n",
      "\n",
      "auprc 1Atelectasis: 0.09631032574031546\n",
      "\n",
      "auroc 2Atelectasis: 0.39417309533926914\n",
      "\n",
      "auprc 2Atelectasis: 0.08114762891907876\n",
      "\n",
      "auroc 3Atelectasis: 0.32440196929846843\n",
      "\n",
      "auprc 3Atelectasis: 0.0736905605194668\n",
      "\n",
      "auroc 4Atelectasis: 0.591768253527769\n",
      "\n",
      "auprc 4Atelectasis: 0.12314473126444613\n",
      "\n",
      "auroc 5Atelectasis: 0.35358862813551356\n",
      "\n",
      "auprc 5Atelectasis: 0.07676102075953013\n",
      "\n",
      "mean auroc: 0.4036903905023719\n",
      "\n",
      "mean auprc: 0.08673612257725567\n",
      "\n",
      "max auroc: 0.591768253527769\n",
      "\n",
      "max auprc: 0.12314473126444613\n",
      "\n",
      "263.1281855106354\n",
      "** set output weights path to: ./experiments/0.014999999999999994_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 7996 steps, validate for 1119 steps\n",
      "Epoch 1/11\n",
      "7995/7996 [============================>.] - ETA: 0s - loss: 5.5603 - leftLayer1_loss: 0.1224 - midLayer1_loss: 1.4050 - rightLayer1_loss: 1.4398 - leftLayer2_loss: 0.1055 - midLayer2_loss: 1.3716 - rightLayer2_loss: 1.1159\n",
      "Epoch 00001: val_loss improved from inf to 5.13427, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "7996/7996 [==============================] - 21s 3ms/step - loss: 5.5602 - leftLayer1_loss: 0.1224 - midLayer1_loss: 1.4050 - rightLayer1_loss: 1.4398 - leftLayer2_loss: 0.1055 - midLayer2_loss: 1.3716 - rightLayer2_loss: 1.1159 - val_loss: 5.1343 - val_leftLayer1_loss: 0.1126 - val_midLayer1_loss: 1.4008 - val_rightLayer1_loss: 1.1190 - val_leftLayer2_loss: 0.0981 - val_midLayer2_loss: 1.3358 - val_rightLayer2_loss: 1.0680\n",
      "Epoch 2/11\n",
      "7976/7996 [============================>.] - ETA: 0s - loss: 4.8178 - leftLayer1_loss: 0.1041 - midLayer1_loss: 1.4051 - rightLayer1_loss: 1.0110 - leftLayer2_loss: 0.0731 - midLayer2_loss: 1.3758 - rightLayer2_loss: 0.8486\n",
      "Epoch 00002: val_loss improved from 5.13427 to 4.78203, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "7996/7996 [==============================] - 21s 3ms/step - loss: 4.8170 - leftLayer1_loss: 0.1041 - midLayer1_loss: 1.4051 - rightLayer1_loss: 1.0107 - leftLayer2_loss: 0.0730 - midLayer2_loss: 1.3757 - rightLayer2_loss: 0.8484 - val_loss: 4.7820 - val_leftLayer1_loss: 0.0963 - val_midLayer1_loss: 1.4008 - val_rightLayer1_loss: 0.9171 - val_leftLayer2_loss: 0.0807 - val_midLayer2_loss: 1.3358 - val_rightLayer2_loss: 0.9514\n",
      "Epoch 3/11\n",
      "7982/7996 [============================>.] - ETA: 0s - loss: 4.6405 - leftLayer1_loss: 0.0899 - midLayer1_loss: 1.4054 - rightLayer1_loss: 0.8973 - leftLayer2_loss: 0.0570 - midLayer2_loss: 1.3751 - rightLayer2_loss: 0.8159\n",
      "Epoch 00003: val_loss improved from 4.78203 to 4.64113, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "7996/7996 [==============================] - 21s 3ms/step - loss: 4.6402 - leftLayer1_loss: 0.0899 - midLayer1_loss: 1.4053 - rightLayer1_loss: 0.8972 - leftLayer2_loss: 0.0570 - midLayer2_loss: 1.3751 - rightLayer2_loss: 0.8158 - val_loss: 4.6411 - val_leftLayer1_loss: 0.0837 - val_midLayer1_loss: 1.4008 - val_rightLayer1_loss: 0.8485 - val_leftLayer2_loss: 0.0703 - val_midLayer2_loss: 1.3358 - val_rightLayer2_loss: 0.9021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/11\n",
      "7974/7996 [============================>.] - ETA: 0s - loss: 4.5624 - leftLayer1_loss: 0.0789 - midLayer1_loss: 1.4055 - rightLayer1_loss: 0.8526 - leftLayer2_loss: 0.0486 - midLayer2_loss: 1.3745 - rightLayer2_loss: 0.8023\n",
      "Epoch 00004: val_loss improved from 4.64113 to 4.56317, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.5620 - leftLayer1_loss: 0.0789 - midLayer1_loss: 1.4055 - rightLayer1_loss: 0.8524 - leftLayer2_loss: 0.0486 - midLayer2_loss: 1.3745 - rightLayer2_loss: 0.8021 - val_loss: 4.5632 - val_leftLayer1_loss: 0.0740 - val_midLayer1_loss: 1.4008 - val_rightLayer1_loss: 0.8156 - val_leftLayer2_loss: 0.0635 - val_midLayer2_loss: 1.3358 - val_rightLayer2_loss: 0.8735\n",
      "Epoch 5/11\n",
      "7977/7996 [============================>.] - ETA: 0s - loss: 4.5170 - leftLayer1_loss: 0.0705 - midLayer1_loss: 1.4053 - rightLayer1_loss: 0.8289 - leftLayer2_loss: 0.0438 - midLayer2_loss: 1.3744 - rightLayer2_loss: 0.7941\n",
      "Epoch 00005: val_loss improved from 4.56317 to 4.51246, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.5166 - leftLayer1_loss: 0.0705 - midLayer1_loss: 1.4052 - rightLayer1_loss: 0.8288 - leftLayer2_loss: 0.0438 - midLayer2_loss: 1.3743 - rightLayer2_loss: 0.7940 - val_loss: 4.5125 - val_leftLayer1_loss: 0.0665 - val_midLayer1_loss: 1.4008 - val_rightLayer1_loss: 0.7962 - val_leftLayer2_loss: 0.0588 - val_midLayer2_loss: 1.3358 - val_rightLayer2_loss: 0.8544\n",
      "Epoch 6/11\n",
      "7980/7996 [============================>.] - ETA: 0s - loss: 4.4870 - leftLayer1_loss: 0.0640 - midLayer1_loss: 1.4059 - rightLayer1_loss: 0.8143 - leftLayer2_loss: 0.0406 - midLayer2_loss: 1.3734 - rightLayer2_loss: 0.7888\n",
      "Epoch 00006: val_loss improved from 4.51246 to 4.47636, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.4870 - leftLayer1_loss: 0.0640 - midLayer1_loss: 1.4058 - rightLayer1_loss: 0.8143 - leftLayer2_loss: 0.0406 - midLayer2_loss: 1.3734 - rightLayer2_loss: 0.7888 - val_loss: 4.4764 - val_leftLayer1_loss: 0.0607 - val_midLayer1_loss: 1.4008 - val_rightLayer1_loss: 0.7834 - val_leftLayer2_loss: 0.0553 - val_midLayer2_loss: 1.3358 - val_rightLayer2_loss: 0.8405\n",
      "Epoch 7/11\n",
      "7988/7996 [============================>.] - ETA: 0s - loss: 4.4664 - leftLayer1_loss: 0.0589 - midLayer1_loss: 1.4055 - rightLayer1_loss: 0.8048 - leftLayer2_loss: 0.0384 - midLayer2_loss: 1.3726 - rightLayer2_loss: 0.7862\n",
      "Epoch 00007: val_loss improved from 4.47636 to 4.44877, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.4658 - leftLayer1_loss: 0.0589 - midLayer1_loss: 1.4054 - rightLayer1_loss: 0.8046 - leftLayer2_loss: 0.0384 - midLayer2_loss: 1.3726 - rightLayer2_loss: 0.7860 - val_loss: 4.4488 - val_leftLayer1_loss: 0.0561 - val_midLayer1_loss: 1.4008 - val_rightLayer1_loss: 0.7741 - val_leftLayer2_loss: 0.0526 - val_midLayer2_loss: 1.3358 - val_rightLayer2_loss: 0.8295\n",
      "Epoch 8/11\n",
      "7988/7996 [============================>.] - ETA: 0s - loss: 4.4524 - leftLayer1_loss: 0.0548 - midLayer1_loss: 1.4052 - rightLayer1_loss: 0.7970 - leftLayer2_loss: 0.0370 - midLayer2_loss: 1.3755 - rightLayer2_loss: 0.7828\n",
      "Epoch 00008: val_loss improved from 4.44877 to 4.42687, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.4518 - leftLayer1_loss: 0.0548 - midLayer1_loss: 1.4051 - rightLayer1_loss: 0.7968 - leftLayer2_loss: 0.0370 - midLayer2_loss: 1.3755 - rightLayer2_loss: 0.7826 - val_loss: 4.4269 - val_leftLayer1_loss: 0.0524 - val_midLayer1_loss: 1.4008 - val_rightLayer1_loss: 0.7669 - val_leftLayer2_loss: 0.0505 - val_midLayer2_loss: 1.3358 - val_rightLayer2_loss: 0.8205\n",
      "Epoch 9/11\n",
      "7989/7996 [============================>.] - ETA: 0s - loss: 4.4333 - leftLayer1_loss: 0.0515 - midLayer1_loss: 1.4056 - rightLayer1_loss: 0.7910 - leftLayer2_loss: 0.0358 - midLayer2_loss: 1.3696 - rightLayer2_loss: 0.7797\n",
      "Epoch 00009: val_loss improved from 4.42687 to 4.40911, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.4325 - leftLayer1_loss: 0.0515 - midLayer1_loss: 1.4055 - rightLayer1_loss: 0.7908 - leftLayer2_loss: 0.0358 - midLayer2_loss: 1.3694 - rightLayer2_loss: 0.7795 - val_loss: 4.4091 - val_leftLayer1_loss: 0.0494 - val_midLayer1_loss: 1.4008 - val_rightLayer1_loss: 0.7612 - val_leftLayer2_loss: 0.0488 - val_midLayer2_loss: 1.3358 - val_rightLayer2_loss: 0.8133\n",
      "Epoch 10/11\n",
      "7988/7996 [============================>.] - ETA: 0s - loss: 4.4280 - leftLayer1_loss: 0.0489 - midLayer1_loss: 1.4054 - rightLayer1_loss: 0.7857 - leftLayer2_loss: 0.0351 - midLayer2_loss: 1.3753 - rightLayer2_loss: 0.7776\n",
      "Epoch 00010: val_loss improved from 4.40911 to 4.39418, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.4275 - leftLayer1_loss: 0.0489 - midLayer1_loss: 1.4053 - rightLayer1_loss: 0.7855 - leftLayer2_loss: 0.0351 - midLayer2_loss: 1.3753 - rightLayer2_loss: 0.7774 - val_loss: 4.3942 - val_leftLayer1_loss: 0.0469 - val_midLayer1_loss: 1.4008 - val_rightLayer1_loss: 0.7564 - val_leftLayer2_loss: 0.0474 - val_midLayer2_loss: 1.3358 - val_rightLayer2_loss: 0.8070\n",
      "Epoch 11/11\n",
      "7992/7996 [============================>.] - ETA: 0s - loss: 4.4157 - leftLayer1_loss: 0.0467 - midLayer1_loss: 1.4057 - rightLayer1_loss: 0.7812 - leftLayer2_loss: 0.0344 - midLayer2_loss: 1.3737 - rightLayer2_loss: 0.7740\n",
      "Epoch 00011: val_loss improved from 4.39418 to 4.38155, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "7996/7996 [==============================] - 20s 3ms/step - loss: 4.4156 - leftLayer1_loss: 0.0467 - midLayer1_loss: 1.4057 - rightLayer1_loss: 0.7811 - leftLayer2_loss: 0.0344 - midLayer2_loss: 1.3738 - rightLayer2_loss: 0.7740 - val_loss: 4.3816 - val_leftLayer1_loss: 0.0448 - val_midLayer1_loss: 1.4008 - val_rightLayer1_loss: 0.7523 - val_leftLayer2_loss: 0.0462 - val_midLayer2_loss: 1.3358 - val_rightLayer2_loss: 0.8018\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.014999999999999994_test.log **\n",
      "auroc 0Atelectasis: 0.6210655429342828\n",
      "\n",
      "auprc 0Atelectasis: 0.15095720687863012\n",
      "\n",
      "auroc 1Atelectasis: 0.6240238989285064\n",
      "\n",
      "auprc 1Atelectasis: 0.14898333052831186\n",
      "\n",
      "auroc 2Atelectasis: 0.47056809767865765\n",
      "\n",
      "auprc 2Atelectasis: 0.09840217627709212\n",
      "\n",
      "auroc 3Atelectasis: 0.4656277138868\n",
      "\n",
      "auprc 3Atelectasis: 0.09544779571416805\n",
      "\n",
      "auroc 4Atelectasis: 0.27789860144625\n",
      "\n",
      "auprc 4Atelectasis: 0.06879863020985937\n",
      "\n",
      "auroc 5Atelectasis: 0.48605208267518674\n",
      "\n",
      "auprc 5Atelectasis: 0.1001296336325676\n",
      "\n",
      "mean auroc: 0.4908726562582806\n",
      "\n",
      "mean auprc: 0.11045312887343821\n",
      "\n",
      "max auroc: 0.6240238989285064\n",
      "\n",
      "max auprc: 0.15095720687863012\n",
      "\n",
      "254.12089037895203\n"
     ]
    }
   ],
   "source": [
    "step = np.arange(0.009, 0.0151, 0.001)\n",
    "maxi = []\n",
    "for k in np.nditer(step):\n",
    "    opn, daTime = optimize_network(k)\n",
    "    print(daTime)\n",
    "    maxi.append(opn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6824147155588537\n"
     ]
    }
   ],
   "source": [
    "print(np.max(maxi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
