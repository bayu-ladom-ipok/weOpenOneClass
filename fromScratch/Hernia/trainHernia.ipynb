{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import pickle\n",
    "from callback import MultipleClassAUROC, MultiGPUModelCheckpoint\n",
    "from configparser import ConfigParser\n",
    "from generator import AugmentedImageSequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from utility import get_sample_counts\n",
    "from weights import get_class_weights\n",
    "from augmenter import augmenter\n",
    "from tensorflow.keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer, Flatten, Input, GaussianNoise\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_radam import RAdam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#print(\"TensorFlow version: \", tf.__version__)\n",
    "#assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "#    \"This notebook requires TensorFlow 2.0 or above.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "# UPDATED: import from tensorflow.keras instead of keras\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics\n",
    "import gc\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneClass = \"Hernia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"./config.ini\"\n",
    "cp = ConfigParser()\n",
    "cp.read(config_file)\n",
    "\n",
    "    # default config\n",
    "output_dir = cp[\"DEFAULT\"].get(\"output_dir\")\n",
    "image_source_dir = cp[\"DEFAULT\"].get(\"image_source_dir\")\n",
    "base_model_name = cp[\"DEFAULT\"].get(\"base_model_name\")\n",
    "class_names = cp[\"DEFAULT\"].get(\"class_names\").split(\",\")\n",
    "\n",
    "    # train config\n",
    "use_base_model_weights = cp[\"TRAIN\"].getboolean(\"use_base_model_weights\")\n",
    "use_trained_model_weights = cp[\"TRAIN\"].getboolean(\"use_trained_model_weights\")\n",
    "use_best_weights = cp[\"TRAIN\"].getboolean(\"use_best_weights\")\n",
    "output_weights_name = cp[\"TRAIN\"].get(\"output_weights_name\")\n",
    "epochs = cp[\"TRAIN\"].getint(\"epochs\")\n",
    "batch_size = cp[\"TRAIN\"].getint(\"batch_size\")\n",
    "initial_learning_rate = cp[\"TRAIN\"].getfloat(\"initial_learning_rate\")\n",
    "generator_workers = cp[\"TRAIN\"].getint(\"generator_workers\")\n",
    "image_dimension = cp[\"TRAIN\"].getint(\"image_dimension\")\n",
    "train_steps = cp[\"TRAIN\"].get(\"train_steps\")\n",
    "patience_reduce_lr = cp[\"TRAIN\"].getint(\"patience_reduce_lr\")\n",
    "min_lr = cp[\"TRAIN\"].getfloat(\"min_lr\")\n",
    "validation_steps = cp[\"TRAIN\"].get(\"validation_steps\")\n",
    "positive_weights_multiply = cp[\"TRAIN\"].getfloat(\"positive_weights_multiply\")\n",
    "dataset_csv_dir = cp[\"TRAIN\"].get(\"dataset_csv_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=1.0, alpha=0.5):\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(tf.cast(y_pred,tf.float32) - tf.cast(y_true,tf.float32)), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network1(dropout=0.08425517073874295, neuronPct=0.1767547775828121, neuronShrink=0.33180474398878285):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "    # Construct neural network\n",
    "    neuronCount = neuronCount * neuronShrink\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,1536)))\n",
    "    model.add(Flatten(name='flat1'))\n",
    "    model.add(Dense(neuronCount,name='dense1'))\n",
    "    model.add(Activation('relu',name='relu1'))\n",
    "    model.add(Dropout(dropout, name='dropout1'))\n",
    "    model.add(Dense(14, activation='sigmoid',name='midLayer1')) # Output\n",
    "    weights_path = None\n",
    "    if weights_path is not None:\n",
    "        print(f\"load model weights_path: {weights_path}\")\n",
    "        model.load_weights(weights_path)\n",
    "    model.layers.pop()\n",
    "    dr = model.layers[-2].output\n",
    "    model.trainable = False\n",
    "    left = Dense(14, activation=\"sigmoid\", name='leftLayer1')(dr)\n",
    "    right = Dense(14, activation=\"sigmoid\", name='rightLayer1')(dr)\n",
    "    model = Model(model.input, [left,model.output,right])\n",
    "    #model = Model(model.input, model.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network2(dropout=0.15672137551441198, neuronPct=0.2197894476507525, neuronShrink=0.3803316528497302, noisePct=0.282563134185142):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "    # Construct neural network\n",
    "    neuronCount = neuronCount * neuronShrink\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,1536)))\n",
    "    model.add(Flatten(name='flat2'))\n",
    "    model.add(Dense(neuronCount,name='dense2'))\n",
    "    model.add(GaussianNoise(noisePct))\n",
    "    model.add(Activation('relu',name='relu2'))\n",
    "    model.add(Dropout(dropout, name='dropout2'))\n",
    "    model.add(Dense(14, activation='sigmoid',name='midLayer2')) # Output\n",
    "    weights_path = None\n",
    "    if weights_path is not None:\n",
    "        print(f\"load model weights_path: {weights_path}\")\n",
    "        model.load_weights(weights_path)\n",
    "    #model.layers.pop()\n",
    "    dr = model.layers[-2].output\n",
    "    model.trainable = False\n",
    "    left = Dense(14, activation=\"sigmoid\", name='leftLayer2')(dr)\n",
    "    right = Dense(14, activation=\"sigmoid\", name='rightLayer2')(dr)\n",
    "    model = Model(model.input, [left,model.output,right])\n",
    "    #model = Model(model.input, model.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network(model1,model2):\n",
    "    model = Model([model1.input,model2.input], [model1.output,model2.output])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** compute class weights from training data **\n",
      "23: 144\n",
      "2: 144\n",
      "9: 144\n",
      "22: 144\n",
      "16: 144\n",
      "6: 144\n",
      "2: 144\n",
      "7: 144\n",
      "4: 144\n",
      "3: 144\n",
      "0: 144\n",
      "4: 144\n",
      "6: 144\n",
      "144: 144\n",
      "** class_weights **\n",
      "[{0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}]\n"
     ]
    }
   ],
   "source": [
    "# compute steps\n",
    "train_counts, train_pos_counts = get_sample_counts(output_dir, \"train\"+oneClass, class_names)\n",
    "dev_counts, _ = get_sample_counts(output_dir, \"dev\"+oneClass, class_names)\n",
    "    \n",
    "if train_steps == \"auto\":\n",
    "    train_steps = int(train_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        train_steps = int(train_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"train_steps: {train_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "    print(f\"** train_steps: {train_steps} **\")\n",
    "\n",
    "if validation_steps == \"auto\":\n",
    "    validation_steps = int(dev_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        validation_steps = int(validation_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"validation_steps: {validation_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        print(f\"** validation_steps: {validation_steps} **\")\n",
    "\n",
    "        # compute class weights\n",
    "keras.backend.clear_session()\n",
    "print(\"** compute class weights from training data **\")\n",
    "class_weights = get_class_weights(train_counts,train_pos_counts,multiply=positive_weights_multiply,)\n",
    "print(\"** class_weights **\")\n",
    "print(class_weights)\n",
    "#print(str(train_steps))\n",
    "#print(str(train_counts))\n",
    "#print(str(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** test_steps: 22433 **\n"
     ]
    }
   ],
   "source": [
    "test_steps = cp[\"TEST\"].get(\"test_steps\")\n",
    "test_counts, _ = get_sample_counts(output_dir, \"test\", class_names)\n",
    "\n",
    "if test_steps == \"auto\":\n",
    "    test_steps = int(test_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        test_steps = int(test_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"test_steps: {test_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        \n",
    "print(f\"** test_steps: {test_steps} **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"train\"+oneClass+\".csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=train_steps,\n",
    "        )\n",
    "validation_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"dev\"+oneClass+\".csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=validation_steps,\n",
    "            shuffle_on_epoch_end=False,\n",
    ")\n",
    "\n",
    "test_sequence = AugmentedImageSequence(\n",
    "        dataset_csv_file=os.path.join(output_dir, \"test.csv\"),\n",
    "        class_names=class_names,\n",
    "        source_image_dir=image_source_dir,\n",
    "        batch_size=batch_size,\n",
    "        target_size=(image_dimension, image_dimension),\n",
    "        augmenter=None,\n",
    "        steps=test_steps,\n",
    "        shuffle_on_epoch_end=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(lr):\n",
    "    gc.collect()\n",
    "      # Define the Keras TensorBoard callback.\n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    model1 = construct_network1()\n",
    "    model2 = construct_network2()\n",
    "    \n",
    "    optimizer = SGD(lr=initial_learning_rate)\n",
    "    \n",
    "    alpha = 0.9340456763831478\n",
    "    gamma = 1.4195808780694898\n",
    "    model1.compile(optimizer=optimizer,loss={'leftLayer1':tf.keras.losses.Huber(),'midLayer1':focal_loss(gamma=gamma,alpha=alpha),'rightLayer1':euclidean_distance_loss})\n",
    "\n",
    "    alpha = 0.7297456293468533\n",
    "    gamma = 1.2700405014991505\n",
    "    model2.compile(optimizer=optimizer,loss={'leftLayer2':tf.keras.losses.Huber(),'midLayer2':focal_loss(gamma=gamma,alpha=alpha),'rightLayer2':euclidean_distance_loss})\n",
    "  \n",
    "    model = construct_network(model1=model1,model2=model2)\n",
    "    model.compile(optimizer=optimizer,loss={'leftLayer1':tf.keras.losses.Huber(),'midLayer1':focal_loss(gamma=gamma,alpha=alpha),'rightLayer1':euclidean_distance_loss,'leftLayer2':tf.keras.losses.Huber(),'midLayer2':focal_loss(gamma=gamma,alpha=alpha),'rightLayer2':euclidean_distance_loss})\n",
    "\n",
    "    output_weights_path = os.path.join(output_dir,  str(lr)+\"_\"+output_weights_name)\n",
    "    \n",
    "    print(f\"** set output weights path to: {output_weights_path} **\")\n",
    "                  \n",
    "    \n",
    "                  \n",
    "    checkpoint = ModelCheckpoint(\n",
    "                 output_weights_path,\n",
    "                 save_weights_only=True,\n",
    "                 save_best_only=True,\n",
    "                 verbose=1,\n",
    "            )\n",
    "    start_time = time.time()\n",
    "  \n",
    "    model.summary()\n",
    "  \n",
    "    callbacks = [\n",
    "            checkpoint,\n",
    "            #keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "            #TensorBoard(log_dir=os.path.join(output_dir, \"logs\"), batch_size=batch_size),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience_reduce_lr,\n",
    "                              verbose=1, mode=\"min\", min_lr=min_lr), \n",
    "            EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    history = model.fit_generator(\n",
    "            generator=train_sequence,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_sequence,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=[class_weights,class_weights,class_weights,class_weights,class_weights,class_weights],\n",
    "            workers=generator_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        \n",
    "    y_hat = model.predict_generator(test_sequence, verbose=1)\n",
    "    y = test_sequence.get_y_true()\n",
    "    \n",
    "    test_log_path = os.path.join(output_dir, str(lr)+\"_\"+\"test.log\")\n",
    "    print(f\"** write log to {test_log_path} **\")\n",
    "    aurocs = []\n",
    "    auprcs = []\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    threshold = dict()\n",
    "    with open(test_log_path, \"w\") as f:\n",
    "        for k in range(6):\n",
    "            for i in range(len(class_names)):\n",
    "                 if(class_names[i] == str(oneClass)):\n",
    "                \n",
    "                    try:\n",
    "                        score = roc_auc_score(y[:, i], y_hat[k][:, i])\n",
    "                        precision[i], recall[i], threshold[i] = precision_recall_curve(y[:, i], y_hat[k][:, i])\n",
    "                        tmp = auc(recall[i], precision[i])\n",
    "                        aurocs.append(score)\n",
    "                        auprcs.append(tmp) \n",
    "                    except ValueError:\n",
    "                        score = 0\n",
    "               \n",
    "                    print(f\"auroc {str(k)+class_names[i]}: {score}\\n\")\n",
    "                    print(f\"auprc {str(k)+class_names[i]}: {tmp}\\n\")\n",
    "                    f.write(f\"auroc {str(k)+class_names[i]}: {score}\\n\")\n",
    "                    f.write(f\"auprc {str(k)+class_names[i]}: {tmp}\\n\")\n",
    "        \n",
    "        mean_auroc = np.mean(aurocs)\n",
    "        mean_auprc = float(np.mean(auprcs))\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(f\"mean auroc: {mean_auroc}\\n\")\n",
    "        print(f\"mean auroc: {mean_auroc}\\n\")\n",
    "        f.write(f\"mean auprc: {mean_auprc}\\n\")\n",
    "        print(f\"mean auprc: {mean_auprc}\\n\")\n",
    "        \n",
    "        max_auroc = np.max(aurocs)\n",
    "        max_auprc = float(np.max(auprcs))\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(f\"max auroc: {max_auroc}\\n\")\n",
    "        print(f\"max auroc: {max_auroc}\\n\")\n",
    "        f.write(f\"max auprc: {max_auprc}\\n\")\n",
    "        print(f\"max auprc: {max_auprc}\\n\")\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "    time_took = time.time() - start_time\n",
    "    \n",
    "    return max_auroc, time_took\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** set output weights path to: ./experiments/0.009_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-15-3539473a5eed>:58: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 144 steps, validate for 41 steps\n",
      "Epoch 1/11\n",
      "127/144 [=========================>....] - ETA: 0s - loss: 6.6420 - leftLayer1_loss: 0.1228 - midLayer1_loss: 1.3386 - rightLayer1_loss: 1.9246 - leftLayer2_loss: 0.1288 - midLayer2_loss: 1.3187 - rightLayer2_loss: 1.8085\n",
      "Epoch 00001: val_loss improved from inf to 6.57070, saving model to ./experiments/0.009_weights.h5\n",
      "144/144 [==============================] - 2s 11ms/step - loss: 6.6281 - leftLayer1_loss: 0.1227 - midLayer1_loss: 1.3364 - rightLayer1_loss: 1.9239 - leftLayer2_loss: 0.1289 - midLayer2_loss: 1.3116 - rightLayer2_loss: 1.8045 - val_loss: 6.5707 - val_leftLayer1_loss: 0.1236 - val_midLayer1_loss: 1.3650 - val_rightLayer1_loss: 1.8979 - val_leftLayer2_loss: 0.1267 - val_midLayer2_loss: 1.2621 - val_rightLayer2_loss: 1.7954\n",
      "Epoch 2/11\n",
      "128/144 [=========================>....] - ETA: 0s - loss: 6.5246 - leftLayer1_loss: 0.1223 - midLayer1_loss: 1.3407 - rightLayer1_loss: 1.8870 - leftLayer2_loss: 0.1289 - midLayer2_loss: 1.3065 - rightLayer2_loss: 1.7393\n",
      "Epoch 00002: val_loss improved from 6.57070 to 6.49383, saving model to ./experiments/0.009_weights.h5\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 6.5134 - leftLayer1_loss: 0.1224 - midLayer1_loss: 1.3386 - rightLayer1_loss: 1.8859 - leftLayer2_loss: 0.1285 - midLayer2_loss: 1.3041 - rightLayer2_loss: 1.7339 - val_loss: 6.4938 - val_leftLayer1_loss: 0.1231 - val_midLayer1_loss: 1.3650 - val_rightLayer1_loss: 1.8678 - val_leftLayer2_loss: 0.1258 - val_midLayer2_loss: 1.2621 - val_rightLayer2_loss: 1.7500\n",
      "Epoch 3/11\n",
      "142/144 [============================>.] - ETA: 0s - loss: 6.3950 - leftLayer1_loss: 0.1217 - midLayer1_loss: 1.3300 - rightLayer1_loss: 1.8548 - leftLayer2_loss: 0.1259 - midLayer2_loss: 1.2951 - rightLayer2_loss: 1.6675\n",
      "Epoch 00003: val_loss improved from 6.49383 to 6.41849, saving model to ./experiments/0.009_weights.h5\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 6.3996 - leftLayer1_loss: 0.1218 - midLayer1_loss: 1.3318 - rightLayer1_loss: 1.8546 - leftLayer2_loss: 0.1261 - midLayer2_loss: 1.2982 - rightLayer2_loss: 1.6671 - val_loss: 6.4185 - val_leftLayer1_loss: 0.1226 - val_midLayer1_loss: 1.3650 - val_rightLayer1_loss: 1.8379 - val_leftLayer2_loss: 0.1249 - val_midLayer2_loss: 1.2621 - val_rightLayer2_loss: 1.7060\n",
      "Epoch 4/11\n",
      "126/144 [=========================>....] - ETA: 0s - loss: 6.3508 - leftLayer1_loss: 0.1215 - midLayer1_loss: 1.3395 - rightLayer1_loss: 1.8234 - leftLayer2_loss: 0.1261 - midLayer2_loss: 1.3408 - rightLayer2_loss: 1.5996\n",
      "Epoch 00004: val_loss improved from 6.41849 to 6.34527, saving model to ./experiments/0.009_weights.h5\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 6.3270 - leftLayer1_loss: 0.1215 - midLayer1_loss: 1.3392 - rightLayer1_loss: 1.8227 - leftLayer2_loss: 0.1256 - midLayer2_loss: 1.3233 - rightLayer2_loss: 1.5948 - val_loss: 6.3453 - val_leftLayer1_loss: 0.1221 - val_midLayer1_loss: 1.3650 - val_rightLayer1_loss: 1.8081 - val_leftLayer2_loss: 0.1241 - val_midLayer2_loss: 1.2621 - val_rightLayer2_loss: 1.6639\n",
      "Epoch 5/11\n",
      "134/144 [==========================>...] - ETA: 0s - loss: 6.2139 - leftLayer1_loss: 0.1207 - midLayer1_loss: 1.3421 - rightLayer1_loss: 1.7895 - leftLayer2_loss: 0.1229 - midLayer2_loss: 1.3028 - rightLayer2_loss: 1.5359\n",
      "Epoch 00005: val_loss improved from 6.34527 to 6.27438, saving model to ./experiments/0.009_weights.h5\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 6.2054 - leftLayer1_loss: 0.1205 - midLayer1_loss: 1.3375 - rightLayer1_loss: 1.7886 - leftLayer2_loss: 0.1229 - midLayer2_loss: 1.3063 - rightLayer2_loss: 1.5296 - val_loss: 6.2744 - val_leftLayer1_loss: 0.1216 - val_midLayer1_loss: 1.3650 - val_rightLayer1_loss: 1.7787 - val_leftLayer2_loss: 0.1233 - val_midLayer2_loss: 1.2621 - val_rightLayer2_loss: 1.6238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/11\n",
      "139/144 [===========================>..] - ETA: 0s - loss: 6.0955 - leftLayer1_loss: 0.1203 - midLayer1_loss: 1.3410 - rightLayer1_loss: 1.7551 - leftLayer2_loss: 0.1216 - midLayer2_loss: 1.2853 - rightLayer2_loss: 1.4723\n",
      "Epoch 00006: val_loss improved from 6.27438 to 6.20553, saving model to ./experiments/0.009_weights.h5\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 6.0935 - leftLayer1_loss: 0.1202 - midLayer1_loss: 1.3403 - rightLayer1_loss: 1.7551 - leftLayer2_loss: 0.1214 - midLayer2_loss: 1.2852 - rightLayer2_loss: 1.4713 - val_loss: 6.2055 - val_leftLayer1_loss: 0.1211 - val_midLayer1_loss: 1.3650 - val_rightLayer1_loss: 1.7496 - val_leftLayer2_loss: 0.1225 - val_midLayer2_loss: 1.2621 - val_rightLayer2_loss: 1.5853\n",
      "Epoch 7/11\n",
      "127/144 [=========================>....] - ETA: 0s - loss: 6.0196 - leftLayer1_loss: 0.1195 - midLayer1_loss: 1.3397 - rightLayer1_loss: 1.7254 - leftLayer2_loss: 0.1208 - midLayer2_loss: 1.2996 - rightLayer2_loss: 1.4147\n",
      "Epoch 00007: val_loss improved from 6.20553 to 6.13917, saving model to ./experiments/0.009_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.0119 - leftLayer1_loss: 0.1194 - midLayer1_loss: 1.3363 - rightLayer1_loss: 1.7242 - leftLayer2_loss: 0.1210 - midLayer2_loss: 1.2994 - rightLayer2_loss: 1.4116 - val_loss: 6.1392 - val_leftLayer1_loss: 0.1206 - val_midLayer1_loss: 1.3650 - val_rightLayer1_loss: 1.7211 - val_leftLayer2_loss: 0.1216 - val_midLayer2_loss: 1.2621 - val_rightLayer2_loss: 1.5488\n",
      "Epoch 8/11\n",
      "136/144 [===========================>..] - ETA: 0s - loss: 5.9306 - leftLayer1_loss: 0.1191 - midLayer1_loss: 1.3365 - rightLayer1_loss: 1.6938 - leftLayer2_loss: 0.1197 - midLayer2_loss: 1.2802 - rightLayer2_loss: 1.3814\n",
      "Epoch 00008: val_loss improved from 6.13917 to 6.07506, saving model to ./experiments/0.009_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.9241 - leftLayer1_loss: 0.1191 - midLayer1_loss: 1.3325 - rightLayer1_loss: 1.6932 - leftLayer2_loss: 0.1197 - midLayer2_loss: 1.2822 - rightLayer2_loss: 1.3775 - val_loss: 6.0751 - val_leftLayer1_loss: 0.1201 - val_midLayer1_loss: 1.3650 - val_rightLayer1_loss: 1.6930 - val_leftLayer2_loss: 0.1208 - val_midLayer2_loss: 1.2621 - val_rightLayer2_loss: 1.5141\n",
      "Epoch 9/11\n",
      "133/144 [==========================>...] - ETA: 0s - loss: 5.8766 - leftLayer1_loss: 0.1188 - midLayer1_loss: 1.3410 - rightLayer1_loss: 1.6638 - leftLayer2_loss: 0.1190 - midLayer2_loss: 1.3155 - rightLayer2_loss: 1.3186\n",
      "Epoch 00009: val_loss improved from 6.07506 to 6.01385, saving model to ./experiments/0.009_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.8609 - leftLayer1_loss: 0.1185 - midLayer1_loss: 1.3365 - rightLayer1_loss: 1.6621 - leftLayer2_loss: 0.1190 - midLayer2_loss: 1.3114 - rightLayer2_loss: 1.3133 - val_loss: 6.0139 - val_leftLayer1_loss: 0.1196 - val_midLayer1_loss: 1.3650 - val_rightLayer1_loss: 1.6655 - val_leftLayer2_loss: 0.1200 - val_midLayer2_loss: 1.2621 - val_rightLayer2_loss: 1.4817\n",
      "Epoch 10/11\n",
      "132/144 [==========================>...] - ETA: 0s - loss: 5.7885 - leftLayer1_loss: 0.1179 - midLayer1_loss: 1.3379 - rightLayer1_loss: 1.6334 - leftLayer2_loss: 0.1165 - midLayer2_loss: 1.3098 - rightLayer2_loss: 1.2730\n",
      "Epoch 00010: val_loss improved from 6.01385 to 5.95488, saving model to ./experiments/0.009_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.7832 - leftLayer1_loss: 0.1178 - midLayer1_loss: 1.3346 - rightLayer1_loss: 1.6322 - leftLayer2_loss: 0.1167 - midLayer2_loss: 1.3130 - rightLayer2_loss: 1.2689 - val_loss: 5.9549 - val_leftLayer1_loss: 0.1191 - val_midLayer1_loss: 1.3650 - val_rightLayer1_loss: 1.6385 - val_leftLayer2_loss: 0.1193 - val_midLayer2_loss: 1.2621 - val_rightLayer2_loss: 1.4510\n",
      "Epoch 11/11\n",
      "143/144 [============================>.] - ETA: 0s - loss: 5.7053 - leftLayer1_loss: 0.1173 - midLayer1_loss: 1.3371 - rightLayer1_loss: 1.6020 - leftLayer2_loss: 0.1173 - midLayer2_loss: 1.3055 - rightLayer2_loss: 1.2260\n",
      "Epoch 00011: val_loss improved from 5.95488 to 5.89839, saving model to ./experiments/0.009_weights.h5\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 5.7030 - leftLayer1_loss: 0.1173 - midLayer1_loss: 1.3362 - rightLayer1_loss: 1.6015 - leftLayer2_loss: 0.1173 - midLayer2_loss: 1.3049 - rightLayer2_loss: 1.2258 - val_loss: 5.8984 - val_leftLayer1_loss: 0.1186 - val_midLayer1_loss: 1.3650 - val_rightLayer1_loss: 1.6121 - val_leftLayer2_loss: 0.1185 - val_midLayer2_loss: 1.2621 - val_rightLayer2_loss: 1.4222\n",
      "WARNING:tensorflow:From <ipython-input-15-3539473a5eed>:61: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "22433/22433 [==============================] - 30s 1ms/step\n",
      "** write log to ./experiments/0.009_test.log **\n",
      "auroc 0Hernia: 0.5433544727792416\n",
      "\n",
      "auprc 0Hernia: 0.00196819027431214\n",
      "\n",
      "auroc 1Hernia: 0.5561949848046941\n",
      "\n",
      "auprc 1Hernia: 0.0019695160493263465\n",
      "\n",
      "auroc 2Hernia: 0.7119516557460375\n",
      "\n",
      "auprc 2Hernia: 0.004531597259865731\n",
      "\n",
      "auroc 3Hernia: 0.32287738908702696\n",
      "\n",
      "auprc 3Hernia: 0.001247586955661538\n",
      "\n",
      "auroc 4Hernia: 0.5005268911190933\n",
      "\n",
      "auprc 4Hernia: 0.0017140237134039185\n",
      "\n",
      "auroc 5Hernia: 0.4175726429198806\n",
      "\n",
      "auprc 5Hernia: 0.0014480038674123063\n",
      "\n",
      "mean auroc: 0.5087463394093291\n",
      "\n",
      "mean auprc: 0.0021464863533303302\n",
      "\n",
      "max auroc: 0.7119516557460375\n",
      "\n",
      "max auprc: 0.004531597259865731\n",
      "\n",
      "37.29181981086731\n",
      "** set output weights path to: ./experiments/0.009999999999999998_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 144 steps, validate for 41 steps\n",
      "Epoch 1/11\n",
      "135/144 [===========================>..] - ETA: 0s - loss: 6.6856 - leftLayer1_loss: 0.1338 - midLayer1_loss: 1.3567 - rightLayer1_loss: 1.9032 - leftLayer2_loss: 0.1386 - midLayer2_loss: 1.2866 - rightLayer2_loss: 1.8666\n",
      "Epoch 00001: val_loss improved from inf to 6.64389, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "144/144 [==============================] - 1s 9ms/step - loss: 6.6795 - leftLayer1_loss: 0.1340 - midLayer1_loss: 1.3549 - rightLayer1_loss: 1.9022 - leftLayer2_loss: 0.1386 - midLayer2_loss: 1.2853 - rightLayer2_loss: 1.8646 - val_loss: 6.6439 - val_leftLayer1_loss: 0.1342 - val_midLayer1_loss: 1.3683 - val_rightLayer1_loss: 1.8907 - val_leftLayer2_loss: 0.1310 - val_midLayer2_loss: 1.2781 - val_rightLayer2_loss: 1.8416\n",
      "Epoch 2/11\n",
      "129/144 [=========================>....] - ETA: 0s - loss: 6.5729 - leftLayer1_loss: 0.1336 - midLayer1_loss: 1.3505 - rightLayer1_loss: 1.8740 - leftLayer2_loss: 0.1366 - midLayer2_loss: 1.2871 - rightLayer2_loss: 1.7910\n",
      "Epoch 00002: val_loss improved from 6.64389 to 6.56872, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 6.5610 - leftLayer1_loss: 0.1338 - midLayer1_loss: 1.3489 - rightLayer1_loss: 1.8725 - leftLayer2_loss: 0.1358 - midLayer2_loss: 1.2852 - rightLayer2_loss: 1.7847 - val_loss: 6.5687 - val_leftLayer1_loss: 0.1337 - val_midLayer1_loss: 1.3683 - val_rightLayer1_loss: 1.8644 - val_leftLayer2_loss: 0.1301 - val_midLayer2_loss: 1.2781 - val_rightLayer2_loss: 1.7943\n",
      "Epoch 3/11\n",
      "129/144 [=========================>....] - ETA: 0s - loss: 6.4519 - leftLayer1_loss: 0.1327 - midLayer1_loss: 1.3490 - rightLayer1_loss: 1.8468 - leftLayer2_loss: 0.1353 - midLayer2_loss: 1.2759 - rightLayer2_loss: 1.7123\n",
      "Epoch 00003: val_loss improved from 6.56872 to 6.49514, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.4349 - leftLayer1_loss: 0.1328 - midLayer1_loss: 1.3477 - rightLayer1_loss: 1.8455 - leftLayer2_loss: 0.1350 - midLayer2_loss: 1.2680 - rightLayer2_loss: 1.7059 - val_loss: 6.4951 - val_leftLayer1_loss: 0.1332 - val_midLayer1_loss: 1.3683 - val_rightLayer1_loss: 1.8382 - val_leftLayer2_loss: 0.1291 - val_midLayer2_loss: 1.2781 - val_rightLayer2_loss: 1.7482\n",
      "Epoch 4/11\n",
      "136/144 [===========================>..] - ETA: 0s - loss: 6.3715 - leftLayer1_loss: 0.1324 - midLayer1_loss: 1.3606 - rightLayer1_loss: 1.8161 - leftLayer2_loss: 0.1331 - midLayer2_loss: 1.2789 - rightLayer2_loss: 1.6504\n",
      "Epoch 00004: val_loss improved from 6.49514 to 6.42252, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.3638 - leftLayer1_loss: 0.1325 - midLayer1_loss: 1.3575 - rightLayer1_loss: 1.8153 - leftLayer2_loss: 0.1331 - midLayer2_loss: 1.2758 - rightLayer2_loss: 1.6495 - val_loss: 6.4225 - val_leftLayer1_loss: 0.1326 - val_midLayer1_loss: 1.3683 - val_rightLayer1_loss: 1.8121 - val_leftLayer2_loss: 0.1282 - val_midLayer2_loss: 1.2781 - val_rightLayer2_loss: 1.7032\n",
      "Epoch 5/11\n",
      "129/144 [=========================>....] - ETA: 0s - loss: 6.2781 - leftLayer1_loss: 0.1315 - midLayer1_loss: 1.3565 - rightLayer1_loss: 1.7891 - leftLayer2_loss: 0.1298 - midLayer2_loss: 1.2814 - rightLayer2_loss: 1.5898\n",
      "Epoch 00005: val_loss improved from 6.42252 to 6.35172, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 6.2684 - leftLayer1_loss: 0.1316 - midLayer1_loss: 1.3548 - rightLayer1_loss: 1.7875 - leftLayer2_loss: 0.1304 - midLayer2_loss: 1.2778 - rightLayer2_loss: 1.5862 - val_loss: 6.3517 - val_leftLayer1_loss: 0.1321 - val_midLayer1_loss: 1.3683 - val_rightLayer1_loss: 1.7862 - val_leftLayer2_loss: 0.1273 - val_midLayer2_loss: 1.2781 - val_rightLayer2_loss: 1.6597\n",
      "Epoch 6/11\n",
      "132/144 [==========================>...] - ETA: 0s - loss: 6.2013 - leftLayer1_loss: 0.1307 - midLayer1_loss: 1.3567 - rightLayer1_loss: 1.7643 - leftLayer2_loss: 0.1298 - midLayer2_loss: 1.2859 - rightLayer2_loss: 1.5340\n",
      "Epoch 00006: val_loss improved from 6.35172 to 6.28304, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.1933 - leftLayer1_loss: 0.1307 - midLayer1_loss: 1.3551 - rightLayer1_loss: 1.7626 - leftLayer2_loss: 0.1298 - midLayer2_loss: 1.2805 - rightLayer2_loss: 1.5345 - val_loss: 6.2830 - val_leftLayer1_loss: 0.1316 - val_midLayer1_loss: 1.3683 - val_rightLayer1_loss: 1.7607 - val_leftLayer2_loss: 0.1264 - val_midLayer2_loss: 1.2781 - val_rightLayer2_loss: 1.6179\n",
      "Epoch 7/11\n",
      "135/144 [===========================>..] - ETA: 0s - loss: 6.0961 - leftLayer1_loss: 0.1304 - midLayer1_loss: 1.3520 - rightLayer1_loss: 1.7329 - leftLayer2_loss: 0.1294 - midLayer2_loss: 1.2853 - rightLayer2_loss: 1.4662\n",
      "Epoch 00007: val_loss improved from 6.28304 to 6.21585, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.0873 - leftLayer1_loss: 0.1304 - midLayer1_loss: 1.3499 - rightLayer1_loss: 1.7318 - leftLayer2_loss: 0.1293 - midLayer2_loss: 1.2815 - rightLayer2_loss: 1.4644 - val_loss: 6.2158 - val_leftLayer1_loss: 0.1311 - val_midLayer1_loss: 1.3683 - val_rightLayer1_loss: 1.7354 - val_leftLayer2_loss: 0.1255 - val_midLayer2_loss: 1.2781 - val_rightLayer2_loss: 1.5774\n",
      "Epoch 8/11\n",
      "131/144 [==========================>...] - ETA: 0s - loss: 6.0108 - leftLayer1_loss: 0.1300 - midLayer1_loss: 1.3578 - rightLayer1_loss: 1.7053 - leftLayer2_loss: 0.1281 - midLayer2_loss: 1.2810 - rightLayer2_loss: 1.4086\n",
      "Epoch 00008: val_loss improved from 6.21585 to 6.15152, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 5.9974 - leftLayer1_loss: 0.1300 - midLayer1_loss: 1.3540 - rightLayer1_loss: 1.7044 - leftLayer2_loss: 0.1283 - midLayer2_loss: 1.2742 - rightLayer2_loss: 1.4064 - val_loss: 6.1515 - val_leftLayer1_loss: 0.1306 - val_midLayer1_loss: 1.3683 - val_rightLayer1_loss: 1.7105 - val_leftLayer2_loss: 0.1247 - val_midLayer2_loss: 1.2781 - val_rightLayer2_loss: 1.5393\n",
      "Epoch 9/11\n",
      "134/144 [==========================>...] - ETA: 0s - loss: 5.9081 - leftLayer1_loss: 0.1294 - midLayer1_loss: 1.3529 - rightLayer1_loss: 1.6807 - leftLayer2_loss: 0.1262 - midLayer2_loss: 1.2705 - rightLayer2_loss: 1.3483\n",
      "Epoch 00009: val_loss improved from 6.15152 to 6.08976, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.8977 - leftLayer1_loss: 0.1295 - midLayer1_loss: 1.3518 - rightLayer1_loss: 1.6786 - leftLayer2_loss: 0.1260 - midLayer2_loss: 1.2660 - rightLayer2_loss: 1.3458 - val_loss: 6.0898 - val_leftLayer1_loss: 0.1301 - val_midLayer1_loss: 1.3683 - val_rightLayer1_loss: 1.6858 - val_leftLayer2_loss: 0.1238 - val_midLayer2_loss: 1.2781 - val_rightLayer2_loss: 1.5036\n",
      "Epoch 10/11\n",
      "135/144 [===========================>..] - ETA: 0s - loss: 5.8575 - leftLayer1_loss: 0.1292 - midLayer1_loss: 1.3570 - rightLayer1_loss: 1.6502 - leftLayer2_loss: 0.1253 - midLayer2_loss: 1.2941 - rightLayer2_loss: 1.3018\n",
      "Epoch 00010: val_loss improved from 6.08976 to 6.03084, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.8442 - leftLayer1_loss: 0.1292 - midLayer1_loss: 1.3544 - rightLayer1_loss: 1.6486 - leftLayer2_loss: 0.1253 - midLayer2_loss: 1.2908 - rightLayer2_loss: 1.2958 - val_loss: 6.0308 - val_leftLayer1_loss: 0.1296 - val_midLayer1_loss: 1.3683 - val_rightLayer1_loss: 1.6615 - val_leftLayer2_loss: 0.1230 - val_midLayer2_loss: 1.2781 - val_rightLayer2_loss: 1.4704\n",
      "Epoch 11/11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/144 [===========================>..] - ETA: 0s - loss: 5.7603 - leftLayer1_loss: 0.1280 - midLayer1_loss: 1.3557 - rightLayer1_loss: 1.6238 - leftLayer2_loss: 0.1232 - midLayer2_loss: 1.2668 - rightLayer2_loss: 1.2627\n",
      "Epoch 00011: val_loss improved from 6.03084 to 5.97398, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.7514 - leftLayer1_loss: 0.1282 - midLayer1_loss: 1.3534 - rightLayer1_loss: 1.6224 - leftLayer2_loss: 0.1229 - midLayer2_loss: 1.2655 - rightLayer2_loss: 1.2590 - val_loss: 5.9740 - val_leftLayer1_loss: 0.1291 - val_midLayer1_loss: 1.3683 - val_rightLayer1_loss: 1.6376 - val_leftLayer2_loss: 0.1221 - val_midLayer2_loss: 1.2781 - val_rightLayer2_loss: 1.4388\n",
      "22433/22433 [==============================] - 30s 1ms/step\n",
      "** write log to ./experiments/0.009999999999999998_test.log **\n",
      "auroc 0Hernia: 0.3590994255770282\n",
      "\n",
      "auprc 0Hernia: 0.0013051105654496872\n",
      "\n",
      "auroc 1Hernia: 0.18794753844550638\n",
      "\n",
      "auprc 1Hernia: 0.0010534330687852244\n",
      "\n",
      "auroc 2Hernia: 0.67441531567743\n",
      "\n",
      "auprc 2Hernia: 0.003171093367092593\n",
      "\n",
      "auroc 3Hernia: 0.5586422903760226\n",
      "\n",
      "auprc 3Hernia: 0.0019535530774762427\n",
      "\n",
      "auroc 4Hernia: 0.5426484067790842\n",
      "\n",
      "auprc 4Hernia: 0.0022355430440078195\n",
      "\n",
      "auroc 5Hernia: 0.7204233843955159\n",
      "\n",
      "auprc 5Hernia: 0.009089697988380054\n",
      "\n",
      "mean auroc: 0.5071960602084312\n",
      "\n",
      "mean auprc: 0.0031347385185319363\n",
      "\n",
      "max auroc: 0.7204233843955159\n",
      "\n",
      "max auprc: 0.009089697988380054\n",
      "\n",
      "36.10655379295349\n",
      "** set output weights path to: ./experiments/0.010999999999999998_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 144 steps, validate for 41 steps\n",
      "Epoch 1/11\n",
      "136/144 [===========================>..] - ETA: 0s - loss: 6.6939 - leftLayer1_loss: 0.1251 - midLayer1_loss: 1.3658 - rightLayer1_loss: 1.8878 - leftLayer2_loss: 0.1317 - midLayer2_loss: 1.3596 - rightLayer2_loss: 1.8239\n",
      "Epoch 00001: val_loss improved from inf to 6.58460, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "144/144 [==============================] - 1s 9ms/step - loss: 6.6862 - leftLayer1_loss: 0.1251 - midLayer1_loss: 1.3636 - rightLayer1_loss: 1.8865 - leftLayer2_loss: 0.1317 - midLayer2_loss: 1.3584 - rightLayer2_loss: 1.8210 - val_loss: 6.5846 - val_leftLayer1_loss: 0.1243 - val_midLayer1_loss: 1.3606 - val_rightLayer1_loss: 1.8625 - val_leftLayer2_loss: 0.1273 - val_midLayer2_loss: 1.3004 - val_rightLayer2_loss: 1.8094\n",
      "Epoch 2/11\n",
      "127/144 [=========================>....] - ETA: 0s - loss: 6.6000 - leftLayer1_loss: 0.1249 - midLayer1_loss: 1.3684 - rightLayer1_loss: 1.8511 - leftLayer2_loss: 0.1292 - midLayer2_loss: 1.3687 - rightLayer2_loss: 1.7576\n",
      "Epoch 00002: val_loss improved from 6.58460 to 6.51072, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.5901 - leftLayer1_loss: 0.1247 - midLayer1_loss: 1.3669 - rightLayer1_loss: 1.8506 - leftLayer2_loss: 0.1294 - midLayer2_loss: 1.3666 - rightLayer2_loss: 1.7519 - val_loss: 6.5107 - val_leftLayer1_loss: 0.1238 - val_midLayer1_loss: 1.3606 - val_rightLayer1_loss: 1.8308 - val_leftLayer2_loss: 0.1266 - val_midLayer2_loss: 1.3004 - val_rightLayer2_loss: 1.7686\n",
      "Epoch 3/11\n",
      "133/144 [==========================>...] - ETA: 0s - loss: 6.4883 - leftLayer1_loss: 0.1242 - midLayer1_loss: 1.3660 - rightLayer1_loss: 1.8182 - leftLayer2_loss: 0.1292 - midLayer2_loss: 1.3599 - rightLayer2_loss: 1.6909\n",
      "Epoch 00003: val_loss improved from 6.51072 to 6.43787, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.4762 - leftLayer1_loss: 0.1240 - midLayer1_loss: 1.3647 - rightLayer1_loss: 1.8173 - leftLayer2_loss: 0.1291 - midLayer2_loss: 1.3525 - rightLayer2_loss: 1.6885 - val_loss: 6.4379 - val_leftLayer1_loss: 0.1232 - val_midLayer1_loss: 1.3606 - val_rightLayer1_loss: 1.7993 - val_leftLayer2_loss: 0.1258 - val_midLayer2_loss: 1.3004 - val_rightLayer2_loss: 1.7286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/11\n",
      "139/144 [===========================>..] - ETA: 0s - loss: 6.3829 - leftLayer1_loss: 0.1232 - midLayer1_loss: 1.3648 - rightLayer1_loss: 1.7812 - leftLayer2_loss: 0.1284 - midLayer2_loss: 1.3663 - rightLayer2_loss: 1.6190\n",
      "Epoch 00004: val_loss improved from 6.43787 to 6.36667, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.3788 - leftLayer1_loss: 0.1232 - midLayer1_loss: 1.3648 - rightLayer1_loss: 1.7816 - leftLayer2_loss: 0.1281 - midLayer2_loss: 1.3638 - rightLayer2_loss: 1.6172 - val_loss: 6.3667 - val_leftLayer1_loss: 0.1226 - val_midLayer1_loss: 1.3606 - val_rightLayer1_loss: 1.7681 - val_leftLayer2_loss: 0.1250 - val_midLayer2_loss: 1.3004 - val_rightLayer2_loss: 1.6899\n",
      "Epoch 5/11\n",
      "137/144 [===========================>..] - ETA: 0s - loss: 6.2620 - leftLayer1_loss: 0.1228 - midLayer1_loss: 1.3696 - rightLayer1_loss: 1.7476 - leftLayer2_loss: 0.1263 - midLayer2_loss: 1.3373 - rightLayer2_loss: 1.5585\n",
      "Epoch 00005: val_loss improved from 6.36667 to 6.29738, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.2569 - leftLayer1_loss: 0.1228 - midLayer1_loss: 1.3682 - rightLayer1_loss: 1.7476 - leftLayer2_loss: 0.1261 - midLayer2_loss: 1.3351 - rightLayer2_loss: 1.5570 - val_loss: 6.2974 - val_leftLayer1_loss: 0.1221 - val_midLayer1_loss: 1.3606 - val_rightLayer1_loss: 1.7373 - val_leftLayer2_loss: 0.1243 - val_midLayer2_loss: 1.3004 - val_rightLayer2_loss: 1.6527\n",
      "Epoch 6/11\n",
      "133/144 [==========================>...] - ETA: 0s - loss: 6.1891 - leftLayer1_loss: 0.1223 - midLayer1_loss: 1.3710 - rightLayer1_loss: 1.7176 - leftLayer2_loss: 0.1245 - midLayer2_loss: 1.3448 - rightLayer2_loss: 1.5088\n",
      "Epoch 00006: val_loss improved from 6.29738 to 6.22991, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.1778 - leftLayer1_loss: 0.1222 - midLayer1_loss: 1.3694 - rightLayer1_loss: 1.7165 - leftLayer2_loss: 0.1245 - midLayer2_loss: 1.3430 - rightLayer2_loss: 1.5022 - val_loss: 6.2299 - val_leftLayer1_loss: 0.1215 - val_midLayer1_loss: 1.3606 - val_rightLayer1_loss: 1.7071 - val_leftLayer2_loss: 0.1235 - val_midLayer2_loss: 1.3004 - val_rightLayer2_loss: 1.6168\n",
      "Epoch 7/11\n",
      "125/144 [=========================>....] - ETA: 0s - loss: 6.1081 - leftLayer1_loss: 0.1223 - midLayer1_loss: 1.3609 - rightLayer1_loss: 1.6811 - leftLayer2_loss: 0.1235 - midLayer2_loss: 1.3667 - rightLayer2_loss: 1.4536\n",
      "Epoch 00007: val_loss improved from 6.22991 to 6.16412, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.0989 - leftLayer1_loss: 0.1221 - midLayer1_loss: 1.3616 - rightLayer1_loss: 1.6808 - leftLayer2_loss: 0.1235 - midLayer2_loss: 1.3639 - rightLayer2_loss: 1.4470 - val_loss: 6.1641 - val_leftLayer1_loss: 0.1210 - val_midLayer1_loss: 1.3606 - val_rightLayer1_loss: 1.6773 - val_leftLayer2_loss: 0.1228 - val_midLayer2_loss: 1.3004 - val_rightLayer2_loss: 1.5821\n",
      "Epoch 8/11\n",
      "136/144 [===========================>..] - ETA: 0s - loss: 6.0283 - leftLayer1_loss: 0.1208 - midLayer1_loss: 1.3687 - rightLayer1_loss: 1.6516 - leftLayer2_loss: 0.1215 - midLayer2_loss: 1.3667 - rightLayer2_loss: 1.3989\n",
      "Epoch 00008: val_loss improved from 6.16412 to 6.10065, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.0178 - leftLayer1_loss: 0.1208 - midLayer1_loss: 1.3669 - rightLayer1_loss: 1.6503 - leftLayer2_loss: 0.1218 - midLayer2_loss: 1.3645 - rightLayer2_loss: 1.3935 - val_loss: 6.1006 - val_leftLayer1_loss: 0.1204 - val_midLayer1_loss: 1.3606 - val_rightLayer1_loss: 1.6482 - val_leftLayer2_loss: 0.1220 - val_midLayer2_loss: 1.3004 - val_rightLayer2_loss: 1.5490\n",
      "Epoch 9/11\n",
      "129/144 [=========================>....] - ETA: 0s - loss: 5.9350 - leftLayer1_loss: 0.1206 - midLayer1_loss: 1.3656 - rightLayer1_loss: 1.6182 - leftLayer2_loss: 0.1211 - midLayer2_loss: 1.3579 - rightLayer2_loss: 1.3515\n",
      "Epoch 00009: val_loss improved from 6.10065 to 6.03935, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.9196 - leftLayer1_loss: 0.1205 - midLayer1_loss: 1.3654 - rightLayer1_loss: 1.6181 - leftLayer2_loss: 0.1210 - midLayer2_loss: 1.3499 - rightLayer2_loss: 1.3445 - val_loss: 6.0393 - val_leftLayer1_loss: 0.1199 - val_midLayer1_loss: 1.3606 - val_rightLayer1_loss: 1.6197 - val_leftLayer2_loss: 0.1213 - val_midLayer2_loss: 1.3004 - val_rightLayer2_loss: 1.5175\n",
      "Epoch 10/11\n",
      "121/144 [========================>.....] - ETA: 0s - loss: 5.8770 - leftLayer1_loss: 0.1201 - midLayer1_loss: 1.3692 - rightLayer1_loss: 1.5909 - leftLayer2_loss: 0.1185 - midLayer2_loss: 1.3691 - rightLayer2_loss: 1.3092\n",
      "Epoch 00010: val_loss improved from 6.03935 to 5.98072, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.8553 - leftLayer1_loss: 0.1200 - midLayer1_loss: 1.3672 - rightLayer1_loss: 1.5891 - leftLayer2_loss: 0.1183 - midLayer2_loss: 1.3605 - rightLayer2_loss: 1.3002 - val_loss: 5.9807 - val_leftLayer1_loss: 0.1194 - val_midLayer1_loss: 1.3606 - val_rightLayer1_loss: 1.5918 - val_leftLayer2_loss: 0.1206 - val_midLayer2_loss: 1.3004 - val_rightLayer2_loss: 1.4880\n",
      "Epoch 11/11\n",
      "133/144 [==========================>...] - ETA: 0s - loss: 5.7620 - leftLayer1_loss: 0.1195 - midLayer1_loss: 1.3651 - rightLayer1_loss: 1.5582 - leftLayer2_loss: 0.1195 - midLayer2_loss: 1.3215 - rightLayer2_loss: 1.2783\n",
      "Epoch 00011: val_loss improved from 5.98072 to 5.92421, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.7531 - leftLayer1_loss: 0.1194 - midLayer1_loss: 1.3638 - rightLayer1_loss: 1.5567 - leftLayer2_loss: 0.1192 - midLayer2_loss: 1.3223 - rightLayer2_loss: 1.2717 - val_loss: 5.9242 - val_leftLayer1_loss: 0.1188 - val_midLayer1_loss: 1.3606 - val_rightLayer1_loss: 1.5645 - val_leftLayer2_loss: 0.1198 - val_midLayer2_loss: 1.3004 - val_rightLayer2_loss: 1.4600\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.010999999999999998_test.log **\n",
      "auroc 0Hernia: 0.30651664890868147\n",
      "\n",
      "auprc 0Hernia: 0.0012219475684803881\n",
      "\n",
      "auroc 1Hernia: 0.8170661681670569\n",
      "\n",
      "auprc 1Hernia: 0.08285041190408447\n",
      "\n",
      "auroc 2Hernia: 0.8295206832677244\n",
      "\n",
      "auprc 2Hernia: 0.010364873091101703\n",
      "\n",
      "auroc 3Hernia: 0.3868242129597138\n",
      "\n",
      "auprc 3Hernia: 0.0013654805946966496\n",
      "\n",
      "auroc 4Hernia: 0.678294425268656\n",
      "\n",
      "auprc 4Hernia: 0.004738672101651471\n",
      "\n",
      "auroc 5Hernia: 0.8324837147578428\n",
      "\n",
      "auprc 5Hernia: 0.08328864524991408\n",
      "\n",
      "mean auroc: 0.6417843088882793\n",
      "\n",
      "mean auprc: 0.030638338418321462\n",
      "\n",
      "max auroc: 0.8324837147578428\n",
      "\n",
      "max auprc: 0.08328864524991408\n",
      "\n",
      "34.59628248214722\n",
      "** set output weights path to: ./experiments/0.011999999999999997_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 144 steps, validate for 41 steps\n",
      "Epoch 1/11\n",
      "135/144 [===========================>..] - ETA: 0s - loss: 6.8123 - leftLayer1_loss: 0.1252 - midLayer1_loss: 1.3935 - rightLayer1_loss: 1.8659 - leftLayer2_loss: 0.1329 - midLayer2_loss: 1.4160 - rightLayer2_loss: 1.8788\n",
      "Epoch 00001: val_loss improved from inf to 6.61602, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "144/144 [==============================] - 1s 9ms/step - loss: 6.8053 - leftLayer1_loss: 0.1252 - midLayer1_loss: 1.3905 - rightLayer1_loss: 1.8649 - leftLayer2_loss: 0.1328 - midLayer2_loss: 1.4141 - rightLayer2_loss: 1.8778 - val_loss: 6.6160 - val_leftLayer1_loss: 0.1240 - val_midLayer1_loss: 1.3895 - val_rightLayer1_loss: 1.8483 - val_leftLayer2_loss: 0.1296 - val_midLayer2_loss: 1.2819 - val_rightLayer2_loss: 1.8427\n",
      "Epoch 2/11\n",
      "134/144 [==========================>...] - ETA: 0s - loss: 6.6933 - leftLayer1_loss: 0.1244 - midLayer1_loss: 1.3951 - rightLayer1_loss: 1.8300 - leftLayer2_loss: 0.1292 - midLayer2_loss: 1.4112 - rightLayer2_loss: 1.8034\n",
      "Epoch 00002: val_loss improved from 6.61602 to 6.53893, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.6865 - leftLayer1_loss: 0.1244 - midLayer1_loss: 1.3913 - rightLayer1_loss: 1.8298 - leftLayer2_loss: 0.1291 - midLayer2_loss: 1.4121 - rightLayer2_loss: 1.7998 - val_loss: 6.5389 - val_leftLayer1_loss: 0.1234 - val_midLayer1_loss: 1.3895 - val_rightLayer1_loss: 1.8164 - val_leftLayer2_loss: 0.1288 - val_midLayer2_loss: 1.2819 - val_rightLayer2_loss: 1.7989\n",
      "Epoch 3/11\n",
      "134/144 [==========================>...] - ETA: 0s - loss: 6.5857 - leftLayer1_loss: 0.1238 - midLayer1_loss: 1.3958 - rightLayer1_loss: 1.7945 - leftLayer2_loss: 0.1292 - midLayer2_loss: 1.4004 - rightLayer2_loss: 1.7420\n",
      "Epoch 00003: val_loss improved from 6.53893 to 6.46381, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.5740 - leftLayer1_loss: 0.1238 - midLayer1_loss: 1.3917 - rightLayer1_loss: 1.7929 - leftLayer2_loss: 0.1292 - midLayer2_loss: 1.3972 - rightLayer2_loss: 1.7392 - val_loss: 6.4638 - val_leftLayer1_loss: 0.1229 - val_midLayer1_loss: 1.3895 - val_rightLayer1_loss: 1.7849 - val_leftLayer2_loss: 0.1280 - val_midLayer2_loss: 1.2819 - val_rightLayer2_loss: 1.7567\n",
      "Epoch 4/11\n",
      "129/144 [=========================>....] - ETA: 0s - loss: 6.4832 - leftLayer1_loss: 0.1230 - midLayer1_loss: 1.3969 - rightLayer1_loss: 1.7604 - leftLayer2_loss: 0.1287 - midLayer2_loss: 1.4100 - rightLayer2_loss: 1.6642\n",
      "Epoch 00004: val_loss improved from 6.46381 to 6.39007, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.4707 - leftLayer1_loss: 0.1230 - midLayer1_loss: 1.3911 - rightLayer1_loss: 1.7594 - leftLayer2_loss: 0.1290 - midLayer2_loss: 1.4085 - rightLayer2_loss: 1.6598 - val_loss: 6.3901 - val_leftLayer1_loss: 0.1223 - val_midLayer1_loss: 1.3895 - val_rightLayer1_loss: 1.7539 - val_leftLayer2_loss: 0.1271 - val_midLayer2_loss: 1.2819 - val_rightLayer2_loss: 1.7153\n",
      "Epoch 5/11\n",
      "137/144 [===========================>..] - ETA: 0s - loss: 6.3921 - leftLayer1_loss: 0.1228 - midLayer1_loss: 1.3922 - rightLayer1_loss: 1.7288 - leftLayer2_loss: 0.1272 - midLayer2_loss: 1.4120 - rightLayer2_loss: 1.6091\n",
      "Epoch 00005: val_loss improved from 6.39007 to 6.31860, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.3883 - leftLayer1_loss: 0.1228 - midLayer1_loss: 1.3909 - rightLayer1_loss: 1.7283 - leftLayer2_loss: 0.1271 - midLayer2_loss: 1.4119 - rightLayer2_loss: 1.6073 - val_loss: 6.3186 - val_leftLayer1_loss: 0.1218 - val_midLayer1_loss: 1.3895 - val_rightLayer1_loss: 1.7234 - val_leftLayer2_loss: 0.1263 - val_midLayer2_loss: 1.2819 - val_rightLayer2_loss: 1.6757\n",
      "Epoch 6/11\n",
      "133/144 [==========================>...] - ETA: 0s - loss: 6.2753 - leftLayer1_loss: 0.1222 - midLayer1_loss: 1.3999 - rightLayer1_loss: 1.6966 - leftLayer2_loss: 0.1257 - midLayer2_loss: 1.3899 - rightLayer2_loss: 1.5411\n",
      "Epoch 00006: val_loss improved from 6.31860 to 6.24904, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.2677 - leftLayer1_loss: 0.1222 - midLayer1_loss: 1.3965 - rightLayer1_loss: 1.6943 - leftLayer2_loss: 0.1257 - midLayer2_loss: 1.3910 - rightLayer2_loss: 1.5380 - val_loss: 6.2490 - val_leftLayer1_loss: 0.1212 - val_midLayer1_loss: 1.3895 - val_rightLayer1_loss: 1.6933 - val_leftLayer2_loss: 0.1256 - val_midLayer2_loss: 1.2819 - val_rightLayer2_loss: 1.6376\n",
      "Epoch 7/11\n",
      "132/144 [==========================>...] - ETA: 0s - loss: 6.1957 - leftLayer1_loss: 0.1214 - midLayer1_loss: 1.3906 - rightLayer1_loss: 1.6626 - leftLayer2_loss: 0.1231 - midLayer2_loss: 1.4012 - rightLayer2_loss: 1.4968\n",
      "Epoch 00007: val_loss improved from 6.24904 to 6.18167, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.1901 - leftLayer1_loss: 0.1214 - midLayer1_loss: 1.3869 - rightLayer1_loss: 1.6605 - leftLayer2_loss: 0.1231 - midLayer2_loss: 1.4075 - rightLayer2_loss: 1.4907 - val_loss: 6.1817 - val_leftLayer1_loss: 0.1207 - val_midLayer1_loss: 1.3895 - val_rightLayer1_loss: 1.6638 - val_leftLayer2_loss: 0.1248 - val_midLayer2_loss: 1.2819 - val_rightLayer2_loss: 1.6011\n",
      "Epoch 8/11\n",
      "135/144 [===========================>..] - ETA: 0s - loss: 6.1026 - leftLayer1_loss: 0.1208 - midLayer1_loss: 1.3962 - rightLayer1_loss: 1.6298 - leftLayer2_loss: 0.1220 - midLayer2_loss: 1.3897 - rightLayer2_loss: 1.4440\n",
      "Epoch 00008: val_loss improved from 6.18167 to 6.11710, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.0905 - leftLayer1_loss: 0.1209 - midLayer1_loss: 1.3916 - rightLayer1_loss: 1.6280 - leftLayer2_loss: 0.1220 - midLayer2_loss: 1.3890 - rightLayer2_loss: 1.4390 - val_loss: 6.1171 - val_leftLayer1_loss: 0.1201 - val_midLayer1_loss: 1.3895 - val_rightLayer1_loss: 1.6348 - val_leftLayer2_loss: 0.1240 - val_midLayer2_loss: 1.2819 - val_rightLayer2_loss: 1.5668\n",
      "Epoch 9/11\n",
      "143/144 [============================>.] - ETA: 0s - loss: 6.0036 - leftLayer1_loss: 0.1203 - midLayer1_loss: 1.3918 - rightLayer1_loss: 1.6003 - leftLayer2_loss: 0.1210 - midLayer2_loss: 1.3981 - rightLayer2_loss: 1.3720\n",
      "Epoch 00009: val_loss improved from 6.11710 to 6.05460, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "144/144 [==============================] - 1s 10ms/step - loss: 5.9987 - leftLayer1_loss: 0.1203 - midLayer1_loss: 1.3913 - rightLayer1_loss: 1.5997 - leftLayer2_loss: 0.1211 - midLayer2_loss: 1.3960 - rightLayer2_loss: 1.3703 - val_loss: 6.0546 - val_leftLayer1_loss: 0.1196 - val_midLayer1_loss: 1.3895 - val_rightLayer1_loss: 1.6066 - val_leftLayer2_loss: 0.1232 - val_midLayer2_loss: 1.2819 - val_rightLayer2_loss: 1.5338\n",
      "Epoch 10/11\n",
      "129/144 [=========================>....] - ETA: 0s - loss: 5.9472 - leftLayer1_loss: 0.1198 - midLayer1_loss: 1.3934 - rightLayer1_loss: 1.5723 - leftLayer2_loss: 0.1216 - midLayer2_loss: 1.4110 - rightLayer2_loss: 1.3291\n",
      "Epoch 00010: val_loss improved from 6.05460 to 5.99441, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.9371 - leftLayer1_loss: 0.1198 - midLayer1_loss: 1.3891 - rightLayer1_loss: 1.5702 - leftLayer2_loss: 0.1216 - midLayer2_loss: 1.4101 - rightLayer2_loss: 1.3264 - val_loss: 5.9944 - val_leftLayer1_loss: 0.1190 - val_midLayer1_loss: 1.3895 - val_rightLayer1_loss: 1.5792 - val_leftLayer2_loss: 0.1225 - val_midLayer2_loss: 1.2819 - val_rightLayer2_loss: 1.5023\n",
      "Epoch 11/11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/144 [=========================>....] - ETA: 0s - loss: 5.8592 - leftLayer1_loss: 0.1190 - midLayer1_loss: 1.3912 - rightLayer1_loss: 1.5351 - leftLayer2_loss: 0.1194 - midLayer2_loss: 1.4095 - rightLayer2_loss: 1.2850\n",
      "Epoch 00011: val_loss improved from 5.99441 to 5.93682, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.8507 - leftLayer1_loss: 0.1190 - midLayer1_loss: 1.3883 - rightLayer1_loss: 1.5360 - leftLayer2_loss: 0.1191 - midLayer2_loss: 1.4042 - rightLayer2_loss: 1.2841 - val_loss: 5.9368 - val_leftLayer1_loss: 0.1185 - val_midLayer1_loss: 1.3895 - val_rightLayer1_loss: 1.5523 - val_leftLayer2_loss: 0.1217 - val_midLayer2_loss: 1.2819 - val_rightLayer2_loss: 1.4730\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.011999999999999997_test.log **\n",
      "auroc 0Hernia: 0.695018831971179\n",
      "\n",
      "auprc 0Hernia: 0.003422756660824259\n",
      "\n",
      "auroc 1Hernia: 0.5699638034839678\n",
      "\n",
      "auprc 1Hernia: 0.002175863850537643\n",
      "\n",
      "auroc 2Hernia: 0.3877902686240858\n",
      "\n",
      "auprc 2Hernia: 0.001382207741703812\n",
      "\n",
      "auroc 3Hernia: 0.420517597419031\n",
      "\n",
      "auprc 3Hernia: 0.001452805436295107\n",
      "\n",
      "auroc 4Hernia: 0.4518253507467924\n",
      "\n",
      "auprc 4Hernia: 0.0016516098510292111\n",
      "\n",
      "auroc 5Hernia: 0.6776766175185183\n",
      "\n",
      "auprc 5Hernia: 0.0038008485383437965\n",
      "\n",
      "mean auroc: 0.5337987449605958\n",
      "\n",
      "mean auprc: 0.002314348679788971\n",
      "\n",
      "max auroc: 0.695018831971179\n",
      "\n",
      "max auprc: 0.0038008485383437965\n",
      "\n",
      "34.85614895820618\n",
      "** set output weights path to: ./experiments/0.012999999999999996_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 144 steps, validate for 41 steps\n",
      "Epoch 1/11\n",
      "134/144 [==========================>...] - ETA: 0s - loss: 6.6678 - leftLayer1_loss: 0.1200 - midLayer1_loss: 1.3703 - rightLayer1_loss: 1.8449 - leftLayer2_loss: 0.1194 - midLayer2_loss: 1.3921 - rightLayer2_loss: 1.8211\n",
      "Epoch 00001: val_loss improved from inf to 6.58737, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "144/144 [==============================] - 1s 9ms/step - loss: 6.6604 - leftLayer1_loss: 0.1200 - midLayer1_loss: 1.3681 - rightLayer1_loss: 1.8429 - leftLayer2_loss: 0.1195 - midLayer2_loss: 1.3904 - rightLayer2_loss: 1.8196 - val_loss: 6.5874 - val_leftLayer1_loss: 0.1214 - val_midLayer1_loss: 1.3728 - val_rightLayer1_loss: 1.8420 - val_leftLayer2_loss: 0.1186 - val_midLayer2_loss: 1.3207 - val_rightLayer2_loss: 1.8118\n",
      "Epoch 2/11\n",
      "143/144 [============================>.] - ETA: 0s - loss: 6.5593 - leftLayer1_loss: 0.1196 - midLayer1_loss: 1.3683 - rightLayer1_loss: 1.8139 - leftLayer2_loss: 0.1187 - midLayer2_loss: 1.3826 - rightLayer2_loss: 1.7563\n",
      "Epoch 00002: val_loss improved from 6.58737 to 6.51539, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.5565 - leftLayer1_loss: 0.1196 - midLayer1_loss: 1.3675 - rightLayer1_loss: 1.8138 - leftLayer2_loss: 0.1186 - midLayer2_loss: 1.3812 - rightLayer2_loss: 1.7558 - val_loss: 6.5154 - val_leftLayer1_loss: 0.1210 - val_midLayer1_loss: 1.3728 - val_rightLayer1_loss: 1.8154 - val_leftLayer2_loss: 0.1179 - val_midLayer2_loss: 1.3207 - val_rightLayer2_loss: 1.7677\n",
      "Epoch 3/11\n",
      "133/144 [==========================>...] - ETA: 0s - loss: 6.5187 - leftLayer1_loss: 0.1191 - midLayer1_loss: 1.3703 - rightLayer1_loss: 1.7873 - leftLayer2_loss: 0.1169 - midLayer2_loss: 1.4181 - rightLayer2_loss: 1.7070\n",
      "Epoch 00003: val_loss improved from 6.51539 to 6.44476, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.5071 - leftLayer1_loss: 0.1190 - midLayer1_loss: 1.3673 - rightLayer1_loss: 1.7843 - leftLayer2_loss: 0.1169 - midLayer2_loss: 1.4165 - rightLayer2_loss: 1.7031 - val_loss: 6.4448 - val_leftLayer1_loss: 0.1205 - val_midLayer1_loss: 1.3728 - val_rightLayer1_loss: 1.7892 - val_leftLayer2_loss: 0.1171 - val_midLayer2_loss: 1.3207 - val_rightLayer2_loss: 1.7244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/11\n",
      "137/144 [===========================>..] - ETA: 0s - loss: 6.3628 - leftLayer1_loss: 0.1188 - midLayer1_loss: 1.3641 - rightLayer1_loss: 1.7540 - leftLayer2_loss: 0.1168 - midLayer2_loss: 1.3894 - rightLayer2_loss: 1.6198\n",
      "Epoch 00004: val_loss improved from 6.44476 to 6.37594, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.3566 - leftLayer1_loss: 0.1187 - midLayer1_loss: 1.3636 - rightLayer1_loss: 1.7528 - leftLayer2_loss: 0.1166 - midLayer2_loss: 1.3871 - rightLayer2_loss: 1.6178 - val_loss: 6.3759 - val_leftLayer1_loss: 0.1201 - val_midLayer1_loss: 1.3728 - val_rightLayer1_loss: 1.7631 - val_leftLayer2_loss: 0.1164 - val_midLayer2_loss: 1.3207 - val_rightLayer2_loss: 1.6829\n",
      "Epoch 5/11\n",
      "136/144 [===========================>..] - ETA: 0s - loss: 6.2749 - leftLayer1_loss: 0.1181 - midLayer1_loss: 1.3655 - rightLayer1_loss: 1.7267 - leftLayer2_loss: 0.1141 - midLayer2_loss: 1.3906 - rightLayer2_loss: 1.5599\n",
      "Epoch 00005: val_loss improved from 6.37594 to 6.30837, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.2678 - leftLayer1_loss: 0.1180 - midLayer1_loss: 1.3637 - rightLayer1_loss: 1.7244 - leftLayer2_loss: 0.1139 - midLayer2_loss: 1.3910 - rightLayer2_loss: 1.5567 - val_loss: 6.3084 - val_leftLayer1_loss: 0.1196 - val_midLayer1_loss: 1.3728 - val_rightLayer1_loss: 1.7373 - val_leftLayer2_loss: 0.1156 - val_midLayer2_loss: 1.3207 - val_rightLayer2_loss: 1.6423\n",
      "Epoch 6/11\n",
      "132/144 [==========================>...] - ETA: 0s - loss: 6.2258 - leftLayer1_loss: 0.1178 - midLayer1_loss: 1.3694 - rightLayer1_loss: 1.7023 - leftLayer2_loss: 0.1139 - midLayer2_loss: 1.4222 - rightLayer2_loss: 1.5004\n",
      "Epoch 00006: val_loss improved from 6.30837 to 6.24350, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.2098 - leftLayer1_loss: 0.1177 - midLayer1_loss: 1.3667 - rightLayer1_loss: 1.6996 - leftLayer2_loss: 0.1135 - midLayer2_loss: 1.4167 - rightLayer2_loss: 1.4955 - val_loss: 6.2435 - val_leftLayer1_loss: 0.1192 - val_midLayer1_loss: 1.3728 - val_rightLayer1_loss: 1.7120 - val_leftLayer2_loss: 0.1149 - val_midLayer2_loss: 1.3207 - val_rightLayer2_loss: 1.6039\n",
      "Epoch 7/11\n",
      "135/144 [===========================>..] - ETA: 0s - loss: 6.1171 - leftLayer1_loss: 0.1172 - midLayer1_loss: 1.3711 - rightLayer1_loss: 1.6698 - leftLayer2_loss: 0.1113 - midLayer2_loss: 1.4050 - rightLayer2_loss: 1.4427\n",
      "Epoch 00007: val_loss improved from 6.24350 to 6.18089, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.1080 - leftLayer1_loss: 0.1170 - midLayer1_loss: 1.3687 - rightLayer1_loss: 1.6677 - leftLayer2_loss: 0.1113 - midLayer2_loss: 1.4013 - rightLayer2_loss: 1.4420 - val_loss: 6.1809 - val_leftLayer1_loss: 0.1187 - val_midLayer1_loss: 1.3728 - val_rightLayer1_loss: 1.6870 - val_leftLayer2_loss: 0.1142 - val_midLayer2_loss: 1.3207 - val_rightLayer2_loss: 1.5675\n",
      "Epoch 8/11\n",
      "134/144 [==========================>...] - ETA: 0s - loss: 6.0409 - leftLayer1_loss: 0.1168 - midLayer1_loss: 1.3688 - rightLayer1_loss: 1.6422 - leftLayer2_loss: 0.1120 - midLayer2_loss: 1.4135 - rightLayer2_loss: 1.3875\n",
      "Epoch 00008: val_loss improved from 6.18089 to 6.12018, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.0285 - leftLayer1_loss: 0.1167 - midLayer1_loss: 1.3662 - rightLayer1_loss: 1.6400 - leftLayer2_loss: 0.1118 - midLayer2_loss: 1.4092 - rightLayer2_loss: 1.3845 - val_loss: 6.1202 - val_leftLayer1_loss: 0.1183 - val_midLayer1_loss: 1.3728 - val_rightLayer1_loss: 1.6624 - val_leftLayer2_loss: 0.1135 - val_midLayer2_loss: 1.3207 - val_rightLayer2_loss: 1.5325\n",
      "Epoch 9/11\n",
      "136/144 [===========================>..] - ETA: 0s - loss: 5.9527 - leftLayer1_loss: 0.1161 - midLayer1_loss: 1.3725 - rightLayer1_loss: 1.6185 - leftLayer2_loss: 0.1096 - midLayer2_loss: 1.3934 - rightLayer2_loss: 1.3426\n",
      "Epoch 00009: val_loss improved from 6.12018 to 6.06240, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.9431 - leftLayer1_loss: 0.1160 - midLayer1_loss: 1.3704 - rightLayer1_loss: 1.6161 - leftLayer2_loss: 0.1093 - midLayer2_loss: 1.3928 - rightLayer2_loss: 1.3385 - val_loss: 6.0624 - val_leftLayer1_loss: 0.1179 - val_midLayer1_loss: 1.3728 - val_rightLayer1_loss: 1.6383 - val_leftLayer2_loss: 0.1128 - val_midLayer2_loss: 1.3207 - val_rightLayer2_loss: 1.5000\n",
      "Epoch 10/11\n",
      "126/144 [=========================>....] - ETA: 0s - loss: 5.8868 - leftLayer1_loss: 0.1160 - midLayer1_loss: 1.3680 - rightLayer1_loss: 1.5908 - leftLayer2_loss: 0.1097 - midLayer2_loss: 1.4054 - rightLayer2_loss: 1.2969\n",
      "Epoch 00010: val_loss improved from 6.06240 to 6.00697, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.8850 - leftLayer1_loss: 0.1158 - midLayer1_loss: 1.3675 - rightLayer1_loss: 1.5904 - leftLayer2_loss: 0.1098 - midLayer2_loss: 1.4068 - rightLayer2_loss: 1.2946 - val_loss: 6.0070 - val_leftLayer1_loss: 0.1174 - val_midLayer1_loss: 1.3728 - val_rightLayer1_loss: 1.6147 - val_leftLayer2_loss: 0.1121 - val_midLayer2_loss: 1.3207 - val_rightLayer2_loss: 1.4693\n",
      "Epoch 11/11\n",
      "135/144 [===========================>..] - ETA: 0s - loss: 5.8230 - leftLayer1_loss: 0.1153 - midLayer1_loss: 1.3692 - rightLayer1_loss: 1.5653 - leftLayer2_loss: 0.1084 - midLayer2_loss: 1.4099 - rightLayer2_loss: 1.2551\n",
      "Epoch 00011: val_loss improved from 6.00697 to 5.95364, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.8025 - leftLayer1_loss: 0.1151 - midLayer1_loss: 1.3669 - rightLayer1_loss: 1.5618 - leftLayer2_loss: 0.1082 - midLayer2_loss: 1.4031 - rightLayer2_loss: 1.2473 - val_loss: 5.9536 - val_leftLayer1_loss: 0.1170 - val_midLayer1_loss: 1.3728 - val_rightLayer1_loss: 1.5915 - val_leftLayer2_loss: 0.1114 - val_midLayer2_loss: 1.3207 - val_rightLayer2_loss: 1.4402\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.012999999999999996_test.log **\n",
      "auroc 0Hernia: 0.44113600064651826\n",
      "\n",
      "auprc 0Hernia: 0.0015558970074343846\n",
      "\n",
      "auroc 1Hernia: 0.22653659739989068\n",
      "\n",
      "auprc 1Hernia: 0.0011007617668254176\n",
      "\n",
      "auroc 2Hernia: 0.5873878960721889\n",
      "\n",
      "auprc 2Hernia: 0.005424326701676047\n",
      "\n",
      "auroc 3Hernia: 0.6584012283847038\n",
      "\n",
      "auprc 3Hernia: 0.0028322927307979735\n",
      "\n",
      "auroc 4Hernia: 0.3682947655414272\n",
      "\n",
      "auprc 4Hernia: 0.0013438850631831926\n",
      "\n",
      "auroc 5Hernia: 0.5974169043259303\n",
      "\n",
      "auprc 5Hernia: 0.002362728699014289\n",
      "\n",
      "mean auroc: 0.47986223206177653\n",
      "\n",
      "mean auprc: 0.0024366486614885506\n",
      "\n",
      "max auroc: 0.6584012283847038\n",
      "\n",
      "max auprc: 0.005424326701676047\n",
      "\n",
      "34.30113744735718\n",
      "** set output weights path to: ./experiments/0.013999999999999995_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 144 steps, validate for 41 steps\n",
      "Epoch 1/11\n",
      "138/144 [===========================>..] - ETA: 0s - loss: 6.7306 - leftLayer1_loss: 0.1216 - midLayer1_loss: 1.3220 - rightLayer1_loss: 1.9036 - leftLayer2_loss: 0.1308 - midLayer2_loss: 1.3893 - rightLayer2_loss: 1.8633\n",
      "Epoch 00001: val_loss improved from inf to 6.58842, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "144/144 [==============================] - 1s 9ms/step - loss: 6.7264 - leftLayer1_loss: 0.1216 - midLayer1_loss: 1.3220 - rightLayer1_loss: 1.9038 - leftLayer2_loss: 0.1309 - midLayer2_loss: 1.3864 - rightLayer2_loss: 1.8617 - val_loss: 6.5884 - val_leftLayer1_loss: 0.1216 - val_midLayer1_loss: 1.3273 - val_rightLayer1_loss: 1.9001 - val_leftLayer2_loss: 0.1280 - val_midLayer2_loss: 1.2933 - val_rightLayer2_loss: 1.8181\n",
      "Epoch 2/11\n",
      "136/144 [===========================>..] - ETA: 0s - loss: 6.6673 - leftLayer1_loss: 0.1210 - midLayer1_loss: 1.3269 - rightLayer1_loss: 1.8710 - leftLayer2_loss: 0.1305 - midLayer2_loss: 1.4201 - rightLayer2_loss: 1.7978\n",
      "Epoch 00002: val_loss improved from 6.58842 to 6.51415, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.6598 - leftLayer1_loss: 0.1209 - midLayer1_loss: 1.3261 - rightLayer1_loss: 1.8702 - leftLayer2_loss: 0.1305 - midLayer2_loss: 1.4166 - rightLayer2_loss: 1.7954 - val_loss: 6.5141 - val_leftLayer1_loss: 0.1211 - val_midLayer1_loss: 1.3273 - val_rightLayer1_loss: 1.8702 - val_leftLayer2_loss: 0.1272 - val_midLayer2_loss: 1.2933 - val_rightLayer2_loss: 1.7750\n",
      "Epoch 3/11\n",
      "134/144 [==========================>...] - ETA: 0s - loss: 6.5557 - leftLayer1_loss: 0.1205 - midLayer1_loss: 1.3252 - rightLayer1_loss: 1.8369 - leftLayer2_loss: 0.1286 - midLayer2_loss: 1.4122 - rightLayer2_loss: 1.7324\n",
      "Epoch 00003: val_loss improved from 6.51415 to 6.44137, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.5440 - leftLayer1_loss: 0.1205 - midLayer1_loss: 1.3240 - rightLayer1_loss: 1.8364 - leftLayer2_loss: 0.1286 - midLayer2_loss: 1.4081 - rightLayer2_loss: 1.7264 - val_loss: 6.4414 - val_leftLayer1_loss: 0.1206 - val_midLayer1_loss: 1.3273 - val_rightLayer1_loss: 1.8406 - val_leftLayer2_loss: 0.1264 - val_midLayer2_loss: 1.2933 - val_rightLayer2_loss: 1.7332\n",
      "Epoch 4/11\n",
      "127/144 [=========================>....] - ETA: 0s - loss: 6.4711 - leftLayer1_loss: 0.1198 - midLayer1_loss: 1.3245 - rightLayer1_loss: 1.8063 - leftLayer2_loss: 0.1272 - midLayer2_loss: 1.4210 - rightLayer2_loss: 1.6723\n",
      "Epoch 00004: val_loss improved from 6.44137 to 6.36961, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.4522 - leftLayer1_loss: 0.1198 - midLayer1_loss: 1.3242 - rightLayer1_loss: 1.8033 - leftLayer2_loss: 0.1267 - midLayer2_loss: 1.4122 - rightLayer2_loss: 1.6661 - val_loss: 6.3696 - val_leftLayer1_loss: 0.1201 - val_midLayer1_loss: 1.3273 - val_rightLayer1_loss: 1.8111 - val_leftLayer2_loss: 0.1256 - val_midLayer2_loss: 1.2933 - val_rightLayer2_loss: 1.6922\n",
      "Epoch 5/11\n",
      "135/144 [===========================>..] - ETA: 0s - loss: 6.3777 - leftLayer1_loss: 0.1195 - midLayer1_loss: 1.3212 - rightLayer1_loss: 1.7727 - leftLayer2_loss: 0.1280 - midLayer2_loss: 1.4335 - rightLayer2_loss: 1.6028\n",
      "Epoch 00005: val_loss improved from 6.36961 to 6.29936, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.3652 - leftLayer1_loss: 0.1195 - midLayer1_loss: 1.3198 - rightLayer1_loss: 1.7722 - leftLayer2_loss: 0.1279 - midLayer2_loss: 1.4258 - rightLayer2_loss: 1.6000 - val_loss: 6.2994 - val_leftLayer1_loss: 0.1196 - val_midLayer1_loss: 1.3273 - val_rightLayer1_loss: 1.7818 - val_leftLayer2_loss: 0.1248 - val_midLayer2_loss: 1.2933 - val_rightLayer2_loss: 1.6525\n",
      "Epoch 6/11\n",
      "135/144 [===========================>..] - ETA: 0s - loss: 6.2576 - leftLayer1_loss: 0.1186 - midLayer1_loss: 1.3194 - rightLayer1_loss: 1.7399 - leftLayer2_loss: 0.1231 - midLayer2_loss: 1.4152 - rightLayer2_loss: 1.5415\n",
      "Epoch 00006: val_loss improved from 6.29936 to 6.23128, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.2464 - leftLayer1_loss: 0.1186 - midLayer1_loss: 1.3183 - rightLayer1_loss: 1.7392 - leftLayer2_loss: 0.1231 - midLayer2_loss: 1.4078 - rightLayer2_loss: 1.5394 - val_loss: 6.2313 - val_leftLayer1_loss: 0.1191 - val_midLayer1_loss: 1.3273 - val_rightLayer1_loss: 1.7530 - val_leftLayer2_loss: 0.1240 - val_midLayer2_loss: 1.2933 - val_rightLayer2_loss: 1.6146\n",
      "Epoch 7/11\n",
      "133/144 [==========================>...] - ETA: 0s - loss: 6.1524 - leftLayer1_loss: 0.1186 - midLayer1_loss: 1.3216 - rightLayer1_loss: 1.7083 - leftLayer2_loss: 0.1233 - midLayer2_loss: 1.4049 - rightLayer2_loss: 1.4756\n",
      "Epoch 00007: val_loss improved from 6.23128 to 6.16523, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.1380 - leftLayer1_loss: 0.1185 - midLayer1_loss: 1.3212 - rightLayer1_loss: 1.7069 - leftLayer2_loss: 0.1233 - midLayer2_loss: 1.3975 - rightLayer2_loss: 1.4706 - val_loss: 6.1652 - val_leftLayer1_loss: 0.1186 - val_midLayer1_loss: 1.3273 - val_rightLayer1_loss: 1.7247 - val_leftLayer2_loss: 0.1232 - val_midLayer2_loss: 1.2933 - val_rightLayer2_loss: 1.5781\n",
      "Epoch 8/11\n",
      "133/144 [==========================>...] - ETA: 0s - loss: 6.0696 - leftLayer1_loss: 0.1177 - midLayer1_loss: 1.3181 - rightLayer1_loss: 1.6774 - leftLayer2_loss: 0.1206 - midLayer2_loss: 1.4099 - rightLayer2_loss: 1.4259\n",
      "Epoch 00008: val_loss improved from 6.16523 to 6.10136, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.0600 - leftLayer1_loss: 0.1177 - midLayer1_loss: 1.3178 - rightLayer1_loss: 1.6755 - leftLayer2_loss: 0.1203 - midLayer2_loss: 1.4055 - rightLayer2_loss: 1.4233 - val_loss: 6.1014 - val_leftLayer1_loss: 0.1181 - val_midLayer1_loss: 1.3273 - val_rightLayer1_loss: 1.6967 - val_leftLayer2_loss: 0.1224 - val_midLayer2_loss: 1.2933 - val_rightLayer2_loss: 1.5435\n",
      "Epoch 9/11\n",
      "131/144 [==========================>...] - ETA: 0s - loss: 6.0040 - leftLayer1_loss: 0.1173 - midLayer1_loss: 1.3266 - rightLayer1_loss: 1.6488 - leftLayer2_loss: 0.1202 - midLayer2_loss: 1.4138 - rightLayer2_loss: 1.3773\n",
      "Epoch 00009: val_loss improved from 6.10136 to 6.03986, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.9944 - leftLayer1_loss: 0.1172 - midLayer1_loss: 1.3239 - rightLayer1_loss: 1.6470 - leftLayer2_loss: 0.1204 - midLayer2_loss: 1.4100 - rightLayer2_loss: 1.3759 - val_loss: 6.0399 - val_leftLayer1_loss: 0.1176 - val_midLayer1_loss: 1.3273 - val_rightLayer1_loss: 1.6693 - val_leftLayer2_loss: 0.1217 - val_midLayer2_loss: 1.2933 - val_rightLayer2_loss: 1.5106\n",
      "Epoch 10/11\n",
      "129/144 [=========================>....] - ETA: 0s - loss: 5.9147 - leftLayer1_loss: 0.1166 - midLayer1_loss: 1.3278 - rightLayer1_loss: 1.6165 - leftLayer2_loss: 0.1206 - midLayer2_loss: 1.3950 - rightLayer2_loss: 1.3383\n",
      "Epoch 00010: val_loss improved from 6.03986 to 5.98086, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.9072 - leftLayer1_loss: 0.1165 - midLayer1_loss: 1.3273 - rightLayer1_loss: 1.6149 - leftLayer2_loss: 0.1203 - midLayer2_loss: 1.3923 - rightLayer2_loss: 1.3359 - val_loss: 5.9809 - val_leftLayer1_loss: 0.1172 - val_midLayer1_loss: 1.3273 - val_rightLayer1_loss: 1.6424 - val_leftLayer2_loss: 0.1209 - val_midLayer2_loss: 1.2933 - val_rightLayer2_loss: 1.4797\n",
      "Epoch 11/11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/144 [===========================>..] - ETA: 0s - loss: 5.8380 - leftLayer1_loss: 0.1161 - midLayer1_loss: 1.3253 - rightLayer1_loss: 1.5878 - leftLayer2_loss: 0.1194 - midLayer2_loss: 1.3966 - rightLayer2_loss: 1.2928\n",
      "Epoch 00011: val_loss improved from 5.98086 to 5.92410, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.8221 - leftLayer1_loss: 0.1161 - midLayer1_loss: 1.3246 - rightLayer1_loss: 1.5860 - leftLayer2_loss: 0.1192 - midLayer2_loss: 1.3905 - rightLayer2_loss: 1.2858 - val_loss: 5.9241 - val_leftLayer1_loss: 0.1167 - val_midLayer1_loss: 1.3273 - val_rightLayer1_loss: 1.6160 - val_leftLayer2_loss: 0.1202 - val_midLayer2_loss: 1.2933 - val_rightLayer2_loss: 1.4506\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.013999999999999995_test.log **\n",
      "auroc 0Hernia: 0.5408157189006637\n",
      "\n",
      "auprc 0Hernia: 0.0018305346754388858\n",
      "\n",
      "auroc 1Hernia: 0.6991334741211924\n",
      "\n",
      "auprc 1Hernia: 0.004885655881880881\n",
      "\n",
      "auroc 2Hernia: 0.7400411730053104\n",
      "\n",
      "auprc 2Hernia: 0.0073343574338880484\n",
      "\n",
      "auroc 3Hernia: 0.4535336263932575\n",
      "\n",
      "auprc 3Hernia: 0.0018091456286201011\n",
      "\n",
      "auroc 4Hernia: 0.798949301483802\n",
      "\n",
      "auprc 4Hernia: 0.0062464399275844115\n",
      "\n",
      "auroc 5Hernia: 0.5160656598846051\n",
      "\n",
      "auprc 5Hernia: 0.0018116857820647757\n",
      "\n",
      "mean auroc: 0.6247564922981385\n",
      "\n",
      "mean auprc: 0.003986303221579517\n",
      "\n",
      "max auroc: 0.798949301483802\n",
      "\n",
      "max auprc: 0.0073343574338880484\n",
      "\n",
      "33.87363314628601\n",
      "** set output weights path to: ./experiments/0.014999999999999994_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 144 steps, validate for 41 steps\n",
      "Epoch 1/11\n",
      "134/144 [==========================>...] - ETA: 0s - loss: 6.7227 - leftLayer1_loss: 0.1220 - midLayer1_loss: 1.3338 - rightLayer1_loss: 1.8881 - leftLayer2_loss: 0.1266 - midLayer2_loss: 1.3842 - rightLayer2_loss: 1.8680\n",
      "Epoch 00001: val_loss improved from inf to 6.62129, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "144/144 [==============================] - 1s 9ms/step - loss: 6.7085 - leftLayer1_loss: 0.1218 - midLayer1_loss: 1.3313 - rightLayer1_loss: 1.8862 - leftLayer2_loss: 0.1265 - midLayer2_loss: 1.3812 - rightLayer2_loss: 1.8615 - val_loss: 6.6213 - val_leftLayer1_loss: 0.1222 - val_midLayer1_loss: 1.3197 - val_rightLayer1_loss: 1.8710 - val_leftLayer2_loss: 0.1236 - val_midLayer2_loss: 1.3554 - val_rightLayer2_loss: 1.8293\n",
      "Epoch 2/11\n",
      "134/144 [==========================>...] - ETA: 0s - loss: 6.6300 - leftLayer1_loss: 0.1215 - midLayer1_loss: 1.3254 - rightLayer1_loss: 1.8553 - leftLayer2_loss: 0.1239 - midLayer2_loss: 1.3966 - rightLayer2_loss: 1.8074\n",
      "Epoch 00002: val_loss improved from 6.62129 to 6.54791, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.6190 - leftLayer1_loss: 0.1212 - midLayer1_loss: 1.3236 - rightLayer1_loss: 1.8533 - leftLayer2_loss: 0.1240 - midLayer2_loss: 1.3934 - rightLayer2_loss: 1.8035 - val_loss: 6.5479 - val_leftLayer1_loss: 0.1217 - val_midLayer1_loss: 1.3197 - val_rightLayer1_loss: 1.8417 - val_leftLayer2_loss: 0.1229 - val_midLayer2_loss: 1.3554 - val_rightLayer2_loss: 1.7866\n",
      "Epoch 3/11\n",
      "123/144 [========================>.....] - ETA: 0s - loss: 6.5322 - leftLayer1_loss: 0.1208 - midLayer1_loss: 1.3295 - rightLayer1_loss: 1.8212 - leftLayer2_loss: 0.1245 - midLayer2_loss: 1.3949 - rightLayer2_loss: 1.7412\n",
      "Epoch 00003: val_loss improved from 6.54791 to 6.47569, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.5160 - leftLayer1_loss: 0.1207 - midLayer1_loss: 1.3280 - rightLayer1_loss: 1.8199 - leftLayer2_loss: 0.1240 - midLayer2_loss: 1.3852 - rightLayer2_loss: 1.7383 - val_loss: 6.4757 - val_leftLayer1_loss: 0.1212 - val_midLayer1_loss: 1.3197 - val_rightLayer1_loss: 1.8125 - val_leftLayer2_loss: 0.1221 - val_midLayer2_loss: 1.3554 - val_rightLayer2_loss: 1.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/11\n",
      "136/144 [===========================>..] - ETA: 0s - loss: 6.4340 - leftLayer1_loss: 0.1199 - midLayer1_loss: 1.3363 - rightLayer1_loss: 1.7899 - leftLayer2_loss: 0.1227 - midLayer2_loss: 1.3918 - rightLayer2_loss: 1.6733\n",
      "Epoch 00004: val_loss improved from 6.47569 to 6.40548, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.4291 - leftLayer1_loss: 0.1198 - midLayer1_loss: 1.3350 - rightLayer1_loss: 1.7882 - leftLayer2_loss: 0.1225 - midLayer2_loss: 1.3932 - rightLayer2_loss: 1.6704 - val_loss: 6.4055 - val_leftLayer1_loss: 0.1207 - val_midLayer1_loss: 1.3197 - val_rightLayer1_loss: 1.7837 - val_leftLayer2_loss: 0.1214 - val_midLayer2_loss: 1.3554 - val_rightLayer2_loss: 1.7047\n",
      "Epoch 5/11\n",
      "135/144 [===========================>..] - ETA: 0s - loss: 6.3443 - leftLayer1_loss: 0.1198 - midLayer1_loss: 1.3305 - rightLayer1_loss: 1.7582 - leftLayer2_loss: 0.1205 - midLayer2_loss: 1.4031 - rightLayer2_loss: 1.6122\n",
      "Epoch 00005: val_loss improved from 6.40548 to 6.33667, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.3296 - leftLayer1_loss: 0.1196 - midLayer1_loss: 1.3276 - rightLayer1_loss: 1.7554 - leftLayer2_loss: 0.1201 - midLayer2_loss: 1.3979 - rightLayer2_loss: 1.6090 - val_loss: 6.3367 - val_leftLayer1_loss: 0.1202 - val_midLayer1_loss: 1.3197 - val_rightLayer1_loss: 1.7551 - val_leftLayer2_loss: 0.1206 - val_midLayer2_loss: 1.3554 - val_rightLayer2_loss: 1.6656\n",
      "Epoch 6/11\n",
      "133/144 [==========================>...] - ETA: 0s - loss: 6.2099 - leftLayer1_loss: 0.1191 - midLayer1_loss: 1.3333 - rightLayer1_loss: 1.7236 - leftLayer2_loss: 0.1195 - midLayer2_loss: 1.3628 - rightLayer2_loss: 1.5516\n",
      "Epoch 00006: val_loss improved from 6.33667 to 6.27008, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.2028 - leftLayer1_loss: 0.1190 - midLayer1_loss: 1.3328 - rightLayer1_loss: 1.7223 - leftLayer2_loss: 0.1194 - midLayer2_loss: 1.3633 - rightLayer2_loss: 1.5460 - val_loss: 6.2701 - val_leftLayer1_loss: 0.1197 - val_midLayer1_loss: 1.3197 - val_rightLayer1_loss: 1.7271 - val_leftLayer2_loss: 0.1199 - val_midLayer2_loss: 1.3554 - val_rightLayer2_loss: 1.6283\n",
      "Epoch 7/11\n",
      "127/144 [=========================>....] - ETA: 0s - loss: 6.1494 - leftLayer1_loss: 0.1184 - midLayer1_loss: 1.3368 - rightLayer1_loss: 1.6928 - leftLayer2_loss: 0.1179 - midLayer2_loss: 1.3919 - rightLayer2_loss: 1.4916\n",
      "Epoch 00007: val_loss improved from 6.27008 to 6.20487, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.1389 - leftLayer1_loss: 0.1184 - midLayer1_loss: 1.3346 - rightLayer1_loss: 1.6906 - leftLayer2_loss: 0.1177 - midLayer2_loss: 1.3947 - rightLayer2_loss: 1.4828 - val_loss: 6.2049 - val_leftLayer1_loss: 0.1192 - val_midLayer1_loss: 1.3197 - val_rightLayer1_loss: 1.6995 - val_leftLayer2_loss: 0.1192 - val_midLayer2_loss: 1.3554 - val_rightLayer2_loss: 1.5919\n",
      "Epoch 8/11\n",
      "134/144 [==========================>...] - ETA: 0s - loss: 6.0709 - leftLayer1_loss: 0.1181 - midLayer1_loss: 1.3342 - rightLayer1_loss: 1.6639 - leftLayer2_loss: 0.1177 - midLayer2_loss: 1.4050 - rightLayer2_loss: 1.4319\n",
      "Epoch 00008: val_loss improved from 6.20487 to 6.14210, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 6.0518 - leftLayer1_loss: 0.1178 - midLayer1_loss: 1.3313 - rightLayer1_loss: 1.6606 - leftLayer2_loss: 0.1178 - midLayer2_loss: 1.3969 - rightLayer2_loss: 1.4274 - val_loss: 6.1421 - val_leftLayer1_loss: 0.1187 - val_midLayer1_loss: 1.3197 - val_rightLayer1_loss: 1.6723 - val_leftLayer2_loss: 0.1185 - val_midLayer2_loss: 1.3554 - val_rightLayer2_loss: 1.5575\n",
      "Epoch 9/11\n",
      "134/144 [==========================>...] - ETA: 0s - loss: 5.9958 - leftLayer1_loss: 0.1174 - midLayer1_loss: 1.3301 - rightLayer1_loss: 1.6327 - leftLayer2_loss: 0.1159 - midLayer2_loss: 1.4166 - rightLayer2_loss: 1.3831\n",
      "Epoch 00009: val_loss improved from 6.14210 to 6.08204, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.9773 - leftLayer1_loss: 0.1172 - midLayer1_loss: 1.3277 - rightLayer1_loss: 1.6291 - leftLayer2_loss: 0.1159 - midLayer2_loss: 1.4115 - rightLayer2_loss: 1.3759 - val_loss: 6.0820 - val_leftLayer1_loss: 0.1182 - val_midLayer1_loss: 1.3197 - val_rightLayer1_loss: 1.6456 - val_leftLayer2_loss: 0.1178 - val_midLayer2_loss: 1.3554 - val_rightLayer2_loss: 1.5253\n",
      "Epoch 10/11\n",
      "136/144 [===========================>..] - ETA: 0s - loss: 5.8721 - leftLayer1_loss: 0.1168 - midLayer1_loss: 1.3289 - rightLayer1_loss: 1.6049 - leftLayer2_loss: 0.1164 - midLayer2_loss: 1.3821 - rightLayer2_loss: 1.3230\n",
      "Epoch 00010: val_loss improved from 6.08204 to 6.02388, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.8625 - leftLayer1_loss: 0.1165 - midLayer1_loss: 1.3290 - rightLayer1_loss: 1.6025 - leftLayer2_loss: 0.1164 - midLayer2_loss: 1.3804 - rightLayer2_loss: 1.3176 - val_loss: 6.0239 - val_leftLayer1_loss: 0.1178 - val_midLayer1_loss: 1.3197 - val_rightLayer1_loss: 1.6196 - val_leftLayer2_loss: 0.1170 - val_midLayer2_loss: 1.3554 - val_rightLayer2_loss: 1.4944\n",
      "Epoch 11/11\n",
      "133/144 [==========================>...] - ETA: 0s - loss: 5.8427 - leftLayer1_loss: 0.1165 - midLayer1_loss: 1.3315 - rightLayer1_loss: 1.5776 - leftLayer2_loss: 0.1141 - midLayer2_loss: 1.4112 - rightLayer2_loss: 1.2918\n",
      "Epoch 00011: val_loss improved from 6.02388 to 5.96786, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 5.8318 - leftLayer1_loss: 0.1163 - midLayer1_loss: 1.3302 - rightLayer1_loss: 1.5738 - leftLayer2_loss: 0.1139 - midLayer2_loss: 1.4123 - rightLayer2_loss: 1.2854 - val_loss: 5.9679 - val_leftLayer1_loss: 0.1173 - val_midLayer1_loss: 1.3197 - val_rightLayer1_loss: 1.5942 - val_leftLayer2_loss: 0.1163 - val_midLayer2_loss: 1.3554 - val_rightLayer2_loss: 1.4650\n",
      "22433/22433 [==============================] - 29s 1ms/step\n",
      "** write log to ./experiments/0.014999999999999994_test.log **\n",
      "auroc 0Hernia: 0.8256001029325133\n",
      "\n",
      "auprc 0Hernia: 0.02388263766225233\n",
      "\n",
      "auroc 1Hernia: 0.5997222523505406\n",
      "\n",
      "auprc 1Hernia: 0.0031875820386083856\n",
      "\n",
      "auroc 2Hernia: 0.7257502482927877\n",
      "\n",
      "auprc 2Hernia: 0.004235402416938516\n",
      "\n",
      "auroc 3Hernia: 0.8500370046638636\n",
      "\n",
      "auprc 3Hernia: 0.012579400293992498\n",
      "\n",
      "auroc 4Hernia: 0.1797847136710966\n",
      "\n",
      "auprc 4Hernia: 0.0010346089672995324\n",
      "\n",
      "auroc 5Hernia: 0.6240634523650022\n",
      "\n",
      "auprc 5Hernia: 0.010562078652191784\n",
      "\n",
      "mean auroc: 0.6341596290459672\n",
      "\n",
      "mean auprc: 0.009246951671880508\n",
      "\n",
      "max auroc: 0.8500370046638636\n",
      "\n",
      "max auprc: 0.02388263766225233\n",
      "\n",
      "34.64024639129639\n"
     ]
    }
   ],
   "source": [
    "step = np.arange(0.009, 0.0151, 0.001)\n",
    "maxi = []\n",
    "for k in np.nditer(step):\n",
    "    opn, daTime = optimize_network(k)\n",
    "    print(daTime)\n",
    "    maxi.append(opn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8500370046638636\n"
     ]
    }
   ],
   "source": [
    "print(np.max(maxi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
