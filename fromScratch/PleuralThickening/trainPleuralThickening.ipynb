{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import pickle\n",
    "from callback import MultipleClassAUROC, MultiGPUModelCheckpoint\n",
    "from configparser import ConfigParser\n",
    "from generator import AugmentedImageSequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from utility import get_sample_counts\n",
    "from weights import get_class_weights\n",
    "from augmenter import augmenter\n",
    "from tensorflow.keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer, Flatten, Input, GaussianNoise\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_radam import RAdam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#print(\"TensorFlow version: \", tf.__version__)\n",
    "#assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "#    \"This notebook requires TensorFlow 2.0 or above.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "# UPDATED: import from tensorflow.keras instead of keras\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics\n",
    "import gc\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneClass = \"PleuralThickening\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"./config.ini\"\n",
    "cp = ConfigParser()\n",
    "cp.read(config_file)\n",
    "\n",
    "    # default config\n",
    "output_dir = cp[\"DEFAULT\"].get(\"output_dir\")\n",
    "image_source_dir = cp[\"DEFAULT\"].get(\"image_source_dir\")\n",
    "base_model_name = cp[\"DEFAULT\"].get(\"base_model_name\")\n",
    "class_names = cp[\"DEFAULT\"].get(\"class_names\").split(\",\")\n",
    "\n",
    "    # train config\n",
    "use_base_model_weights = cp[\"TRAIN\"].getboolean(\"use_base_model_weights\")\n",
    "use_trained_model_weights = cp[\"TRAIN\"].getboolean(\"use_trained_model_weights\")\n",
    "use_best_weights = cp[\"TRAIN\"].getboolean(\"use_best_weights\")\n",
    "output_weights_name = cp[\"TRAIN\"].get(\"output_weights_name\")\n",
    "epochs = cp[\"TRAIN\"].getint(\"epochs\")\n",
    "batch_size = cp[\"TRAIN\"].getint(\"batch_size\")\n",
    "initial_learning_rate = cp[\"TRAIN\"].getfloat(\"initial_learning_rate\")\n",
    "generator_workers = cp[\"TRAIN\"].getint(\"generator_workers\")\n",
    "image_dimension = cp[\"TRAIN\"].getint(\"image_dimension\")\n",
    "train_steps = cp[\"TRAIN\"].get(\"train_steps\")\n",
    "patience_reduce_lr = cp[\"TRAIN\"].getint(\"patience_reduce_lr\")\n",
    "min_lr = cp[\"TRAIN\"].getfloat(\"min_lr\")\n",
    "validation_steps = cp[\"TRAIN\"].get(\"validation_steps\")\n",
    "positive_weights_multiply = cp[\"TRAIN\"].getfloat(\"positive_weights_multiply\")\n",
    "dataset_csv_dir = cp[\"TRAIN\"].get(\"dataset_csv_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=1.0, alpha=0.5):\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(tf.cast(y_pred,tf.float32) - tf.cast(y_true,tf.float32)), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network1(dropout=0.08425517073874295, neuronPct=0.1767547775828121, neuronShrink=0.33180474398878285):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "    # Construct neural network\n",
    "    neuronCount = neuronCount * neuronShrink\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,1536)))\n",
    "    model.add(Flatten(name='flat1'))\n",
    "    model.add(Dense(neuronCount,name='dense1'))\n",
    "    model.add(Activation('relu',name='relu1'))\n",
    "    model.add(Dropout(dropout, name='dropout1'))\n",
    "    model.add(Dense(14, activation='sigmoid',name='midLayer1')) # Output\n",
    "    weights_path=None\n",
    "    if weights_path is not None:\n",
    "        print(f\"load model weights_path: {weights_path}\")\n",
    "        model.load_weights(weights_path)\n",
    "    model.layers.pop()\n",
    "    dr = model.layers[-2].output\n",
    "    model.trainable = False\n",
    "    left = Dense(14, activation=\"sigmoid\", name='leftLayer1')(dr)\n",
    "    right = Dense(14, activation=\"sigmoid\", name='rightLayer1')(dr)\n",
    "    model = Model(model.input, [left,model.output,right])\n",
    "    #model = Model(model.input, model.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network2(dropout=0.15672137551441198, neuronPct=0.2197894476507525, neuronShrink=0.3803316528497302, noisePct=0.282563134185142):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "    # Construct neural network\n",
    "    neuronCount = neuronCount * neuronShrink\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,1536)))\n",
    "    model.add(Flatten(name='flat2'))\n",
    "    model.add(Dense(neuronCount,name='dense2'))\n",
    "    model.add(GaussianNoise(noisePct))\n",
    "    model.add(Activation('relu',name='relu2'))\n",
    "    model.add(Dropout(dropout, name='dropout2'))\n",
    "    model.add(Dense(14, activation='sigmoid',name='midLayer2')) # Output\n",
    "    weights_path=None\n",
    "    if weights_path is not None:\n",
    "        print(f\"load model weights_path: {weights_path}\")\n",
    "        model.load_weights(weights_path)\n",
    "    #model.layers.pop()\n",
    "    dr = model.layers[-2].output\n",
    "    model.trainable = False\n",
    "    left = Dense(14, activation=\"sigmoid\", name='leftLayer2')(dr)\n",
    "    right = Dense(14, activation=\"sigmoid\", name='rightLayer2')(dr)\n",
    "    model = Model(model.input, [left,model.output,right])\n",
    "    #model = Model(model.input, model.output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network(model1,model2):\n",
    "    model = Model([model1.input,model2.input], [model1.output,model2.output])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** compute class weights from training data **\n",
      "323: 2279\n",
      "85: 2279\n",
      "594: 2279\n",
      "511: 2279\n",
      "306: 2279\n",
      "274: 2279\n",
      "34: 2279\n",
      "192: 2279\n",
      "171: 2279\n",
      "41: 2279\n",
      "107: 2279\n",
      "113: 2279\n",
      "2279: 2279\n",
      "6: 2279\n",
      "** class_weights **\n",
      "[{0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1}]\n"
     ]
    }
   ],
   "source": [
    "# compute steps\n",
    "train_counts, train_pos_counts = get_sample_counts(output_dir, \"train\"+oneClass, class_names)\n",
    "dev_counts, _ = get_sample_counts(output_dir, \"dev\"+oneClass, class_names)\n",
    "    \n",
    "if train_steps == \"auto\":\n",
    "    train_steps = int(train_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        train_steps = int(train_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"train_steps: {train_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "    print(f\"** train_steps: {train_steps} **\")\n",
    "\n",
    "if validation_steps == \"auto\":\n",
    "    validation_steps = int(dev_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        validation_steps = int(validation_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"validation_steps: {validation_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        print(f\"** validation_steps: {validation_steps} **\")\n",
    "\n",
    "        # compute class weights\n",
    "keras.backend.clear_session()\n",
    "print(\"** compute class weights from training data **\")\n",
    "class_weights = get_class_weights(train_counts,train_pos_counts,multiply=positive_weights_multiply,)\n",
    "print(\"** class_weights **\")\n",
    "print(class_weights)\n",
    "#print(str(train_steps))\n",
    "#print(str(train_counts))\n",
    "#print(str(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** test_steps: 22433 **\n"
     ]
    }
   ],
   "source": [
    "test_steps = cp[\"TEST\"].get(\"test_steps\")\n",
    "test_counts, _ = get_sample_counts(output_dir, \"test\", class_names)\n",
    "\n",
    "if test_steps == \"auto\":\n",
    "    test_steps = int(test_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        test_steps = int(test_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"test_steps: {test_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        \n",
    "print(f\"** test_steps: {test_steps} **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"train\"+oneClass+\".csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=train_steps,\n",
    "        )\n",
    "validation_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"dev\"+oneClass+\".csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=validation_steps,\n",
    "            shuffle_on_epoch_end=False,\n",
    ")\n",
    "\n",
    "test_sequence = AugmentedImageSequence(\n",
    "        dataset_csv_file=os.path.join(output_dir, \"test.csv\"),\n",
    "        class_names=class_names,\n",
    "        source_image_dir=image_source_dir,\n",
    "        batch_size=batch_size,\n",
    "        target_size=(image_dimension, image_dimension),\n",
    "        augmenter=None,\n",
    "        steps=test_steps,\n",
    "        shuffle_on_epoch_end=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(lr):\n",
    "    gc.collect()\n",
    "      # Define the Keras TensorBoard callback.\n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    model1 = construct_network1()\n",
    "    model2 = construct_network2()\n",
    "    \n",
    "    optimizer = SGD(lr=initial_learning_rate)\n",
    "    \n",
    "    alpha = 0.9340456763831478\n",
    "    gamma = 1.4195808780694898\n",
    "    model1.compile(optimizer=optimizer,loss={'leftLayer1':tf.keras.losses.Huber(),'midLayer1':focal_loss(gamma=gamma,alpha=alpha),'rightLayer1':euclidean_distance_loss})\n",
    "\n",
    "    alpha = 0.7297456293468533\n",
    "    gamma = 1.2700405014991505\n",
    "    model2.compile(optimizer=optimizer,loss={'leftLayer2':tf.keras.losses.Huber(),'midLayer2':focal_loss(gamma=gamma,alpha=alpha),'rightLayer2':euclidean_distance_loss})\n",
    "  \n",
    "    model = construct_network(model1=model1,model2=model2)\n",
    "    model.compile(optimizer=optimizer,loss={'leftLayer1':tf.keras.losses.Huber(),'midLayer1':focal_loss(gamma=gamma,alpha=alpha),'rightLayer1':euclidean_distance_loss,'leftLayer2':tf.keras.losses.Huber(),'midLayer2':focal_loss(gamma=gamma,alpha=alpha),'rightLayer2':euclidean_distance_loss})\n",
    "\n",
    "    output_weights_path = os.path.join(output_dir,  str(lr)+\"_\"+output_weights_name)\n",
    "    \n",
    "    print(f\"** set output weights path to: {output_weights_path} **\")\n",
    "                  \n",
    "    \n",
    "                  \n",
    "    checkpoint = ModelCheckpoint(\n",
    "                 output_weights_path,\n",
    "                 save_weights_only=True,\n",
    "                 save_best_only=True,\n",
    "                 verbose=1,\n",
    "            )\n",
    "    start_time = time.time()\n",
    "  \n",
    "    model.summary()\n",
    "  \n",
    "    callbacks = [\n",
    "            checkpoint,\n",
    "            #keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "            #TensorBoard(log_dir=os.path.join(output_dir, \"logs\"), batch_size=batch_size),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience_reduce_lr,\n",
    "                              verbose=1, mode=\"min\", min_lr=min_lr), \n",
    "            EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    history = model.fit_generator(\n",
    "            generator=train_sequence,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_sequence,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=[class_weights,class_weights,class_weights,class_weights,class_weights,class_weights],\n",
    "            workers=generator_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        \n",
    "    y_hat = model.predict_generator(test_sequence, verbose=1)\n",
    "    y = test_sequence.get_y_true()\n",
    "    \n",
    "    test_log_path = os.path.join(output_dir, str(lr)+\"_\"+\"test.log\")\n",
    "    print(f\"** write log to {test_log_path} **\")\n",
    "    aurocs = []\n",
    "    auprcs = []\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    threshold = dict()\n",
    "    with open(test_log_path, \"w\") as f:\n",
    "        for k in range(6):\n",
    "            for i in range(len(class_names)):\n",
    "                 if(class_names[i] == str(oneClass)):\n",
    "                \n",
    "                    try:\n",
    "                        score = roc_auc_score(y[:, i], y_hat[k][:, i])\n",
    "                        precision[i], recall[i], threshold[i] = precision_recall_curve(y[:, i], y_hat[k][:, i])\n",
    "                        tmp = auc(recall[i], precision[i])\n",
    "                        aurocs.append(score)\n",
    "                        auprcs.append(tmp) \n",
    "                    except ValueError:\n",
    "                        score = 0\n",
    "               \n",
    "                    print(f\"auroc {str(k)+class_names[i]}: {score}\\n\")\n",
    "                    print(f\"auprc {str(k)+class_names[i]}: {tmp}\\n\")\n",
    "                    f.write(f\"auroc {str(k)+class_names[i]}: {score}\\n\")\n",
    "                    f.write(f\"auprc {str(k)+class_names[i]}: {tmp}\\n\")\n",
    "        \n",
    "        mean_auroc = np.mean(aurocs)\n",
    "        mean_auprc = float(np.mean(auprcs))\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(f\"mean auroc: {mean_auroc}\\n\")\n",
    "        print(f\"mean auroc: {mean_auroc}\\n\")\n",
    "        f.write(f\"mean auprc: {mean_auprc}\\n\")\n",
    "        print(f\"mean auprc: {mean_auprc}\\n\")\n",
    "        \n",
    "        max_auroc = np.max(aurocs)\n",
    "        max_auprc = float(np.max(auprcs))\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(f\"max auroc: {max_auroc}\\n\")\n",
    "        print(f\"max auroc: {max_auroc}\\n\")\n",
    "        f.write(f\"max auprc: {max_auprc}\\n\")\n",
    "        print(f\"max auprc: {max_auprc}\\n\")\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "    time_took = time.time() - start_time\n",
    "    \n",
    "    return max_auroc, time_took\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** set output weights path to: ./experiments/0.009_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-15-3539473a5eed>:58: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 2279 steps, validate for 372 steps\n",
      "Epoch 1/11\n",
      "2260/2279 [============================>.] - ETA: 0s - loss: 6.3970 - leftLayer1_loss: 0.1208 - midLayer1_loss: 1.4270 - rightLayer1_loss: 1.7094 - leftLayer2_loss: 0.1177 - midLayer2_loss: 1.4670 - rightLayer2_loss: 1.5551\n",
      "Epoch 00001: val_loss improved from inf to 6.03241, saving model to ./experiments/0.009_weights.h5\n",
      "2279/2279 [==============================] - 7s 3ms/step - loss: 6.3951 - leftLayer1_loss: 0.1207 - midLayer1_loss: 1.4274 - rightLayer1_loss: 1.7084 - leftLayer2_loss: 0.1176 - midLayer2_loss: 1.4678 - rightLayer2_loss: 1.5532 - val_loss: 6.0324 - val_leftLayer1_loss: 0.1186 - val_midLayer1_loss: 1.4083 - val_rightLayer1_loss: 1.5673 - val_leftLayer2_loss: 0.1164 - val_midLayer2_loss: 1.3821 - val_rightLayer2_loss: 1.4397\n",
      "Epoch 2/11\n",
      "2271/2279 [============================>.] - ETA: 0s - loss: 5.7306 - leftLayer1_loss: 0.1156 - midLayer1_loss: 1.4278 - rightLayer1_loss: 1.4629 - leftLayer2_loss: 0.1043 - midLayer2_loss: 1.4725 - rightLayer2_loss: 1.1476\n",
      "Epoch 00002: val_loss improved from 6.03241 to 5.61409, saving model to ./experiments/0.009_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.7308 - leftLayer1_loss: 0.1156 - midLayer1_loss: 1.4279 - rightLayer1_loss: 1.4626 - leftLayer2_loss: 0.1043 - midLayer2_loss: 1.4729 - rightLayer2_loss: 1.1475 - val_loss: 5.6141 - val_leftLayer1_loss: 0.1135 - val_midLayer1_loss: 1.4083 - val_rightLayer1_loss: 1.3583 - val_leftLayer2_loss: 0.1083 - val_midLayer2_loss: 1.3821 - val_rightLayer2_loss: 1.2436\n",
      "Epoch 3/11\n",
      "2268/2279 [============================>.] - ETA: 0s - loss: 5.4134 - leftLayer1_loss: 0.1108 - midLayer1_loss: 1.4277 - rightLayer1_loss: 1.2921 - leftLayer2_loss: 0.0936 - midLayer2_loss: 1.4712 - rightLayer2_loss: 1.0180\n",
      "Epoch 00003: val_loss improved from 5.61409 to 5.36945, saving model to ./experiments/0.009_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.4144 - leftLayer1_loss: 0.1108 - midLayer1_loss: 1.4280 - rightLayer1_loss: 1.2920 - leftLayer2_loss: 0.0936 - midLayer2_loss: 1.4715 - rightLayer2_loss: 1.0185 - val_loss: 5.3695 - val_leftLayer1_loss: 0.1088 - val_midLayer1_loss: 1.4083 - val_rightLayer1_loss: 1.2191 - val_leftLayer2_loss: 0.1014 - val_midLayer2_loss: 1.3821 - val_rightLayer2_loss: 1.1498\n",
      "Epoch 4/11\n",
      "2258/2279 [============================>.] - ETA: 0s - loss: 5.2406 - leftLayer1_loss: 0.1063 - midLayer1_loss: 1.4265 - rightLayer1_loss: 1.1806 - leftLayer2_loss: 0.0849 - midLayer2_loss: 1.4771 - rightLayer2_loss: 0.9651\n",
      "Epoch 00004: val_loss improved from 5.36945 to 5.21397, saving model to ./experiments/0.009_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.2436 - leftLayer1_loss: 0.1063 - midLayer1_loss: 1.4269 - rightLayer1_loss: 1.1809 - leftLayer2_loss: 0.0849 - midLayer2_loss: 1.4784 - rightLayer2_loss: 0.9663 - val_loss: 5.2140 - val_leftLayer1_loss: 0.1044 - val_midLayer1_loss: 1.4083 - val_rightLayer1_loss: 1.1281 - val_leftLayer2_loss: 0.0955 - val_midLayer2_loss: 1.3821 - val_rightLayer2_loss: 1.0956\n",
      "Epoch 5/11\n",
      "2266/2279 [============================>.] - ETA: 0s - loss: 5.1349 - leftLayer1_loss: 0.1020 - midLayer1_loss: 1.4275 - rightLayer1_loss: 1.1071 - leftLayer2_loss: 0.0774 - midLayer2_loss: 1.4817 - rightLayer2_loss: 0.9390\n",
      "Epoch 00005: val_loss improved from 5.21397 to 5.10842, saving model to ./experiments/0.009_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.1361 - leftLayer1_loss: 0.1020 - midLayer1_loss: 1.4278 - rightLayer1_loss: 1.1073 - leftLayer2_loss: 0.0774 - midLayer2_loss: 1.4821 - rightLayer2_loss: 0.9395 - val_loss: 5.1084 - val_leftLayer1_loss: 0.1003 - val_midLayer1_loss: 1.4083 - val_rightLayer1_loss: 1.0674 - val_leftLayer2_loss: 0.0905 - val_midLayer2_loss: 1.3821 - val_rightLayer2_loss: 1.0598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/11\n",
      "2275/2279 [============================>.] - ETA: 0s - loss: 5.0559 - leftLayer1_loss: 0.0983 - midLayer1_loss: 1.4271 - rightLayer1_loss: 1.0577 - leftLayer2_loss: 0.0720 - midLayer2_loss: 1.4782 - rightLayer2_loss: 0.9226\n",
      "Epoch 00006: val_loss improved from 5.10842 to 5.03298, saving model to ./experiments/0.009_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0561 - leftLayer1_loss: 0.0983 - midLayer1_loss: 1.4270 - rightLayer1_loss: 1.0578 - leftLayer2_loss: 0.0720 - midLayer2_loss: 1.4782 - rightLayer2_loss: 0.9227 - val_loss: 5.0330 - val_leftLayer1_loss: 0.0964 - val_midLayer1_loss: 1.4083 - val_rightLayer1_loss: 1.0253 - val_leftLayer2_loss: 0.0863 - val_midLayer2_loss: 1.3821 - val_rightLayer2_loss: 1.0345\n",
      "Epoch 7/11\n",
      "2260/2279 [============================>.] - ETA: 0s - loss: 4.9975 - leftLayer1_loss: 0.0945 - midLayer1_loss: 1.4273 - rightLayer1_loss: 1.0214 - leftLayer2_loss: 0.0674 - midLayer2_loss: 1.4761 - rightLayer2_loss: 0.9108\n",
      "Epoch 00007: val_loss improved from 5.03298 to 4.97645, saving model to ./experiments/0.009_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0015 - leftLayer1_loss: 0.0945 - midLayer1_loss: 1.4278 - rightLayer1_loss: 1.0224 - leftLayer2_loss: 0.0674 - midLayer2_loss: 1.4772 - rightLayer2_loss: 0.9121 - val_loss: 4.9765 - val_leftLayer1_loss: 0.0929 - val_midLayer1_loss: 1.4083 - val_rightLayer1_loss: 0.9951 - val_leftLayer2_loss: 0.0826 - val_midLayer2_loss: 1.3821 - val_rightLayer2_loss: 1.0154\n",
      "Epoch 8/11\n",
      "2266/2279 [============================>.] - ETA: 0s - loss: 4.9549 - leftLayer1_loss: 0.0911 - midLayer1_loss: 1.4269 - rightLayer1_loss: 0.9960 - leftLayer2_loss: 0.0637 - midLayer2_loss: 1.4721 - rightLayer2_loss: 0.9051\n",
      "Epoch 00008: val_loss improved from 4.97645 to 4.93252, saving model to ./experiments/0.009_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9567 - leftLayer1_loss: 0.0911 - midLayer1_loss: 1.4272 - rightLayer1_loss: 0.9964 - leftLayer2_loss: 0.0637 - midLayer2_loss: 1.4728 - rightLayer2_loss: 0.9056 - val_loss: 4.9325 - val_leftLayer1_loss: 0.0896 - val_midLayer1_loss: 1.4083 - val_rightLayer1_loss: 0.9727 - val_leftLayer2_loss: 0.0794 - val_midLayer2_loss: 1.3821 - val_rightLayer2_loss: 1.0003\n",
      "Epoch 9/11\n",
      "2267/2279 [============================>.] - ETA: 0s - loss: 4.9340 - leftLayer1_loss: 0.0880 - midLayer1_loss: 1.4288 - rightLayer1_loss: 0.9771 - leftLayer2_loss: 0.0607 - midLayer2_loss: 1.4784 - rightLayer2_loss: 0.9011\n",
      "Epoch 00009: val_loss improved from 4.93252 to 4.89715, saving model to ./experiments/0.009_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9347 - leftLayer1_loss: 0.0880 - midLayer1_loss: 1.4290 - rightLayer1_loss: 0.9774 - leftLayer2_loss: 0.0607 - midLayer2_loss: 1.4781 - rightLayer2_loss: 0.9015 - val_loss: 4.8971 - val_leftLayer1_loss: 0.0865 - val_midLayer1_loss: 1.4083 - val_rightLayer1_loss: 0.9555 - val_leftLayer2_loss: 0.0767 - val_midLayer2_loss: 1.3821 - val_rightLayer2_loss: 0.9880\n",
      "Epoch 10/11\n",
      "2271/2279 [============================>.] - ETA: 0s - loss: 4.9059 - leftLayer1_loss: 0.0851 - midLayer1_loss: 1.4278 - rightLayer1_loss: 0.9620 - leftLayer2_loss: 0.0578 - midLayer2_loss: 1.4792 - rightLayer2_loss: 0.8939\n",
      "Epoch 00010: val_loss improved from 4.89715 to 4.86841, saving model to ./experiments/0.009_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9069 - leftLayer1_loss: 0.0851 - midLayer1_loss: 1.4279 - rightLayer1_loss: 0.9623 - leftLayer2_loss: 0.0578 - midLayer2_loss: 1.4795 - rightLayer2_loss: 0.8943 - val_loss: 4.8684 - val_leftLayer1_loss: 0.0837 - val_midLayer1_loss: 1.4083 - val_rightLayer1_loss: 0.9421 - val_leftLayer2_loss: 0.0742 - val_midLayer2_loss: 1.3821 - val_rightLayer2_loss: 0.9780\n",
      "Epoch 11/11\n",
      "2273/2279 [============================>.] - ETA: 0s - loss: 4.8843 - leftLayer1_loss: 0.0824 - midLayer1_loss: 1.4288 - rightLayer1_loss: 0.9504 - leftLayer2_loss: 0.0554 - midLayer2_loss: 1.4744 - rightLayer2_loss: 0.8930\n",
      "Epoch 00011: val_loss improved from 4.86841 to 4.84419, saving model to ./experiments/0.009_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.8848 - leftLayer1_loss: 0.0824 - midLayer1_loss: 1.4288 - rightLayer1_loss: 0.9505 - leftLayer2_loss: 0.0554 - midLayer2_loss: 1.4747 - rightLayer2_loss: 0.8930 - val_loss: 4.8442 - val_leftLayer1_loss: 0.0811 - val_midLayer1_loss: 1.4083 - val_rightLayer1_loss: 0.9312 - val_leftLayer2_loss: 0.0721 - val_midLayer2_loss: 1.3821 - val_rightLayer2_loss: 0.9694\n",
      "WARNING:tensorflow:From <ipython-input-15-3539473a5eed>:61: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.009_test.log **\n",
      "auroc 0PleuralThickening: 0.3762389758415015\n",
      "\n",
      "auprc 0PleuralThickening: 0.023587379786090584\n",
      "\n",
      "auroc 1PleuralThickening: 0.5839152044701768\n",
      "\n",
      "auprc 1PleuralThickening: 0.04978847066528243\n",
      "\n",
      "auroc 2PleuralThickening: 0.36203331486163237\n",
      "\n",
      "auprc 2PleuralThickening: 0.02297813125264567\n",
      "\n",
      "auroc 3PleuralThickening: 0.3814592719085863\n",
      "\n",
      "auprc 3PleuralThickening: 0.02374252503988093\n",
      "\n",
      "auroc 4PleuralThickening: 0.4516952463184368\n",
      "\n",
      "auprc 4PleuralThickening: 0.026840806891191067\n",
      "\n",
      "auroc 5PleuralThickening: 0.45106176492267946\n",
      "\n",
      "auprc 5PleuralThickening: 0.02744817264529005\n",
      "\n",
      "mean auroc: 0.43440062972050225\n",
      "\n",
      "mean auprc: 0.02906424771339679\n",
      "\n",
      "max auroc: 0.5839152044701768\n",
      "\n",
      "max auprc: 0.04978847066528243\n",
      "\n",
      "96.01089644432068\n",
      "** set output weights path to: ./experiments/0.009999999999999998_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 2279 steps, validate for 372 steps\n",
      "Epoch 1/11\n",
      "2270/2279 [============================>.] - ETA: 0s - loss: 6.3518 - leftLayer1_loss: 0.1241 - midLayer1_loss: 1.3087 - rightLayer1_loss: 1.7112 - leftLayer2_loss: 0.1140 - midLayer2_loss: 1.5538 - rightLayer2_loss: 1.5401\n",
      "Epoch 00001: val_loss improved from inf to 5.97891, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 7s 3ms/step - loss: 6.3506 - leftLayer1_loss: 0.1240 - midLayer1_loss: 1.3086 - rightLayer1_loss: 1.7106 - leftLayer2_loss: 0.1140 - midLayer2_loss: 1.5543 - rightLayer2_loss: 1.5391 - val_loss: 5.9789 - val_leftLayer1_loss: 0.1213 - val_midLayer1_loss: 1.3016 - val_rightLayer1_loss: 1.5755 - val_leftLayer2_loss: 0.1130 - val_midLayer2_loss: 1.3999 - val_rightLayer2_loss: 1.4676\n",
      "Epoch 2/11\n",
      "2262/2279 [============================>.] - ETA: 0s - loss: 5.7026 - leftLayer1_loss: 0.1187 - midLayer1_loss: 1.3068 - rightLayer1_loss: 1.4660 - leftLayer2_loss: 0.1027 - midLayer2_loss: 1.5470 - rightLayer2_loss: 1.1613\n",
      "Epoch 00002: val_loss improved from 5.97891 to 5.57356, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.7030 - leftLayer1_loss: 0.1187 - midLayer1_loss: 1.3071 - rightLayer1_loss: 1.4655 - leftLayer2_loss: 0.1026 - midLayer2_loss: 1.5478 - rightLayer2_loss: 1.1613 - val_loss: 5.5736 - val_leftLayer1_loss: 0.1161 - val_midLayer1_loss: 1.3016 - val_rightLayer1_loss: 1.3660 - val_leftLayer2_loss: 0.1062 - val_midLayer2_loss: 1.3999 - val_rightLayer2_loss: 1.2838\n",
      "Epoch 3/11\n",
      "2270/2279 [============================>.] - ETA: 0s - loss: 5.3865 - leftLayer1_loss: 0.1137 - midLayer1_loss: 1.3071 - rightLayer1_loss: 1.2938 - leftLayer2_loss: 0.0926 - midLayer2_loss: 1.5490 - rightLayer2_loss: 1.0303\n",
      "Epoch 00003: val_loss improved from 5.57356 to 5.32950, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.3861 - leftLayer1_loss: 0.1137 - midLayer1_loss: 1.3071 - rightLayer1_loss: 1.2936 - leftLayer2_loss: 0.0926 - midLayer2_loss: 1.5489 - rightLayer2_loss: 1.0303 - val_loss: 5.3295 - val_leftLayer1_loss: 0.1112 - val_midLayer1_loss: 1.3016 - val_rightLayer1_loss: 1.2258 - val_leftLayer2_loss: 0.1003 - val_midLayer2_loss: 1.3999 - val_rightLayer2_loss: 1.1907\n",
      "Epoch 4/11\n",
      "2259/2279 [============================>.] - ETA: 0s - loss: 5.2146 - leftLayer1_loss: 0.1090 - midLayer1_loss: 1.3078 - rightLayer1_loss: 1.1813 - leftLayer2_loss: 0.0848 - midLayer2_loss: 1.5551 - rightLayer2_loss: 0.9766\n",
      "Epoch 00004: val_loss improved from 5.32950 to 5.17182, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.2171 - leftLayer1_loss: 0.1090 - midLayer1_loss: 1.3082 - rightLayer1_loss: 1.1815 - leftLayer2_loss: 0.0848 - midLayer2_loss: 1.5560 - rightLayer2_loss: 0.9776 - val_loss: 5.1718 - val_leftLayer1_loss: 0.1067 - val_midLayer1_loss: 1.3016 - val_rightLayer1_loss: 1.1339 - val_leftLayer2_loss: 0.0952 - val_midLayer2_loss: 1.3999 - val_rightLayer2_loss: 1.1345\n",
      "Epoch 5/11\n",
      "2278/2279 [============================>.] - ETA: 0s - loss: 5.0993 - leftLayer1_loss: 0.1046 - midLayer1_loss: 1.3079 - rightLayer1_loss: 1.1076 - leftLayer2_loss: 0.0782 - midLayer2_loss: 1.5527 - rightLayer2_loss: 0.9483\n",
      "Epoch 00005: val_loss improved from 5.17182 to 5.06438, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0993 - leftLayer1_loss: 0.1046 - midLayer1_loss: 1.3080 - rightLayer1_loss: 1.1075 - leftLayer2_loss: 0.0782 - midLayer2_loss: 1.5527 - rightLayer2_loss: 0.9483 - val_loss: 5.0644 - val_leftLayer1_loss: 0.1024 - val_midLayer1_loss: 1.3016 - val_rightLayer1_loss: 1.0725 - val_leftLayer2_loss: 0.0908 - val_midLayer2_loss: 1.3999 - val_rightLayer2_loss: 1.0971\n",
      "Epoch 6/11\n",
      "2264/2279 [============================>.] - ETA: 0s - loss: 5.0237 - leftLayer1_loss: 0.1007 - midLayer1_loss: 1.3078 - rightLayer1_loss: 1.0570 - leftLayer2_loss: 0.0731 - midLayer2_loss: 1.5554 - rightLayer2_loss: 0.9297\n",
      "Epoch 00006: val_loss improved from 5.06438 to 4.98683, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0258 - leftLayer1_loss: 0.1006 - midLayer1_loss: 1.3081 - rightLayer1_loss: 1.0574 - leftLayer2_loss: 0.0731 - midLayer2_loss: 1.5559 - rightLayer2_loss: 0.9306 - val_loss: 4.9868 - val_leftLayer1_loss: 0.0985 - val_midLayer1_loss: 1.3016 - val_rightLayer1_loss: 1.0300 - val_leftLayer2_loss: 0.0870 - val_midLayer2_loss: 1.3999 - val_rightLayer2_loss: 1.0698\n",
      "Epoch 7/11\n",
      "2267/2279 [============================>.] - ETA: 0s - loss: 4.9670 - leftLayer1_loss: 0.0968 - midLayer1_loss: 1.3077 - rightLayer1_loss: 1.0220 - leftLayer2_loss: 0.0687 - midLayer2_loss: 1.5507 - rightLayer2_loss: 0.9211\n",
      "Epoch 00007: val_loss improved from 4.98683 to 4.92842, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9682 - leftLayer1_loss: 0.0968 - midLayer1_loss: 1.3079 - rightLayer1_loss: 1.0223 - leftLayer2_loss: 0.0687 - midLayer2_loss: 1.5510 - rightLayer2_loss: 0.9216 - val_loss: 4.9284 - val_leftLayer1_loss: 0.0948 - val_midLayer1_loss: 1.3016 - val_rightLayer1_loss: 0.9994 - val_leftLayer2_loss: 0.0837 - val_midLayer2_loss: 1.3999 - val_rightLayer2_loss: 1.0490\n",
      "Epoch 8/11\n",
      "2261/2279 [============================>.] - ETA: 0s - loss: 4.9207 - leftLayer1_loss: 0.0933 - midLayer1_loss: 1.3067 - rightLayer1_loss: 0.9963 - leftLayer2_loss: 0.0648 - midLayer2_loss: 1.5501 - rightLayer2_loss: 0.9096\n",
      "Epoch 00008: val_loss improved from 4.92842 to 4.88296, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9235 - leftLayer1_loss: 0.0933 - midLayer1_loss: 1.3071 - rightLayer1_loss: 0.9970 - leftLayer2_loss: 0.0648 - midLayer2_loss: 1.5508 - rightLayer2_loss: 0.9105 - val_loss: 4.8830 - val_leftLayer1_loss: 0.0914 - val_midLayer1_loss: 1.3016 - val_rightLayer1_loss: 0.9767 - val_leftLayer2_loss: 0.0808 - val_midLayer2_loss: 1.3999 - val_rightLayer2_loss: 1.0326\n",
      "Epoch 9/11\n",
      "2261/2279 [============================>.] - ETA: 0s - loss: 4.8857 - leftLayer1_loss: 0.0901 - midLayer1_loss: 1.3077 - rightLayer1_loss: 0.9769 - leftLayer2_loss: 0.0616 - midLayer2_loss: 1.5463 - rightLayer2_loss: 0.9032\n",
      "Epoch 00009: val_loss improved from 4.88296 to 4.84645, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.8891 - leftLayer1_loss: 0.0901 - midLayer1_loss: 1.3080 - rightLayer1_loss: 0.9777 - leftLayer2_loss: 0.0616 - midLayer2_loss: 1.5472 - rightLayer2_loss: 0.9045 - val_loss: 4.8465 - val_leftLayer1_loss: 0.0883 - val_midLayer1_loss: 1.3016 - val_rightLayer1_loss: 0.9593 - val_leftLayer2_loss: 0.0782 - val_midLayer2_loss: 1.3999 - val_rightLayer2_loss: 1.0192\n",
      "Epoch 10/11\n",
      "2277/2279 [============================>.] - ETA: 0s - loss: 4.8751 - leftLayer1_loss: 0.0871 - midLayer1_loss: 1.3082 - rightLayer1_loss: 0.9629 - leftLayer2_loss: 0.0590 - midLayer2_loss: 1.5586 - rightLayer2_loss: 0.8993\n",
      "Epoch 00010: val_loss improved from 4.84645 to 4.81635, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.8745 - leftLayer1_loss: 0.0871 - midLayer1_loss: 1.3082 - rightLayer1_loss: 0.9628 - leftLayer2_loss: 0.0590 - midLayer2_loss: 1.5584 - rightLayer2_loss: 0.8991 - val_loss: 4.8164 - val_leftLayer1_loss: 0.0853 - val_midLayer1_loss: 1.3016 - val_rightLayer1_loss: 0.9456 - val_leftLayer2_loss: 0.0759 - val_midLayer2_loss: 1.3999 - val_rightLayer2_loss: 1.0081\n",
      "Epoch 11/11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2271/2279 [============================>.] - ETA: 0s - loss: 4.8491 - leftLayer1_loss: 0.0843 - midLayer1_loss: 1.3073 - rightLayer1_loss: 0.9503 - leftLayer2_loss: 0.0570 - midLayer2_loss: 1.5547 - rightLayer2_loss: 0.8956\n",
      "Epoch 00011: val_loss improved from 4.81635 to 4.79096, saving model to ./experiments/0.009999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.8504 - leftLayer1_loss: 0.0843 - midLayer1_loss: 1.3073 - rightLayer1_loss: 0.9506 - leftLayer2_loss: 0.0570 - midLayer2_loss: 1.5553 - rightLayer2_loss: 0.8960 - val_loss: 4.7910 - val_leftLayer1_loss: 0.0826 - val_midLayer1_loss: 1.3016 - val_rightLayer1_loss: 0.9345 - val_leftLayer2_loss: 0.0739 - val_midLayer2_loss: 1.3999 - val_rightLayer2_loss: 0.9984\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.009999999999999998_test.log **\n",
      "auroc 0PleuralThickening: 0.3102487300548638\n",
      "\n",
      "auprc 0PleuralThickening: 0.021287907731393458\n",
      "\n",
      "auroc 1PleuralThickening: 0.3733383160464081\n",
      "\n",
      "auprc 1PleuralThickening: 0.02375240178165767\n",
      "\n",
      "auroc 2PleuralThickening: 0.49731874659149405\n",
      "\n",
      "auprc 2PleuralThickening: 0.03140785857416327\n",
      "\n",
      "auroc 3PleuralThickening: 0.42197991770737936\n",
      "\n",
      "auprc 3PleuralThickening: 0.026149573514715363\n",
      "\n",
      "auroc 4PleuralThickening: 0.5601606724050745\n",
      "\n",
      "auprc 4PleuralThickening: 0.04594856284930678\n",
      "\n",
      "auroc 5PleuralThickening: 0.39412453618262144\n",
      "\n",
      "auprc 5PleuralThickening: 0.024431639190794968\n",
      "\n",
      "mean auroc: 0.42619515316464024\n",
      "\n",
      "mean auprc: 0.028829657273671917\n",
      "\n",
      "max auroc: 0.5601606724050745\n",
      "\n",
      "max auprc: 0.04594856284930678\n",
      "\n",
      "95.0400002002716\n",
      "** set output weights path to: ./experiments/0.010999999999999998_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 2279 steps, validate for 372 steps\n",
      "Epoch 1/11\n",
      "2257/2279 [============================>.] - ETA: 0s - loss: 6.3539 - leftLayer1_loss: 0.1201 - midLayer1_loss: 1.3305 - rightLayer1_loss: 1.7296 - leftLayer2_loss: 0.1134 - midLayer2_loss: 1.5413 - rightLayer2_loss: 1.5188\n",
      "Epoch 00001: val_loss improved from inf to 5.99112, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 7s 3ms/step - loss: 6.3518 - leftLayer1_loss: 0.1201 - midLayer1_loss: 1.3307 - rightLayer1_loss: 1.7284 - leftLayer2_loss: 0.1134 - midLayer2_loss: 1.5425 - rightLayer2_loss: 1.5168 - val_loss: 5.9911 - val_leftLayer1_loss: 0.1176 - val_midLayer1_loss: 1.3228 - val_rightLayer1_loss: 1.5813 - val_leftLayer2_loss: 0.1137 - val_midLayer2_loss: 1.4182 - val_rightLayer2_loss: 1.4376\n",
      "Epoch 2/11\n",
      "2272/2279 [============================>.] - ETA: 0s - loss: 5.6936 - leftLayer1_loss: 0.1146 - midLayer1_loss: 1.3300 - rightLayer1_loss: 1.4624 - leftLayer2_loss: 0.1015 - midLayer2_loss: 1.5397 - rightLayer2_loss: 1.1455\n",
      "Epoch 00002: val_loss improved from 5.99112 to 5.57206, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.6940 - leftLayer1_loss: 0.1146 - midLayer1_loss: 1.3301 - rightLayer1_loss: 1.4622 - leftLayer2_loss: 0.1015 - midLayer2_loss: 1.5402 - rightLayer2_loss: 1.1454 - val_loss: 5.5721 - val_leftLayer1_loss: 0.1122 - val_midLayer1_loss: 1.3228 - val_rightLayer1_loss: 1.3581 - val_leftLayer2_loss: 0.1064 - val_midLayer2_loss: 1.4182 - val_rightLayer2_loss: 1.2543\n",
      "Epoch 3/11\n",
      "2264/2279 [============================>.] - ETA: 0s - loss: 5.3753 - leftLayer1_loss: 0.1095 - midLayer1_loss: 1.3308 - rightLayer1_loss: 1.2830 - leftLayer2_loss: 0.0918 - midLayer2_loss: 1.5408 - rightLayer2_loss: 1.0194\n",
      "Epoch 00003: val_loss improved from 5.57206 to 5.32627, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.3768 - leftLayer1_loss: 0.1095 - midLayer1_loss: 1.3311 - rightLayer1_loss: 1.2829 - leftLayer2_loss: 0.0918 - midLayer2_loss: 1.5415 - rightLayer2_loss: 1.0200 - val_loss: 5.3263 - val_leftLayer1_loss: 0.1073 - val_midLayer1_loss: 1.3228 - val_rightLayer1_loss: 1.2144 - val_leftLayer2_loss: 0.1001 - val_midLayer2_loss: 1.4182 - val_rightLayer2_loss: 1.1635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/11\n",
      "2256/2279 [============================>.] - ETA: 0s - loss: 5.1959 - leftLayer1_loss: 0.1048 - midLayer1_loss: 1.3306 - rightLayer1_loss: 1.1695 - leftLayer2_loss: 0.0835 - midLayer2_loss: 1.5402 - rightLayer2_loss: 0.9673\n",
      "Epoch 00004: val_loss improved from 5.32627 to 5.17114, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.1992 - leftLayer1_loss: 0.1048 - midLayer1_loss: 1.3310 - rightLayer1_loss: 1.1698 - leftLayer2_loss: 0.0835 - midLayer2_loss: 1.5414 - rightLayer2_loss: 0.9687 - val_loss: 5.1711 - val_leftLayer1_loss: 0.1027 - val_midLayer1_loss: 1.3228 - val_rightLayer1_loss: 1.1230 - val_leftLayer2_loss: 0.0948 - val_midLayer2_loss: 1.4182 - val_rightLayer2_loss: 1.1097\n",
      "Epoch 5/11\n",
      "2268/2279 [============================>.] - ETA: 0s - loss: 5.0905 - leftLayer1_loss: 0.1003 - midLayer1_loss: 1.3315 - rightLayer1_loss: 1.0975 - leftLayer2_loss: 0.0766 - midLayer2_loss: 1.5461 - rightLayer2_loss: 0.9386\n",
      "Epoch 00005: val_loss improved from 5.17114 to 5.06703, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0921 - leftLayer1_loss: 0.1003 - midLayer1_loss: 1.3316 - rightLayer1_loss: 1.0977 - leftLayer2_loss: 0.0766 - midLayer2_loss: 1.5466 - rightLayer2_loss: 0.9392 - val_loss: 5.0670 - val_leftLayer1_loss: 0.0985 - val_midLayer1_loss: 1.3228 - val_rightLayer1_loss: 1.0631 - val_leftLayer2_loss: 0.0902 - val_midLayer2_loss: 1.4182 - val_rightLayer2_loss: 1.0743\n",
      "Epoch 6/11\n",
      "2263/2279 [============================>.] - ETA: 0s - loss: 5.0113 - leftLayer1_loss: 0.0962 - midLayer1_loss: 1.3312 - rightLayer1_loss: 1.0490 - leftLayer2_loss: 0.0717 - midLayer2_loss: 1.5381 - rightLayer2_loss: 0.9251\n",
      "Epoch 00006: val_loss improved from 5.06703 to 4.99223, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0131 - leftLayer1_loss: 0.0962 - midLayer1_loss: 1.3313 - rightLayer1_loss: 1.0494 - leftLayer2_loss: 0.0717 - midLayer2_loss: 1.5387 - rightLayer2_loss: 0.9258 - val_loss: 4.9922 - val_leftLayer1_loss: 0.0945 - val_midLayer1_loss: 1.3228 - val_rightLayer1_loss: 1.0221 - val_leftLayer2_loss: 0.0863 - val_midLayer2_loss: 1.4182 - val_rightLayer2_loss: 1.0483\n",
      "Epoch 7/11\n",
      "2270/2279 [============================>.] - ETA: 0s - loss: 4.9570 - leftLayer1_loss: 0.0925 - midLayer1_loss: 1.3311 - rightLayer1_loss: 1.0160 - leftLayer2_loss: 0.0672 - midLayer2_loss: 1.5366 - rightLayer2_loss: 0.9136\n",
      "Epoch 00007: val_loss improved from 4.99223 to 4.93637, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9572 - leftLayer1_loss: 0.0925 - midLayer1_loss: 1.3311 - rightLayer1_loss: 1.0160 - leftLayer2_loss: 0.0672 - midLayer2_loss: 1.5367 - rightLayer2_loss: 0.9137 - val_loss: 4.9364 - val_leftLayer1_loss: 0.0909 - val_midLayer1_loss: 1.3228 - val_rightLayer1_loss: 0.9928 - val_leftLayer2_loss: 0.0829 - val_midLayer2_loss: 1.4182 - val_rightLayer2_loss: 1.0288\n",
      "Epoch 8/11\n",
      "2270/2279 [============================>.] - ETA: 0s - loss: 4.9227 - leftLayer1_loss: 0.0890 - midLayer1_loss: 1.3315 - rightLayer1_loss: 0.9916 - leftLayer2_loss: 0.0637 - midLayer2_loss: 1.5412 - rightLayer2_loss: 0.9057\n",
      "Epoch 00008: val_loss improved from 4.93637 to 4.89284, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9231 - leftLayer1_loss: 0.0890 - midLayer1_loss: 1.3315 - rightLayer1_loss: 0.9917 - leftLayer2_loss: 0.0637 - midLayer2_loss: 1.5413 - rightLayer2_loss: 0.9058 - val_loss: 4.8928 - val_leftLayer1_loss: 0.0875 - val_midLayer1_loss: 1.3228 - val_rightLayer1_loss: 0.9712 - val_leftLayer2_loss: 0.0799 - val_midLayer2_loss: 1.4182 - val_rightLayer2_loss: 1.0133\n",
      "Epoch 9/11\n",
      "2266/2279 [============================>.] - ETA: 0s - loss: 4.8916 - leftLayer1_loss: 0.0858 - midLayer1_loss: 1.3312 - rightLayer1_loss: 0.9731 - leftLayer2_loss: 0.0604 - midLayer2_loss: 1.5423 - rightLayer2_loss: 0.8987\n",
      "Epoch 00009: val_loss improved from 4.89284 to 4.85800, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.8936 - leftLayer1_loss: 0.0858 - midLayer1_loss: 1.3314 - rightLayer1_loss: 0.9737 - leftLayer2_loss: 0.0604 - midLayer2_loss: 1.5430 - rightLayer2_loss: 0.8993 - val_loss: 4.8580 - val_leftLayer1_loss: 0.0844 - val_midLayer1_loss: 1.3228 - val_rightLayer1_loss: 0.9546 - val_leftLayer2_loss: 0.0772 - val_midLayer2_loss: 1.4182 - val_rightLayer2_loss: 1.0007\n",
      "Epoch 10/11\n",
      "2265/2279 [============================>.] - ETA: 0s - loss: 4.8691 - leftLayer1_loss: 0.0829 - midLayer1_loss: 1.3311 - rightLayer1_loss: 0.9596 - leftLayer2_loss: 0.0577 - midLayer2_loss: 1.5426 - rightLayer2_loss: 0.8952\n",
      "Epoch 00010: val_loss improved from 4.85800 to 4.82925, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.8719 - leftLayer1_loss: 0.0829 - midLayer1_loss: 1.3314 - rightLayer1_loss: 0.9602 - leftLayer2_loss: 0.0578 - midLayer2_loss: 1.5437 - rightLayer2_loss: 0.8960 - val_loss: 4.8293 - val_leftLayer1_loss: 0.0816 - val_midLayer1_loss: 1.3228 - val_rightLayer1_loss: 0.9415 - val_leftLayer2_loss: 0.0749 - val_midLayer2_loss: 1.4182 - val_rightLayer2_loss: 0.9902\n",
      "Epoch 11/11\n",
      "2265/2279 [============================>.] - ETA: 0s - loss: 4.8383 - leftLayer1_loss: 0.0801 - midLayer1_loss: 1.3309 - rightLayer1_loss: 0.9474 - leftLayer2_loss: 0.0557 - midLayer2_loss: 1.5315 - rightLayer2_loss: 0.8926\n",
      "Epoch 00011: val_loss improved from 4.82925 to 4.80510, saving model to ./experiments/0.010999999999999998_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.8405 - leftLayer1_loss: 0.0801 - midLayer1_loss: 1.3312 - rightLayer1_loss: 0.9481 - leftLayer2_loss: 0.0557 - midLayer2_loss: 1.5321 - rightLayer2_loss: 0.8933 - val_loss: 4.8051 - val_leftLayer1_loss: 0.0790 - val_midLayer1_loss: 1.3228 - val_rightLayer1_loss: 0.9310 - val_leftLayer2_loss: 0.0729 - val_midLayer2_loss: 1.4182 - val_rightLayer2_loss: 0.9813\n",
      "22433/22433 [==============================] - 30s 1ms/step\n",
      "** write log to ./experiments/0.010999999999999998_test.log **\n",
      "auroc 0PleuralThickening: 0.504418610433334\n",
      "\n",
      "auprc 0PleuralThickening: 0.03674258570851236\n",
      "\n",
      "auroc 1PleuralThickening: 0.5949590464433312\n",
      "\n",
      "auprc 1PleuralThickening: 0.046707160787742355\n",
      "\n",
      "auroc 2PleuralThickening: 0.4514406482650351\n",
      "\n",
      "auprc 2PleuralThickening: 0.027009356095539033\n",
      "\n",
      "auroc 3PleuralThickening: 0.3532011168912089\n",
      "\n",
      "auprc 3PleuralThickening: 0.02273665289061061\n",
      "\n",
      "auroc 4PleuralThickening: 0.5583999903058103\n",
      "\n",
      "auprc 4PleuralThickening: 0.03708694889513905\n",
      "\n",
      "auroc 5PleuralThickening: 0.3946327277101759\n",
      "\n",
      "auprc 5PleuralThickening: 0.02453361358532066\n",
      "\n",
      "mean auroc: 0.47617535667481586\n",
      "\n",
      "mean auprc: 0.032469386327144016\n",
      "\n",
      "max auroc: 0.5949590464433312\n",
      "\n",
      "max auprc: 0.046707160787742355\n",
      "\n",
      "97.98017907142639\n",
      "** set output weights path to: ./experiments/0.011999999999999997_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 2279 steps, validate for 372 steps\n",
      "Epoch 1/11\n",
      "2278/2279 [============================>.] - ETA: 0s - loss: 6.5080 - leftLayer1_loss: 0.1204 - midLayer1_loss: 1.3533 - rightLayer1_loss: 1.7408 - leftLayer2_loss: 0.1216 - midLayer2_loss: 1.5919 - rightLayer2_loss: 1.5800\n",
      "Epoch 00001: val_loss improved from inf to 6.07708, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "2279/2279 [==============================] - 7s 3ms/step - loss: 6.5078 - leftLayer1_loss: 0.1204 - midLayer1_loss: 1.3532 - rightLayer1_loss: 1.7408 - leftLayer2_loss: 0.1216 - midLayer2_loss: 1.5919 - rightLayer2_loss: 1.5798 - val_loss: 6.0771 - val_leftLayer1_loss: 0.1180 - val_midLayer1_loss: 1.3414 - val_rightLayer1_loss: 1.6040 - val_leftLayer2_loss: 0.1167 - val_midLayer2_loss: 1.4160 - val_rightLayer2_loss: 1.4809\n",
      "Epoch 2/11\n",
      "2269/2279 [============================>.] - ETA: 0s - loss: 5.8473 - leftLayer1_loss: 0.1156 - midLayer1_loss: 1.3524 - rightLayer1_loss: 1.5034 - leftLayer2_loss: 0.1086 - midLayer2_loss: 1.5929 - rightLayer2_loss: 1.1744\n",
      "Epoch 00002: val_loss improved from 6.07708 to 5.65938, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "2279/2279 [==============================] - 7s 3ms/step - loss: 5.8467 - leftLayer1_loss: 0.1156 - midLayer1_loss: 1.3525 - rightLayer1_loss: 1.5030 - leftLayer2_loss: 0.1086 - midLayer2_loss: 1.5926 - rightLayer2_loss: 1.1744 - val_loss: 5.6594 - val_leftLayer1_loss: 0.1133 - val_midLayer1_loss: 1.3414 - val_rightLayer1_loss: 1.3970 - val_leftLayer2_loss: 0.1091 - val_midLayer2_loss: 1.4160 - val_rightLayer2_loss: 1.2826\n",
      "Epoch 3/11\n",
      "2278/2279 [============================>.] - ETA: 0s - loss: 5.5176 - leftLayer1_loss: 0.1112 - midLayer1_loss: 1.3524 - rightLayer1_loss: 1.3321 - leftLayer2_loss: 0.0978 - midLayer2_loss: 1.5885 - rightLayer2_loss: 1.0357\n",
      "Epoch 00003: val_loss improved from 5.65938 to 5.40704, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.5176 - leftLayer1_loss: 0.1112 - midLayer1_loss: 1.3523 - rightLayer1_loss: 1.3321 - leftLayer2_loss: 0.0978 - midLayer2_loss: 1.5885 - rightLayer2_loss: 1.0356 - val_loss: 5.4070 - val_leftLayer1_loss: 0.1089 - val_midLayer1_loss: 1.3414 - val_rightLayer1_loss: 1.2543 - val_leftLayer2_loss: 0.1025 - val_midLayer2_loss: 1.4160 - val_rightLayer2_loss: 1.1840\n",
      "Epoch 4/11\n",
      "2259/2279 [============================>.] - ETA: 0s - loss: 5.3348 - leftLayer1_loss: 0.1069 - midLayer1_loss: 1.3523 - rightLayer1_loss: 1.2145 - leftLayer2_loss: 0.0892 - midLayer2_loss: 1.5951 - rightLayer2_loss: 0.9769\n",
      "Epoch 00004: val_loss improved from 5.40704 to 5.24333, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.3371 - leftLayer1_loss: 0.1069 - midLayer1_loss: 1.3528 - rightLayer1_loss: 1.2148 - leftLayer2_loss: 0.0891 - midLayer2_loss: 1.5959 - rightLayer2_loss: 0.9777 - val_loss: 5.2433 - val_leftLayer1_loss: 0.1048 - val_midLayer1_loss: 1.3414 - val_rightLayer1_loss: 1.1582 - val_leftLayer2_loss: 0.0969 - val_midLayer2_loss: 1.4160 - val_rightLayer2_loss: 1.1261\n",
      "Epoch 5/11\n",
      "2260/2279 [============================>.] - ETA: 0s - loss: 5.2100 - leftLayer1_loss: 0.1030 - midLayer1_loss: 1.3528 - rightLayer1_loss: 1.1365 - leftLayer2_loss: 0.0817 - midLayer2_loss: 1.5905 - rightLayer2_loss: 0.9455\n",
      "Epoch 00005: val_loss improved from 5.24333 to 5.13153, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.2130 - leftLayer1_loss: 0.1030 - midLayer1_loss: 1.3535 - rightLayer1_loss: 1.1370 - leftLayer2_loss: 0.0817 - midLayer2_loss: 1.5913 - rightLayer2_loss: 0.9466 - val_loss: 5.1315 - val_leftLayer1_loss: 0.1009 - val_midLayer1_loss: 1.3414 - val_rightLayer1_loss: 1.0930 - val_leftLayer2_loss: 0.0920 - val_midLayer2_loss: 1.4160 - val_rightLayer2_loss: 1.0882\n",
      "Epoch 6/11\n",
      "2258/2279 [============================>.] - ETA: 0s - loss: 5.1353 - leftLayer1_loss: 0.0993 - midLayer1_loss: 1.3519 - rightLayer1_loss: 1.0822 - leftLayer2_loss: 0.0756 - midLayer2_loss: 1.5980 - rightLayer2_loss: 0.9284\n",
      "Epoch 00006: val_loss improved from 5.13153 to 5.05085, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.1379 - leftLayer1_loss: 0.0993 - midLayer1_loss: 1.3525 - rightLayer1_loss: 1.0828 - leftLayer2_loss: 0.0756 - midLayer2_loss: 1.5981 - rightLayer2_loss: 0.9296 - val_loss: 5.0508 - val_leftLayer1_loss: 0.0973 - val_midLayer1_loss: 1.3414 - val_rightLayer1_loss: 1.0474 - val_leftLayer2_loss: 0.0878 - val_midLayer2_loss: 1.4160 - val_rightLayer2_loss: 1.0610\n",
      "Epoch 7/11\n",
      "2272/2279 [============================>.] - ETA: 0s - loss: 5.0761 - leftLayer1_loss: 0.0959 - midLayer1_loss: 1.3525 - rightLayer1_loss: 1.0443 - leftLayer2_loss: 0.0705 - midLayer2_loss: 1.5959 - rightLayer2_loss: 0.9170\n",
      "Epoch 00007: val_loss improved from 5.05085 to 4.99026, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0770 - leftLayer1_loss: 0.0958 - midLayer1_loss: 1.3527 - rightLayer1_loss: 1.0444 - leftLayer2_loss: 0.0705 - midLayer2_loss: 1.5962 - rightLayer2_loss: 0.9174 - val_loss: 4.9903 - val_leftLayer1_loss: 0.0940 - val_midLayer1_loss: 1.3414 - val_rightLayer1_loss: 1.0144 - val_leftLayer2_loss: 0.0842 - val_midLayer2_loss: 1.4160 - val_rightLayer2_loss: 1.0404\n",
      "Epoch 8/11\n",
      "2270/2279 [============================>.] - ETA: 0s - loss: 5.0262 - leftLayer1_loss: 0.0927 - midLayer1_loss: 1.3535 - rightLayer1_loss: 1.0164 - leftLayer2_loss: 0.0663 - midLayer2_loss: 1.5877 - rightLayer2_loss: 0.9096\n",
      "Epoch 00008: val_loss improved from 4.99026 to 4.94300, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0264 - leftLayer1_loss: 0.0927 - midLayer1_loss: 1.3535 - rightLayer1_loss: 1.0165 - leftLayer2_loss: 0.0663 - midLayer2_loss: 1.5877 - rightLayer2_loss: 0.9097 - val_loss: 4.9430 - val_leftLayer1_loss: 0.0908 - val_midLayer1_loss: 1.3414 - val_rightLayer1_loss: 0.9898 - val_leftLayer2_loss: 0.0810 - val_midLayer2_loss: 1.4160 - val_rightLayer2_loss: 1.0241\n",
      "Epoch 9/11\n",
      "2271/2279 [============================>.] - ETA: 0s - loss: 4.9945 - leftLayer1_loss: 0.0897 - midLayer1_loss: 1.3530 - rightLayer1_loss: 0.9946 - leftLayer2_loss: 0.0628 - midLayer2_loss: 1.5918 - rightLayer2_loss: 0.9026\n",
      "Epoch 00009: val_loss improved from 4.94300 to 4.90522, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9954 - leftLayer1_loss: 0.0897 - midLayer1_loss: 1.3531 - rightLayer1_loss: 0.9949 - leftLayer2_loss: 0.0628 - midLayer2_loss: 1.5920 - rightLayer2_loss: 0.9030 - val_loss: 4.9052 - val_leftLayer1_loss: 0.0879 - val_midLayer1_loss: 1.3414 - val_rightLayer1_loss: 0.9708 - val_leftLayer2_loss: 0.0782 - val_midLayer2_loss: 1.4160 - val_rightLayer2_loss: 1.0109\n",
      "Epoch 10/11\n",
      "2263/2279 [============================>.] - ETA: 0s - loss: 4.9652 - leftLayer1_loss: 0.0869 - midLayer1_loss: 1.3523 - rightLayer1_loss: 0.9777 - leftLayer2_loss: 0.0601 - midLayer2_loss: 1.5912 - rightLayer2_loss: 0.8971\n",
      "Epoch 00010: val_loss improved from 4.90522 to 4.87421, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "2279/2279 [==============================] - 7s 3ms/step - loss: 4.9670 - leftLayer1_loss: 0.0869 - midLayer1_loss: 1.3524 - rightLayer1_loss: 0.9782 - leftLayer2_loss: 0.0601 - midLayer2_loss: 1.5916 - rightLayer2_loss: 0.8978 - val_loss: 4.8742 - val_leftLayer1_loss: 0.0852 - val_midLayer1_loss: 1.3414 - val_rightLayer1_loss: 0.9559 - val_leftLayer2_loss: 0.0758 - val_midLayer2_loss: 1.4160 - val_rightLayer2_loss: 1.0000\n",
      "Epoch 11/11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2270/2279 [============================>.] - ETA: 0s - loss: 4.9511 - leftLayer1_loss: 0.0843 - midLayer1_loss: 1.3526 - rightLayer1_loss: 0.9648 - leftLayer2_loss: 0.0576 - midLayer2_loss: 1.5964 - rightLayer2_loss: 0.8954\n",
      "Epoch 00011: val_loss improved from 4.87421 to 4.84815, saving model to ./experiments/0.011999999999999997_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9510 - leftLayer1_loss: 0.0842 - midLayer1_loss: 1.3526 - rightLayer1_loss: 0.9649 - leftLayer2_loss: 0.0576 - midLayer2_loss: 1.5961 - rightLayer2_loss: 0.8955 - val_loss: 4.8481 - val_leftLayer1_loss: 0.0826 - val_midLayer1_loss: 1.3414 - val_rightLayer1_loss: 0.9440 - val_leftLayer2_loss: 0.0736 - val_midLayer2_loss: 1.4160 - val_rightLayer2_loss: 0.9906\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.011999999999999997_test.log **\n",
      "auroc 0PleuralThickening: 0.4234871632979985\n",
      "\n",
      "auprc 0PleuralThickening: 0.026049480461104068\n",
      "\n",
      "auroc 1PleuralThickening: 0.5172427238011069\n",
      "\n",
      "auprc 1PleuralThickening: 0.0360985108157814\n",
      "\n",
      "auroc 2PleuralThickening: 0.5239758848239845\n",
      "\n",
      "auprc 2PleuralThickening: 0.0331012117603985\n",
      "\n",
      "auroc 3PleuralThickening: 0.43289686876415284\n",
      "\n",
      "auprc 3PleuralThickening: 0.027036493163787305\n",
      "\n",
      "auroc 4PleuralThickening: 0.6380067113428173\n",
      "\n",
      "auprc 4PleuralThickening: 0.05606721912101585\n",
      "\n",
      "auroc 5PleuralThickening: 0.42927633375789365\n",
      "\n",
      "auprc 5PleuralThickening: 0.02631834385005102\n",
      "\n",
      "mean auroc: 0.4941476142979922\n",
      "\n",
      "mean auprc: 0.034111876528689695\n",
      "\n",
      "max auroc: 0.6380067113428173\n",
      "\n",
      "max auprc: 0.05606721912101585\n",
      "\n",
      "98.17277026176453\n",
      "** set output weights path to: ./experiments/0.012999999999999996_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 2279 steps, validate for 372 steps\n",
      "Epoch 1/11\n",
      "2262/2279 [============================>.] - ETA: 0s - loss: 6.4287 - leftLayer1_loss: 0.1215 - midLayer1_loss: 1.3970 - rightLayer1_loss: 1.7365 - leftLayer2_loss: 0.1236 - midLayer2_loss: 1.5178 - rightLayer2_loss: 1.5322\n",
      "Epoch 00001: val_loss improved from inf to 6.12725, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "2279/2279 [==============================] - 7s 3ms/step - loss: 6.4260 - leftLayer1_loss: 0.1215 - midLayer1_loss: 1.3974 - rightLayer1_loss: 1.7356 - leftLayer2_loss: 0.1235 - midLayer2_loss: 1.5174 - rightLayer2_loss: 1.5307 - val_loss: 6.1273 - val_leftLayer1_loss: 0.1188 - val_midLayer1_loss: 1.3876 - val_rightLayer1_loss: 1.6005 - val_leftLayer2_loss: 0.1185 - val_midLayer2_loss: 1.4356 - val_rightLayer2_loss: 1.4663\n",
      "Epoch 2/11\n",
      "2259/2279 [============================>.] - ETA: 0s - loss: 5.7818 - leftLayer1_loss: 0.1167 - midLayer1_loss: 1.3964 - rightLayer1_loss: 1.4970 - leftLayer2_loss: 0.1101 - midLayer2_loss: 1.5108 - rightLayer2_loss: 1.1507\n",
      "Epoch 00002: val_loss improved from 6.12725 to 5.71699, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.7819 - leftLayer1_loss: 0.1167 - midLayer1_loss: 1.3970 - rightLayer1_loss: 1.4965 - leftLayer2_loss: 0.1101 - midLayer2_loss: 1.5109 - rightLayer2_loss: 1.1508 - val_loss: 5.7170 - val_leftLayer1_loss: 0.1141 - val_midLayer1_loss: 1.3876 - val_rightLayer1_loss: 1.3942 - val_leftLayer2_loss: 0.1104 - val_midLayer2_loss: 1.4356 - val_rightLayer2_loss: 1.2752\n",
      "Epoch 3/11\n",
      "2263/2279 [============================>.] - ETA: 0s - loss: 5.4711 - leftLayer1_loss: 0.1120 - midLayer1_loss: 1.3964 - rightLayer1_loss: 1.3246 - leftLayer2_loss: 0.0990 - midLayer2_loss: 1.5178 - rightLayer2_loss: 1.0213\n",
      "Epoch 00003: val_loss improved from 5.71699 to 5.46732, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.4711 - leftLayer1_loss: 0.1120 - midLayer1_loss: 1.3966 - rightLayer1_loss: 1.3243 - leftLayer2_loss: 0.0990 - midLayer2_loss: 1.5176 - rightLayer2_loss: 1.0215 - val_loss: 5.4673 - val_leftLayer1_loss: 0.1096 - val_midLayer1_loss: 1.3876 - val_rightLayer1_loss: 1.2512 - val_leftLayer2_loss: 0.1035 - val_midLayer2_loss: 1.4356 - val_rightLayer2_loss: 1.1799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/11\n",
      "2275/2279 [============================>.] - ETA: 0s - loss: 5.2915 - leftLayer1_loss: 0.1077 - midLayer1_loss: 1.3966 - rightLayer1_loss: 1.2079 - leftLayer2_loss: 0.0893 - midLayer2_loss: 1.5185 - rightLayer2_loss: 0.9715\n",
      "Epoch 00004: val_loss improved from 5.46732 to 5.30415, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.2913 - leftLayer1_loss: 0.1077 - midLayer1_loss: 1.3966 - rightLayer1_loss: 1.2079 - leftLayer2_loss: 0.0893 - midLayer2_loss: 1.5182 - rightLayer2_loss: 0.9716 - val_loss: 5.3041 - val_leftLayer1_loss: 0.1054 - val_midLayer1_loss: 1.3876 - val_rightLayer1_loss: 1.1549 - val_leftLayer2_loss: 0.0975 - val_midLayer2_loss: 1.4356 - val_rightLayer2_loss: 1.1231\n",
      "Epoch 5/11\n",
      "2268/2279 [============================>.] - ETA: 0s - loss: 5.1688 - leftLayer1_loss: 0.1037 - midLayer1_loss: 1.3979 - rightLayer1_loss: 1.1285 - leftLayer2_loss: 0.0817 - midLayer2_loss: 1.5157 - rightLayer2_loss: 0.9414\n",
      "Epoch 00005: val_loss improved from 5.30415 to 5.19212, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.1697 - leftLayer1_loss: 0.1037 - midLayer1_loss: 1.3981 - rightLayer1_loss: 1.1288 - leftLayer2_loss: 0.0817 - midLayer2_loss: 1.5155 - rightLayer2_loss: 0.9420 - val_loss: 5.1921 - val_leftLayer1_loss: 0.1015 - val_midLayer1_loss: 1.3876 - val_rightLayer1_loss: 1.0892 - val_leftLayer2_loss: 0.0925 - val_midLayer2_loss: 1.4356 - val_rightLayer2_loss: 1.0858\n",
      "Epoch 6/11\n",
      "2267/2279 [============================>.] - ETA: 0s - loss: 5.0927 - leftLayer1_loss: 0.0999 - midLayer1_loss: 1.3971 - rightLayer1_loss: 1.0743 - leftLayer2_loss: 0.0752 - midLayer2_loss: 1.5214 - rightLayer2_loss: 0.9247\n",
      "Epoch 00006: val_loss improved from 5.19212 to 5.11121, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0935 - leftLayer1_loss: 0.0999 - midLayer1_loss: 1.3973 - rightLayer1_loss: 1.0746 - leftLayer2_loss: 0.0752 - midLayer2_loss: 1.5213 - rightLayer2_loss: 0.9251 - val_loss: 5.1112 - val_leftLayer1_loss: 0.0979 - val_midLayer1_loss: 1.3876 - val_rightLayer1_loss: 1.0431 - val_leftLayer2_loss: 0.0881 - val_midLayer2_loss: 1.4356 - val_rightLayer2_loss: 1.0590\n",
      "Epoch 7/11\n",
      "2278/2279 [============================>.] - ETA: 0s - loss: 5.0308 - leftLayer1_loss: 0.0964 - midLayer1_loss: 1.3977 - rightLayer1_loss: 1.0359 - leftLayer2_loss: 0.0705 - midLayer2_loss: 1.5154 - rightLayer2_loss: 0.9149\n",
      "Epoch 00007: val_loss improved from 5.11121 to 5.05013, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0308 - leftLayer1_loss: 0.0964 - midLayer1_loss: 1.3977 - rightLayer1_loss: 1.0359 - leftLayer2_loss: 0.0705 - midLayer2_loss: 1.5153 - rightLayer2_loss: 0.9149 - val_loss: 5.0501 - val_leftLayer1_loss: 0.0945 - val_midLayer1_loss: 1.3876 - val_rightLayer1_loss: 1.0096 - val_leftLayer2_loss: 0.0843 - val_midLayer2_loss: 1.4356 - val_rightLayer2_loss: 1.0385\n",
      "Epoch 8/11\n",
      "2270/2279 [============================>.] - ETA: 0s - loss: 4.9874 - leftLayer1_loss: 0.0931 - midLayer1_loss: 1.3973 - rightLayer1_loss: 1.0080 - leftLayer2_loss: 0.0663 - midLayer2_loss: 1.5161 - rightLayer2_loss: 0.9065\n",
      "Epoch 00008: val_loss improved from 5.05013 to 5.00256, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9873 - leftLayer1_loss: 0.0931 - midLayer1_loss: 1.3973 - rightLayer1_loss: 1.0081 - leftLayer2_loss: 0.0663 - midLayer2_loss: 1.5159 - rightLayer2_loss: 0.9066 - val_loss: 5.0026 - val_leftLayer1_loss: 0.0913 - val_midLayer1_loss: 1.3876 - val_rightLayer1_loss: 0.9847 - val_leftLayer2_loss: 0.0811 - val_midLayer2_loss: 1.4356 - val_rightLayer2_loss: 1.0223\n",
      "Epoch 9/11\n",
      "2274/2279 [============================>.] - ETA: 0s - loss: 4.9570 - leftLayer1_loss: 0.0900 - midLayer1_loss: 1.3973 - rightLayer1_loss: 0.9862 - leftLayer2_loss: 0.0627 - midLayer2_loss: 1.5198 - rightLayer2_loss: 0.9009\n",
      "Epoch 00009: val_loss improved from 5.00256 to 4.96429, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9573 - leftLayer1_loss: 0.0900 - midLayer1_loss: 1.3974 - rightLayer1_loss: 0.9865 - leftLayer2_loss: 0.0627 - midLayer2_loss: 1.5195 - rightLayer2_loss: 0.9012 - val_loss: 4.9643 - val_leftLayer1_loss: 0.0883 - val_midLayer1_loss: 1.3876 - val_rightLayer1_loss: 0.9656 - val_leftLayer2_loss: 0.0782 - val_midLayer2_loss: 1.4356 - val_rightLayer2_loss: 1.0090\n",
      "Epoch 10/11\n",
      "2264/2279 [============================>.] - ETA: 0s - loss: 4.9212 - leftLayer1_loss: 0.0871 - midLayer1_loss: 1.3970 - rightLayer1_loss: 0.9689 - leftLayer2_loss: 0.0598 - midLayer2_loss: 1.5125 - rightLayer2_loss: 0.8959\n",
      "Epoch 00010: val_loss improved from 4.96429 to 4.93274, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9231 - leftLayer1_loss: 0.0871 - midLayer1_loss: 1.3972 - rightLayer1_loss: 0.9697 - leftLayer2_loss: 0.0598 - midLayer2_loss: 1.5123 - rightLayer2_loss: 0.8969 - val_loss: 4.9327 - val_leftLayer1_loss: 0.0856 - val_midLayer1_loss: 1.3876 - val_rightLayer1_loss: 0.9505 - val_leftLayer2_loss: 0.0757 - val_midLayer2_loss: 1.4356 - val_rightLayer2_loss: 0.9979\n",
      "Epoch 11/11\n",
      "2278/2279 [============================>.] - ETA: 0s - loss: 4.9037 - leftLayer1_loss: 0.0845 - midLayer1_loss: 1.3981 - rightLayer1_loss: 0.9561 - leftLayer2_loss: 0.0573 - midLayer2_loss: 1.5145 - rightLayer2_loss: 0.8931\n",
      "Epoch 00011: val_loss improved from 4.93274 to 4.90639, saving model to ./experiments/0.012999999999999996_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9036 - leftLayer1_loss: 0.0845 - midLayer1_loss: 1.3981 - rightLayer1_loss: 0.9562 - leftLayer2_loss: 0.0573 - midLayer2_loss: 1.5145 - rightLayer2_loss: 0.8932 - val_loss: 4.9064 - val_leftLayer1_loss: 0.0830 - val_midLayer1_loss: 1.3876 - val_rightLayer1_loss: 0.9383 - val_leftLayer2_loss: 0.0735 - val_midLayer2_loss: 1.4356 - val_rightLayer2_loss: 0.9884\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.012999999999999996_test.log **\n",
      "auroc 0PleuralThickening: 0.5285734673291365\n",
      "\n",
      "auprc 0PleuralThickening: 0.035456085557429726\n",
      "\n",
      "auroc 1PleuralThickening: 0.49114199690011956\n",
      "\n",
      "auprc 1PleuralThickening: 0.033429907977959146\n",
      "\n",
      "auroc 2PleuralThickening: 0.4124302304015065\n",
      "\n",
      "auprc 2PleuralThickening: 0.02519272482484667\n",
      "\n",
      "auroc 3PleuralThickening: 0.4681198345005917\n",
      "\n",
      "auprc 3PleuralThickening: 0.028606661192111277\n",
      "\n",
      "auroc 4PleuralThickening: 0.6462519524939496\n",
      "\n",
      "auprc 4PleuralThickening: 0.05120888283012366\n",
      "\n",
      "auroc 5PleuralThickening: 0.39346995862263645\n",
      "\n",
      "auprc 5PleuralThickening: 0.02484270025729014\n",
      "\n",
      "mean auroc: 0.48999790670799004\n",
      "\n",
      "mean auprc: 0.033122827106626763\n",
      "\n",
      "max auroc: 0.6462519524939496\n",
      "\n",
      "max auprc: 0.05120888283012366\n",
      "\n",
      "96.44118046760559\n",
      "** set output weights path to: ./experiments/0.013999999999999995_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 2279 steps, validate for 372 steps\n",
      "Epoch 1/11\n",
      "2277/2279 [============================>.] - ETA: 0s - loss: 6.4182 - leftLayer1_loss: 0.1247 - midLayer1_loss: 1.3795 - rightLayer1_loss: 1.7263 - leftLayer2_loss: 0.1329 - midLayer2_loss: 1.5116 - rightLayer2_loss: 1.5432\n",
      "Epoch 00001: val_loss improved from inf to 6.06420, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "2279/2279 [==============================] - 7s 3ms/step - loss: 6.4174 - leftLayer1_loss: 0.1247 - midLayer1_loss: 1.3795 - rightLayer1_loss: 1.7262 - leftLayer2_loss: 0.1329 - midLayer2_loss: 1.5114 - rightLayer2_loss: 1.5428 - val_loss: 6.0642 - val_leftLayer1_loss: 0.1219 - val_midLayer1_loss: 1.3643 - val_rightLayer1_loss: 1.5872 - val_leftLayer2_loss: 0.1234 - val_midLayer2_loss: 1.4248 - val_rightLayer2_loss: 1.4427\n",
      "Epoch 2/11\n",
      "2259/2279 [============================>.] - ETA: 0s - loss: 5.7555 - leftLayer1_loss: 0.1193 - midLayer1_loss: 1.3790 - rightLayer1_loss: 1.4811 - leftLayer2_loss: 0.1176 - midLayer2_loss: 1.5147 - rightLayer2_loss: 1.1437\n",
      "Epoch 00002: val_loss improved from 6.06420 to 5.64898, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.7556 - leftLayer1_loss: 0.1193 - midLayer1_loss: 1.3794 - rightLayer1_loss: 1.4805 - leftLayer2_loss: 0.1175 - midLayer2_loss: 1.5149 - rightLayer2_loss: 1.1438 - val_loss: 5.6490 - val_leftLayer1_loss: 0.1167 - val_midLayer1_loss: 1.3643 - val_rightLayer1_loss: 1.3782 - val_leftLayer2_loss: 0.1143 - val_midLayer2_loss: 1.4248 - val_rightLayer2_loss: 1.2507\n",
      "Epoch 3/11\n",
      "2269/2279 [============================>.] - ETA: 0s - loss: 5.4373 - leftLayer1_loss: 0.1143 - midLayer1_loss: 1.3796 - rightLayer1_loss: 1.3083 - leftLayer2_loss: 0.1047 - midLayer2_loss: 1.5125 - rightLayer2_loss: 1.0179\n",
      "Epoch 00003: val_loss improved from 5.64898 to 5.40296, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.4372 - leftLayer1_loss: 0.1143 - midLayer1_loss: 1.3796 - rightLayer1_loss: 1.3081 - leftLayer2_loss: 0.1047 - midLayer2_loss: 1.5123 - rightLayer2_loss: 1.0181 - val_loss: 5.4030 - val_leftLayer1_loss: 0.1119 - val_midLayer1_loss: 1.3643 - val_rightLayer1_loss: 1.2375 - val_leftLayer2_loss: 0.1065 - val_midLayer2_loss: 1.4248 - val_rightLayer2_loss: 1.1580\n",
      "Epoch 4/11\n",
      "2267/2279 [============================>.] - ETA: 0s - loss: 5.2557 - leftLayer1_loss: 0.1096 - midLayer1_loss: 1.3794 - rightLayer1_loss: 1.1941 - leftLayer2_loss: 0.0941 - midLayer2_loss: 1.5120 - rightLayer2_loss: 0.9666\n",
      "Epoch 00004: val_loss improved from 5.40296 to 5.24469, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.2561 - leftLayer1_loss: 0.1096 - midLayer1_loss: 1.3793 - rightLayer1_loss: 1.1940 - leftLayer2_loss: 0.0940 - midLayer2_loss: 1.5122 - rightLayer2_loss: 0.9670 - val_loss: 5.2447 - val_leftLayer1_loss: 0.1074 - val_midLayer1_loss: 1.3643 - val_rightLayer1_loss: 1.1449 - val_leftLayer2_loss: 0.0998 - val_midLayer2_loss: 1.4248 - val_rightLayer2_loss: 1.1035\n",
      "Epoch 5/11\n",
      "2271/2279 [============================>.] - ETA: 0s - loss: 5.1437 - leftLayer1_loss: 0.1053 - midLayer1_loss: 1.3797 - rightLayer1_loss: 1.1186 - leftLayer2_loss: 0.0854 - midLayer2_loss: 1.5153 - rightLayer2_loss: 0.9394\n",
      "Epoch 00005: val_loss improved from 5.24469 to 5.13636, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.1437 - leftLayer1_loss: 0.1053 - midLayer1_loss: 1.3796 - rightLayer1_loss: 1.1187 - leftLayer2_loss: 0.0853 - midLayer2_loss: 1.5151 - rightLayer2_loss: 0.9397 - val_loss: 5.1364 - val_leftLayer1_loss: 0.1031 - val_midLayer1_loss: 1.3643 - val_rightLayer1_loss: 1.0826 - val_leftLayer2_loss: 0.0942 - val_midLayer2_loss: 1.4248 - val_rightLayer2_loss: 1.0673\n",
      "Epoch 6/11\n",
      "2263/2279 [============================>.] - ETA: 0s - loss: 5.0668 - leftLayer1_loss: 0.1012 - midLayer1_loss: 1.3794 - rightLayer1_loss: 1.0672 - leftLayer2_loss: 0.0783 - midLayer2_loss: 1.5171 - rightLayer2_loss: 0.9235\n",
      "Epoch 00006: val_loss improved from 5.13636 to 5.05856, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0677 - leftLayer1_loss: 0.1012 - midLayer1_loss: 1.3794 - rightLayer1_loss: 1.0675 - leftLayer2_loss: 0.0783 - midLayer2_loss: 1.5170 - rightLayer2_loss: 0.9242 - val_loss: 5.0586 - val_leftLayer1_loss: 0.0992 - val_midLayer1_loss: 1.3643 - val_rightLayer1_loss: 1.0392 - val_leftLayer2_loss: 0.0893 - val_midLayer2_loss: 1.4248 - val_rightLayer2_loss: 1.0417\n",
      "Epoch 7/11\n",
      "2277/2279 [============================>.] - ETA: 0s - loss: 5.0075 - leftLayer1_loss: 0.0975 - midLayer1_loss: 1.3801 - rightLayer1_loss: 1.0319 - leftLayer2_loss: 0.0723 - midLayer2_loss: 1.5136 - rightLayer2_loss: 0.9121\n",
      "Epoch 00007: val_loss improved from 5.05856 to 5.00015, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0068 - leftLayer1_loss: 0.0975 - midLayer1_loss: 1.3800 - rightLayer1_loss: 1.0318 - leftLayer2_loss: 0.0723 - midLayer2_loss: 1.5134 - rightLayer2_loss: 0.9119 - val_loss: 5.0001 - val_leftLayer1_loss: 0.0955 - val_midLayer1_loss: 1.3643 - val_rightLayer1_loss: 1.0080 - val_leftLayer2_loss: 0.0852 - val_midLayer2_loss: 1.4248 - val_rightLayer2_loss: 1.0223\n",
      "Epoch 8/11\n",
      "2274/2279 [============================>.] - ETA: 0s - loss: 4.9628 - leftLayer1_loss: 0.0939 - midLayer1_loss: 1.3803 - rightLayer1_loss: 1.0056 - leftLayer2_loss: 0.0677 - midLayer2_loss: 1.5107 - rightLayer2_loss: 0.9046\n",
      "Epoch 00008: val_loss improved from 5.00015 to 4.95444, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9632 - leftLayer1_loss: 0.0939 - midLayer1_loss: 1.3802 - rightLayer1_loss: 1.0058 - leftLayer2_loss: 0.0677 - midLayer2_loss: 1.5107 - rightLayer2_loss: 0.9049 - val_loss: 4.9544 - val_leftLayer1_loss: 0.0921 - val_midLayer1_loss: 1.3643 - val_rightLayer1_loss: 0.9846 - val_leftLayer2_loss: 0.0816 - val_midLayer2_loss: 1.4248 - val_rightLayer2_loss: 1.0069\n",
      "Epoch 9/11\n",
      "2259/2279 [============================>.] - ETA: 0s - loss: 4.9303 - leftLayer1_loss: 0.0906 - midLayer1_loss: 1.3798 - rightLayer1_loss: 0.9846 - leftLayer2_loss: 0.0638 - midLayer2_loss: 1.5147 - rightLayer2_loss: 0.8968\n",
      "Epoch 00009: val_loss improved from 4.95444 to 4.91792, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9331 - leftLayer1_loss: 0.0907 - midLayer1_loss: 1.3802 - rightLayer1_loss: 0.9856 - leftLayer2_loss: 0.0638 - midLayer2_loss: 1.5146 - rightLayer2_loss: 0.8982 - val_loss: 4.9179 - val_leftLayer1_loss: 0.0890 - val_midLayer1_loss: 1.3643 - val_rightLayer1_loss: 0.9667 - val_leftLayer2_loss: 0.0785 - val_midLayer2_loss: 1.4248 - val_rightLayer2_loss: 0.9946\n",
      "Epoch 10/11\n",
      "2267/2279 [============================>.] - ETA: 0s - loss: 4.9077 - leftLayer1_loss: 0.0876 - midLayer1_loss: 1.3791 - rightLayer1_loss: 0.9689 - leftLayer2_loss: 0.0605 - midLayer2_loss: 1.5168 - rightLayer2_loss: 0.8947\n",
      "Epoch 00010: val_loss improved from 4.91792 to 4.88783, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.9087 - leftLayer1_loss: 0.0876 - midLayer1_loss: 1.3792 - rightLayer1_loss: 0.9693 - leftLayer2_loss: 0.0606 - midLayer2_loss: 1.5169 - rightLayer2_loss: 0.8952 - val_loss: 4.8878 - val_leftLayer1_loss: 0.0860 - val_midLayer1_loss: 1.3643 - val_rightLayer1_loss: 0.9526 - val_leftLayer2_loss: 0.0759 - val_midLayer2_loss: 1.4248 - val_rightLayer2_loss: 0.9842\n",
      "Epoch 11/11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2276/2279 [============================>.] - ETA: 0s - loss: 4.8789 - leftLayer1_loss: 0.0848 - midLayer1_loss: 1.3794 - rightLayer1_loss: 0.9575 - leftLayer2_loss: 0.0580 - midLayer2_loss: 1.5080 - rightLayer2_loss: 0.8912\n",
      "Epoch 00011: val_loss improved from 4.88783 to 4.86248, saving model to ./experiments/0.013999999999999995_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 4.8785 - leftLayer1_loss: 0.0848 - midLayer1_loss: 1.3793 - rightLayer1_loss: 0.9575 - leftLayer2_loss: 0.0580 - midLayer2_loss: 1.5078 - rightLayer2_loss: 0.8912 - val_loss: 4.8625 - val_leftLayer1_loss: 0.0833 - val_midLayer1_loss: 1.3643 - val_rightLayer1_loss: 0.9412 - val_leftLayer2_loss: 0.0735 - val_midLayer2_loss: 1.4248 - val_rightLayer2_loss: 0.9754\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.013999999999999995_test.log **\n",
      "auroc 0PleuralThickening: 0.3336674187198069\n",
      "\n",
      "auprc 0PleuralThickening: 0.02205512443385797\n",
      "\n",
      "auroc 1PleuralThickening: 0.5678317651223395\n",
      "\n",
      "auprc 1PleuralThickening: 0.041301280480681615\n",
      "\n",
      "auroc 2PleuralThickening: 0.46361178512100093\n",
      "\n",
      "auprc 2PleuralThickening: 0.02825390939741032\n",
      "\n",
      "auroc 3PleuralThickening: 0.40985404970381867\n",
      "\n",
      "auprc 3PleuralThickening: 0.024855774292779517\n",
      "\n",
      "auroc 4PleuralThickening: 0.4974488396042309\n",
      "\n",
      "auprc 4PleuralThickening: 0.02966710418176315\n",
      "\n",
      "auroc 5PleuralThickening: 0.4427484007412288\n",
      "\n",
      "auprc 5PleuralThickening: 0.026886911928174315\n",
      "\n",
      "mean auroc: 0.45252704316873765\n",
      "\n",
      "mean auprc: 0.02883668411911115\n",
      "\n",
      "max auroc: 0.5678317651223395\n",
      "\n",
      "max auprc: 0.041301280480681615\n",
      "\n",
      "94.80045008659363\n",
      "** set output weights path to: ./experiments/0.014999999999999994_weights.h5 **\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 1536)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat2 (Flatten)                 (None, 1536)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat1 (Flatten)                 (None, 1536)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 417)          640929      flat2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 292)          448804      flat1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 417)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 292)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu2 (Activation)              (None, 417)          0           gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, 292)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 417)          0           relu2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer1 (Dense)              (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer1 (Dense)               (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer1 (Dense)             (None, 14)           4102        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leftLayer2 (Dense)              (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "midLayer2 (Dense)               (None, 14)           5852        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "rightLayer2 (Dense)             (None, 14)           5852        dropout2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,119,595\n",
      "Trainable params: 19,908\n",
      "Non-trainable params: 1,099,687\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  {'leftLayer1': '...', 'midLayer1': '...', 'rightLayer1': '...', 'leftLayer2': '...', 'midLayer2': '...', 'rightLayer2': '...'}\n",
      "    to  \n",
      "  ['...', '...', '...', '...', '...', '...']\n",
      "Train for 2279 steps, validate for 372 steps\n",
      "Epoch 1/11\n",
      "2270/2279 [============================>.] - ETA: 0s - loss: 6.5249 - leftLayer1_loss: 0.1214 - midLayer1_loss: 1.4630 - rightLayer1_loss: 1.7483 - leftLayer2_loss: 0.1249 - midLayer2_loss: 1.5837 - rightLayer2_loss: 1.4835\n",
      "Epoch 00001: val_loss improved from inf to 6.14620, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "2279/2279 [==============================] - 7s 3ms/step - loss: 6.5241 - leftLayer1_loss: 0.1213 - midLayer1_loss: 1.4631 - rightLayer1_loss: 1.7478 - leftLayer2_loss: 0.1249 - midLayer2_loss: 1.5843 - rightLayer2_loss: 1.4826 - val_loss: 6.1462 - val_leftLayer1_loss: 0.1189 - val_midLayer1_loss: 1.4546 - val_rightLayer1_loss: 1.6161 - val_leftLayer2_loss: 0.1159 - val_midLayer2_loss: 1.4262 - val_rightLayer2_loss: 1.4145\n",
      "Epoch 2/11\n",
      "2258/2279 [============================>.] - ETA: 0s - loss: 5.9056 - leftLayer1_loss: 0.1164 - midLayer1_loss: 1.4615 - rightLayer1_loss: 1.5026 - leftLayer2_loss: 0.1104 - midLayer2_loss: 1.5867 - rightLayer2_loss: 1.1281\n",
      "Epoch 00002: val_loss improved from 6.14620 to 5.74389, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.9058 - leftLayer1_loss: 0.1164 - midLayer1_loss: 1.4621 - rightLayer1_loss: 1.5019 - leftLayer2_loss: 0.1103 - midLayer2_loss: 1.5869 - rightLayer2_loss: 1.1282 - val_loss: 5.7439 - val_leftLayer1_loss: 0.1141 - val_midLayer1_loss: 1.4546 - val_rightLayer1_loss: 1.4038 - val_leftLayer2_loss: 0.1075 - val_midLayer2_loss: 1.4262 - val_rightLayer2_loss: 1.2377\n",
      "Epoch 3/11\n",
      "2263/2279 [============================>.] - ETA: 0s - loss: 5.5976 - leftLayer1_loss: 0.1118 - midLayer1_loss: 1.4620 - rightLayer1_loss: 1.3261 - leftLayer2_loss: 0.0989 - midLayer2_loss: 1.5854 - rightLayer2_loss: 1.0135\n",
      "Epoch 00003: val_loss improved from 5.74389 to 5.49837, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.5973 - leftLayer1_loss: 0.1118 - midLayer1_loss: 1.4621 - rightLayer1_loss: 1.3257 - leftLayer2_loss: 0.0989 - midLayer2_loss: 1.5851 - rightLayer2_loss: 1.0138 - val_loss: 5.4984 - val_leftLayer1_loss: 0.1096 - val_midLayer1_loss: 1.4546 - val_rightLayer1_loss: 1.2575 - val_leftLayer2_loss: 0.1004 - val_midLayer2_loss: 1.4262 - val_rightLayer2_loss: 1.1502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/11\n",
      "2264/2279 [============================>.] - ETA: 0s - loss: 5.4060 - leftLayer1_loss: 0.1074 - midLayer1_loss: 1.4618 - rightLayer1_loss: 1.2079 - leftLayer2_loss: 0.0889 - midLayer2_loss: 1.5780 - rightLayer2_loss: 0.9620\n",
      "Epoch 00004: val_loss improved from 5.49837 to 5.33893, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.4075 - leftLayer1_loss: 0.1074 - midLayer1_loss: 1.4621 - rightLayer1_loss: 1.2080 - leftLayer2_loss: 0.0889 - midLayer2_loss: 1.5785 - rightLayer2_loss: 0.9626 - val_loss: 5.3389 - val_leftLayer1_loss: 0.1054 - val_midLayer1_loss: 1.4546 - val_rightLayer1_loss: 1.1596 - val_leftLayer2_loss: 0.0943 - val_midLayer2_loss: 1.4262 - val_rightLayer2_loss: 1.0989\n",
      "Epoch 5/11\n",
      "2274/2279 [============================>.] - ETA: 0s - loss: 5.2957 - leftLayer1_loss: 0.1033 - midLayer1_loss: 1.4623 - rightLayer1_loss: 1.1293 - leftLayer2_loss: 0.0811 - midLayer2_loss: 1.5817 - rightLayer2_loss: 0.9379\n",
      "Epoch 00005: val_loss improved from 5.33893 to 5.22929, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.2962 - leftLayer1_loss: 0.1033 - midLayer1_loss: 1.4625 - rightLayer1_loss: 1.1293 - leftLayer2_loss: 0.0811 - midLayer2_loss: 1.5818 - rightLayer2_loss: 0.9382 - val_loss: 5.2293 - val_leftLayer1_loss: 0.1014 - val_midLayer1_loss: 1.4546 - val_rightLayer1_loss: 1.0931 - val_leftLayer2_loss: 0.0892 - val_midLayer2_loss: 1.4262 - val_rightLayer2_loss: 1.0648\n",
      "Epoch 6/11\n",
      "2260/2279 [============================>.] - ETA: 0s - loss: 5.2173 - leftLayer1_loss: 0.0995 - midLayer1_loss: 1.4613 - rightLayer1_loss: 1.0750 - leftLayer2_loss: 0.0745 - midLayer2_loss: 1.5855 - rightLayer2_loss: 0.9214\n",
      "Epoch 00006: val_loss improved from 5.22929 to 5.15026, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.2203 - leftLayer1_loss: 0.0995 - midLayer1_loss: 1.4617 - rightLayer1_loss: 1.0758 - leftLayer2_loss: 0.0745 - midLayer2_loss: 1.5861 - rightLayer2_loss: 0.9227 - val_loss: 5.1503 - val_leftLayer1_loss: 0.0977 - val_midLayer1_loss: 1.4546 - val_rightLayer1_loss: 1.0467 - val_leftLayer2_loss: 0.0849 - val_midLayer2_loss: 1.4262 - val_rightLayer2_loss: 1.0402\n",
      "Epoch 7/11\n",
      "2256/2279 [============================>.] - ETA: 0s - loss: 5.1578 - leftLayer1_loss: 0.0959 - midLayer1_loss: 1.4613 - rightLayer1_loss: 1.0372 - leftLayer2_loss: 0.0696 - midLayer2_loss: 1.5812 - rightLayer2_loss: 0.9126\n",
      "Epoch 00007: val_loss improved from 5.15026 to 5.09073, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.1619 - leftLayer1_loss: 0.0960 - midLayer1_loss: 1.4619 - rightLayer1_loss: 1.0380 - leftLayer2_loss: 0.0696 - midLayer2_loss: 1.5823 - rightLayer2_loss: 0.9142 - val_loss: 5.0907 - val_leftLayer1_loss: 0.0943 - val_midLayer1_loss: 1.4546 - val_rightLayer1_loss: 1.0131 - val_leftLayer2_loss: 0.0811 - val_midLayer2_loss: 1.4262 - val_rightLayer2_loss: 1.0214\n",
      "Epoch 8/11\n",
      "2261/2279 [============================>.] - ETA: 0s - loss: 5.1150 - leftLayer1_loss: 0.0926 - midLayer1_loss: 1.4603 - rightLayer1_loss: 1.0094 - leftLayer2_loss: 0.0653 - midLayer2_loss: 1.5819 - rightLayer2_loss: 0.9055\n",
      "Epoch 00008: val_loss improved from 5.09073 to 5.04458, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.1178 - leftLayer1_loss: 0.0926 - midLayer1_loss: 1.4607 - rightLayer1_loss: 1.0101 - leftLayer2_loss: 0.0654 - midLayer2_loss: 1.5825 - rightLayer2_loss: 0.9065 - val_loss: 5.0446 - val_leftLayer1_loss: 0.0911 - val_midLayer1_loss: 1.4546 - val_rightLayer1_loss: 0.9881 - val_leftLayer2_loss: 0.0779 - val_midLayer2_loss: 1.4262 - val_rightLayer2_loss: 1.0067\n",
      "Epoch 9/11\n",
      "2270/2279 [============================>.] - ETA: 0s - loss: 5.0869 - leftLayer1_loss: 0.0896 - midLayer1_loss: 1.4627 - rightLayer1_loss: 0.9889 - leftLayer2_loss: 0.0615 - midLayer2_loss: 1.5844 - rightLayer2_loss: 0.8998\n",
      "Epoch 00009: val_loss improved from 5.04458 to 5.00768, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0875 - leftLayer1_loss: 0.0895 - midLayer1_loss: 1.4627 - rightLayer1_loss: 0.9890 - leftLayer2_loss: 0.0615 - midLayer2_loss: 1.5848 - rightLayer2_loss: 0.9000 - val_loss: 5.0077 - val_leftLayer1_loss: 0.0881 - val_midLayer1_loss: 1.4546 - val_rightLayer1_loss: 0.9689 - val_leftLayer2_loss: 0.0751 - val_midLayer2_loss: 1.4262 - val_rightLayer2_loss: 0.9948\n",
      "Epoch 10/11\n",
      "2265/2279 [============================>.] - ETA: 0s - loss: 5.0538 - leftLayer1_loss: 0.0866 - midLayer1_loss: 1.4613 - rightLayer1_loss: 0.9718 - leftLayer2_loss: 0.0590 - midLayer2_loss: 1.5799 - rightLayer2_loss: 0.8951\n",
      "Epoch 00010: val_loss improved from 5.00768 to 4.97733, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0553 - leftLayer1_loss: 0.0866 - midLayer1_loss: 1.4617 - rightLayer1_loss: 0.9724 - leftLayer2_loss: 0.0590 - midLayer2_loss: 1.5797 - rightLayer2_loss: 0.8958 - val_loss: 4.9773 - val_leftLayer1_loss: 0.0853 - val_midLayer1_loss: 1.4546 - val_rightLayer1_loss: 0.9538 - val_leftLayer2_loss: 0.0727 - val_midLayer2_loss: 1.4262 - val_rightLayer2_loss: 0.9848\n",
      "Epoch 11/11\n",
      "2273/2279 [============================>.] - ETA: 0s - loss: 5.0378 - leftLayer1_loss: 0.0840 - midLayer1_loss: 1.4622 - rightLayer1_loss: 0.9595 - leftLayer2_loss: 0.0565 - midLayer2_loss: 1.5828 - rightLayer2_loss: 0.8927\n",
      "Epoch 00011: val_loss improved from 4.97733 to 4.95186, saving model to ./experiments/0.014999999999999994_weights.h5\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 5.0381 - leftLayer1_loss: 0.0840 - midLayer1_loss: 1.4623 - rightLayer1_loss: 0.9596 - leftLayer2_loss: 0.0565 - midLayer2_loss: 1.5830 - rightLayer2_loss: 0.8928 - val_loss: 4.9519 - val_leftLayer1_loss: 0.0827 - val_midLayer1_loss: 1.4546 - val_rightLayer1_loss: 0.9416 - val_leftLayer2_loss: 0.0705 - val_midLayer2_loss: 1.4262 - val_rightLayer2_loss: 0.9763\n",
      "22433/22433 [==============================] - 28s 1ms/step\n",
      "** write log to ./experiments/0.014999999999999994_test.log **\n",
      "auroc 0PleuralThickening: 0.5973576991518714\n",
      "\n",
      "auprc 0PleuralThickening: 0.04072379321620731\n",
      "\n",
      "auroc 1PleuralThickening: 0.613900921864705\n",
      "\n",
      "auprc 1PleuralThickening: 0.04455246478749149\n",
      "\n",
      "auroc 2PleuralThickening: 0.5442901724649097\n",
      "\n",
      "auprc 2PleuralThickening: 0.03425923233490831\n",
      "\n",
      "auroc 3PleuralThickening: 0.5090338358615454\n",
      "\n",
      "auprc 3PleuralThickening: 0.03151819672748456\n",
      "\n",
      "auroc 4PleuralThickening: 0.5334043005786502\n",
      "\n",
      "auprc 4PleuralThickening: 0.03692690471310156\n",
      "\n",
      "auroc 5PleuralThickening: 0.37466630702729553\n",
      "\n",
      "auprc 5PleuralThickening: 0.0237346689052039\n",
      "\n",
      "mean auroc: 0.5287755394914962\n",
      "\n",
      "mean auprc: 0.035285876780732854\n",
      "\n",
      "max auroc: 0.613900921864705\n",
      "\n",
      "max auprc: 0.04455246478749149\n",
      "\n",
      "95.09299564361572\n"
     ]
    }
   ],
   "source": [
    "step = np.arange(0.009, 0.0151, 0.001)\n",
    "maxi = []\n",
    "for k in np.nditer(step):\n",
    "    opn, daTime = optimize_network(k)\n",
    "    print(daTime)\n",
    "    maxi.append(opn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6462519524939496\n"
     ]
    }
   ],
   "source": [
    "print(np.max(maxi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
